{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 666"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import MissingIndicator, SimpleImputer, IterativeImputer, KNNImputer\n",
    "from category_encoders.cat_boost import CatBoostEncoder\n",
    "from deepctr.inputs import  SparseFeat, DenseFeat, get_feature_names\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam,RMSprop\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from deepctr.models import DeepFM\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import re\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import utils\n",
    "from keras_tqdm import TQDMNotebookCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('cat_in_dat/train_cat_kaggle.csv')\n",
    "test = pd.read_csv('cat_in_dat/test_cat_kaggle.csv')\n",
    "\n",
    "test[\"target\"] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [feat for feat in train.columns if feat not in ['target','id']]\n",
    "train['nan_features'] = train[features].isnull().sum(axis=1)\n",
    "test['nan_features'] = test[features].isnull().sum(axis=1)\n",
    "\n",
    "data = pd.concat([train, test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label encode and fillna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_numeric(df):\n",
    "    \n",
    "    bin_3_mapping = {'T':1 , 'F':0}\n",
    "    bin_4_mapping = {'Y':1 , 'N':0}\n",
    "    nom_0_mapping = {'Red' : 0, 'Blue' : 1, 'Green' : 2}\n",
    "    nom_1_mapping = {'Trapezoid' : 0, 'Star' : 1, 'Circle': 2, 'Triangle' : 3, 'Polygon' : 4}\n",
    "    nom_2_mapping = {'Hamster' : 0 , 'Axolotl' : 1, 'Lion' : 2, 'Dog' : 3, 'Cat' : 4, 'Snake' : 5}\n",
    "    nom_3_mapping = {'Russia' : 0, 'Canada' : 1, 'Finland' : 2, 'Costa Rica' : 3, 'China' : 4, 'India' : 5}\n",
    "    nom_4_mapping = {'Bassoon' : 0, 'Theremin' : 1, 'Oboe' : 2, 'Piano' : 3}\n",
    "    nom_5_mapping = dict(zip((df.nom_5.dropna().unique()), range(len((df.nom_5.dropna().unique())))))\n",
    "    nom_6_mapping = dict(zip((df.nom_6.dropna().unique()), range(len((df.nom_6.dropna().unique())))))\n",
    "    nom_7_mapping = dict(zip((df.nom_7.dropna().unique()), range(len((df.nom_7.dropna().unique())))))\n",
    "    nom_8_mapping = dict(zip((df.nom_8.dropna().unique()), range(len((df.nom_8.dropna().unique())))))\n",
    "    nom_9_mapping = dict(zip((df.nom_9.dropna().unique()), range(len((df.nom_9.dropna().unique())))))\n",
    "    ord_1_mapping = {'Novice' : 0, 'Contributor' : 1, 'Expert' : 2, 'Master': 3, 'Grandmaster': 4}\n",
    "    ord_2_mapping = { 'Freezing': 0, 'Cold': 1, 'Warm' : 2, 'Hot': 3, 'Boiling Hot' : 4, 'Lava Hot' : 5}\n",
    "    ord_3_mapping = {'a':0, 'b':1, 'c':2 ,'d':3 ,'e':4, 'f':5, 'g':6, 'h':7, 'i':8, 'j':9, 'k':10, 'l':11, 'm':12, 'n':13, 'o':14}\n",
    "    ord_4_mapping = {'A':0, 'B':1, 'C':2, 'D':3, 'E':4, 'F':5, 'G':6, 'H':7, 'I':8, 'J':9, 'K':10,'L':11,'M':12,\n",
    "                 'N':13,'O':14,'P':15,'Q':16,'R':17,'S':18,'T':19,'U':20,'V':21,'W':22,'X':23,'Y':24,'Z':25}\n",
    "    sorted_ord_5 = sorted(df.ord_5.dropna().unique())\n",
    "    ord_5_mapping = dict(zip(sorted_ord_5, range(len(sorted_ord_5))))\n",
    "\n",
    "    df['bin_3'] = df.loc[df.bin_3.notnull(), 'bin_3'].map(bin_3_mapping)\n",
    "    df['bin_4'] = df.loc[df.bin_4.notnull(), 'bin_4'].map(bin_4_mapping)\n",
    "    df['nom_0'] = df.loc[df.nom_0.notnull(), 'nom_0'].map(nom_0_mapping)\n",
    "    df['nom_1'] = df.loc[df.nom_1.notnull(), 'nom_1'].map(nom_1_mapping)\n",
    "    df['nom_2'] = df.loc[df.nom_2.notnull(), 'nom_2'].map(nom_2_mapping)\n",
    "    df['nom_3'] = df.loc[df.nom_3.notnull(), 'nom_3'].map(nom_3_mapping)\n",
    "    df['nom_4'] = df.loc[df.nom_4.notnull(), 'nom_4'].map(nom_4_mapping)\n",
    "    df['nom_5'] = df.loc[df.nom_5.notnull(), 'nom_5'].map(nom_5_mapping)\n",
    "    df['nom_6'] = df.loc[df.nom_6.notnull(), 'nom_6'].map(nom_6_mapping)\n",
    "    df['nom_7'] = df.loc[df.nom_7.notnull(), 'nom_7'].map(nom_7_mapping)\n",
    "    df['nom_8'] = df.loc[df.nom_8.notnull(), 'nom_8'].map(nom_8_mapping)\n",
    "    df['nom_9'] = df.loc[df.nom_9.notnull(), 'nom_9'].map(nom_9_mapping)\n",
    "    df['ord_1'] = df.loc[df.ord_1.notnull(), 'ord_1'].map(ord_1_mapping)\n",
    "    df['ord_2'] = df.loc[df.ord_2.notnull(), 'ord_2'].map(ord_2_mapping)\n",
    "    df['ord_3'] = df.loc[df.ord_3.notnull(), 'ord_3'].map(ord_3_mapping)\n",
    "    df['ord_4'] = df.loc[df.ord_4.notnull(), 'ord_4'].map(ord_4_mapping)\n",
    "    df['ord_5'] = df.loc[df.ord_5.notnull(), 'ord_5'].map(ord_5_mapping)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define sparse features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NaN count\n",
    "# data['nan_count'] = data.isnull().sum(axis=1) Problem with nan count\n",
    "\n",
    "sparse_features = [feat for feat in train.columns if feat not in ['id', 'target']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep categories present in train AND test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 values in nom_5, {'7331b57f0', '0385d0739', 'd6bb2181a', 'b3ad70fcb', 'd1d7d8352'} Replaced with nan\n",
      "7 values in nom_6, {'b4b8de4b9', 'a18f02793', 'f2c0f1d10', '3a121fefb', '27fadf6ea', 'ee6983c6d', 'd6ea07c05'} Replaced with nan\n",
      "8 values in nom_9, {'d1e6704ed', '68a201317', '5f565a682', '47a0cd9da', 'b5f21647b', '432e3fc6a', '1538d82e9', '2394a46de'} Replaced with nan\n",
      "1 values in nan_features, {7} Replaced with nan\n"
     ]
    }
   ],
   "source": [
    "for col in sparse_features:\n",
    "    train_unique_values = set(train[col].dropna().unique())\n",
    "    test_unique_values  = set(test[col].dropna().unique())\n",
    "\n",
    "    symmetric_difference_values = train_unique_values.symmetric_difference(test_unique_values)\n",
    "    if symmetric_difference_values:\n",
    "        print(f'{len(symmetric_difference_values)} values in {col}, {symmetric_difference_values} Replaced with nan')\n",
    "        data.loc[data[col].isin(symmetric_difference_values), col] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fillna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "features = [feat for feat in data.columns if feat not in ['target','id']]\n",
    "data[features] = convert_data_to_numeric(data[features])\n",
    "data[features] = data[features].astype('category')\n",
    "imp = IterativeImputer(max_iter=500, initial_strategy='most_frequent', random_state=SEED, add_indicator=True)\n",
    "indicator_cols = [feat + '_ind' for feat in features]\n",
    "for col in indicator_cols:\n",
    "    data[col] = 0\n",
    "data[features+indicator_cols] = imp.fit_transform(data[features])\n",
    "data[features] = data[features].round(0).astype(np.int16)\n",
    "data[indicator_cols] = data[indicator_cols].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[features] = convert_data_to_numeric(data[features])\n",
    "data = data.fillna(-1)\n",
    "data[features] = data[features] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[data.target != -1].reset_index(drop=True)\n",
    "test  = data[data.target == -1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dense features with catboostencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 s, sys: 513 ms, total: 16.5 s\n",
      "Wall time: 17.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features_enc = [col + '_enc' for col in features]\n",
    "for col in features:\n",
    "    catenc = CatBoostEncoder(return_df=False, random_state=SEED)\n",
    "    train[col + '_enc'] = catenc.fit_transform(train.loc[:, col].astype('str'), train.target.values)\n",
    "    test[col + '_enc'] = catenc.transform(test.loc[:, col].astype('str'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick nan count study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tab = pd.crosstab(train.nan_count, train.target)\n",
    "# tab.T/tab.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(y_true, y_pred):\n",
    "    def fallback_auc(y_true, y_pred):\n",
    "        try:\n",
    "            return roc_auc_score(y_true, y_pred)\n",
    "        except:\n",
    "            return 0.5\n",
    "    return tf.py_function(fallback_auc, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicLR(keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1 / (2. ** (x - 1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma ** (x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "\n",
    "    def clr(self):\n",
    "        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n",
    "        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x)) * self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x)) * self.scale_fn(\n",
    "                self.clr_iterations)\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "\n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['target']\n",
    "N_Splits = 5\n",
    "Verbose = 1\n",
    "Epochs = 50\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "DROPOUT = 0.3\n",
    "NNLAYERS = (300, 300)\n",
    "PATIENCE = 5\n",
    "\n",
    "MAX_EMB_DIM = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_enc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-1a825e55cdc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdense_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures_enc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mall_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mindicator_cols\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfeatures_enc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features_enc' is not defined"
     ]
    }
   ],
   "source": [
    "dense_features = features_enc\n",
    "all_features = features + indicator_cols + features_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features += indicator_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(data, catcols, densecols, dnn_layers, last_dense):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    outputs_emb = []\n",
    "    \n",
    "    # Cat cols\n",
    "    for c in catcols:\n",
    "        \n",
    "        num_unique_values = int(data[c].nunique())\n",
    "        embed_dim = int(min(np.ceil(math.log(num_unique_values)), MAX_EMB_DIM))\n",
    "        inp = layers.Input(shape=(1,))\n",
    "        out = layers.Embedding(num_unique_values + 1, embed_dim, name=c)(inp)\n",
    "        out = layers.SpatialDropout1D(DROPOUT)(out)\n",
    "        out = layers.Reshape(target_shape=(embed_dim, ))(out)\n",
    "        inputs.append(inp)\n",
    "        outputs.append(out)\n",
    "        outputs_emb.append(out)\n",
    "        \n",
    "    # First dense for embeddings\n",
    "#     outputs_emb = layers.Concatenate()(outputs_emb)\n",
    "#     outputs_emb = layers.Dense(last_dense, activation=\"relu\")(outputs_emb)\n",
    "#     outputs_emb = layers.Dropout(DROPOUT)(outputs_emb)\n",
    "        \n",
    "    # Dense cols\n",
    "#     if densecols:\n",
    "#         dense_inp = layers.Input(shape=(len(densecols),))\n",
    "#         inputs.append(dense_inp)\n",
    "\n",
    "#         outputs.append(dense_inp)\n",
    "        \n",
    "    x = layers.Concatenate()(outputs)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # DNN layers\n",
    "    for size in dnn_layers:\n",
    "#         x = layers.Concatenate()([x, outputs_emb])\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(size, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(DROPOUT)(x)\n",
    "        \n",
    "#     x = layers.Concatenate()([x, outputs_emb])\n",
    "    \n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    y = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=y)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_cv():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    oof_pred_deepfm = np.zeros((len(train), ))\n",
    "    y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "\n",
    "    for fold, (tr_ind, val_ind) in enumerate(skf.split(train, train[target])):\n",
    "\n",
    "        # Split\n",
    "        X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "        y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "        train_model_input = [X_train.loc[:, sparse_features].values[:, k] \\\n",
    "                             for k in range(X_train.loc[:, sparse_features].values.shape[1])]\n",
    "    #     train_model_input += [X_train.loc[:, dense_features].values]\n",
    "        val_model_input = [X_val.loc[:, sparse_features].values[:, k] \\\n",
    "                           for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "    #     val_model_input += [X_val.loc[:, dense_features].values]\n",
    "        test_model_input = [test.loc[:, sparse_features].values[:, k] \\\n",
    "                            for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "    #     test_model_input += [test.loc[:, dense_features].values]\n",
    "\n",
    "\n",
    "        # Define model\n",
    "        model = create_model(data, sparse_features, ['id'], (300, 300), 256)\n",
    "        opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "\n",
    "        # Define callbacks\n",
    "        es = callbacks.EarlyStopping(\n",
    "            monitor='val_auc', \n",
    "            min_delta=0.001, \n",
    "            patience=PATIENCE, \n",
    "            verbose=Verbose, \n",
    "            mode='max', \n",
    "            baseline=None, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        #sb = callbacks.ModelCheckpoint(\n",
    "         #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "        #)\n",
    "    #     clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "    #                        step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "    #                        gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_auc', \n",
    "            mode='max',\n",
    "            factor=0.5,\n",
    "            patience=3, \n",
    "            min_lr=1e-6,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "    #     cb = TQDMNotebookCallback()\n",
    "    #     setattr(cb,'on_train_batch_begin',lambda x,y:None)\n",
    "    #     setattr(cb,'on_train_batch_end',lambda x,y:None)\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train_model_input, utils.to_categorical(y_train),\n",
    "            validation_data=(val_model_input, utils.to_categorical(y_val)),\n",
    "            batch_size=1024, \n",
    "            epochs=Epochs, \n",
    "            verbose=1,\n",
    "            callbacks=[reduce_lr, es]\n",
    "        )\n",
    "\n",
    "        # Predict\n",
    "        val_pred = model.predict(val_model_input, batch_size=512)[:, 1]\n",
    "        print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "        oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "        y_pred_deepfm += model.predict(test_model_input, batch_size=512)[:, 1].ravel() / (N_Splits)\n",
    "        K.clear_session()\n",
    "        \n",
    "    return oof_pred_deepfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 383639 samples, validate on 95910 samples\n",
      "Epoch 1/50\n",
      "383639/383639 [==============================] - 12s 33us/sample - loss: 0.4368 - auc: 0.7161 - val_loss: 0.4016 - val_auc: 0.7841\n",
      "Epoch 2/50\n",
      "383639/383639 [==============================] - 10s 26us/sample - loss: 0.4107 - auc: 0.7660 - val_loss: 0.4001 - val_auc: 0.7851\n",
      "Epoch 3/50\n",
      "383639/383639 [==============================] - 9s 24us/sample - loss: 0.4069 - auc: 0.7721 - val_loss: 0.4034 - val_auc: 0.7850\n",
      "Epoch 4/50\n",
      "383639/383639 [==============================] - 10s 26us/sample - loss: 0.4054 - auc: 0.7741 - val_loss: 0.4042 - val_auc: 0.7833\n",
      "Epoch 5/50\n",
      "381952/383639 [============================>.] - ETA: 0s - loss: 0.4038 - auc: 0.7769\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "383639/383639 [==============================] - 9s 24us/sample - loss: 0.4037 - auc: 0.7769 - val_loss: 0.4037 - val_auc: 0.7820\n",
      "Epoch 6/50\n",
      "383639/383639 [==============================] - 9s 23us/sample - loss: 0.3995 - auc: 0.7830 - val_loss: 0.4036 - val_auc: 0.7817\n",
      "Epoch 7/50\n",
      "380928/383639 [============================>.] - ETA: 0s - loss: 0.3983 - auc: 0.7851Restoring model weights from the end of the best epoch.\n",
      "383639/383639 [==============================] - 8s 22us/sample - loss: 0.3982 - auc: 0.7850 - val_loss: 0.4047 - val_auc: 0.7801\n",
      "Epoch 00007: early stopping\n",
      "validation AUC fold 1 : 0.78518\n",
      "Train on 383639 samples, validate on 95910 samples\n",
      "Epoch 1/50\n",
      "383639/383639 [==============================] - 13s 34us/sample - loss: 0.4352 - auc: 0.7185 - val_loss: 0.4013 - val_auc: 0.7831\n",
      "Epoch 2/50\n",
      "383639/383639 [==============================] - 9s 24us/sample - loss: 0.4104 - auc: 0.7667 - val_loss: 0.4001 - val_auc: 0.7855\n",
      "Epoch 3/50\n",
      "383639/383639 [==============================] - 11s 29us/sample - loss: 0.4067 - auc: 0.7723 - val_loss: 0.3992 - val_auc: 0.7857\n",
      "Epoch 4/50\n",
      "383639/383639 [==============================] - 11s 27us/sample - loss: 0.4040 - auc: 0.7766 - val_loss: 0.4008 - val_auc: 0.7840\n",
      "Epoch 5/50\n",
      "383639/383639 [==============================] - 11s 27us/sample - loss: 0.4019 - auc: 0.7796 - val_loss: 0.4019 - val_auc: 0.7835\n",
      "Epoch 6/50\n",
      "381952/383639 [============================>.] - ETA: 0s - loss: 0.4009 - auc: 0.7816\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "383639/383639 [==============================] - 11s 28us/sample - loss: 0.4009 - auc: 0.7816 - val_loss: 0.4025 - val_auc: 0.7817\n",
      "Epoch 7/50\n",
      "380928/383639 [============================>.] - ETA: 0s - loss: 0.3964 - auc: 0.7871Restoring model weights from the end of the best epoch.\n",
      "383639/383639 [==============================] - 9s 25us/sample - loss: 0.3964 - auc: 0.7872 - val_loss: 0.4052 - val_auc: 0.7796\n",
      "Epoch 00007: early stopping\n",
      "validation AUC fold 2 : 0.78533\n",
      "Train on 383639 samples, validate on 95910 samples\n",
      "Epoch 1/50\n",
      "383639/383639 [==============================] - 11s 29us/sample - loss: 0.4340 - auc: 0.7236 - val_loss: 0.4008 - val_auc: 0.7831\n",
      "Epoch 2/50\n",
      "383639/383639 [==============================] - 8s 21us/sample - loss: 0.4101 - auc: 0.7672 - val_loss: 0.4010 - val_auc: 0.7858\n",
      "Epoch 3/50\n",
      "383639/383639 [==============================] - 8s 21us/sample - loss: 0.4067 - auc: 0.7727 - val_loss: 0.3994 - val_auc: 0.7853\n",
      "Epoch 4/50\n",
      "383639/383639 [==============================] - 8s 21us/sample - loss: 0.4044 - auc: 0.7762 - val_loss: 0.3989 - val_auc: 0.7844\n",
      "Epoch 5/50\n",
      "382976/383639 [============================>.] - ETA: 0s - loss: 0.4028 - auc: 0.7785\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "383639/383639 [==============================] - 9s 24us/sample - loss: 0.4028 - auc: 0.7785 - val_loss: 0.4044 - val_auc: 0.7840\n",
      "Epoch 6/50\n",
      "383639/383639 [==============================] - 11s 28us/sample - loss: 0.3987 - auc: 0.7845 - val_loss: 0.4016 - val_auc: 0.7821\n",
      "Epoch 7/50\n",
      "380928/383639 [============================>.] - ETA: 0s - loss: 0.3967 - auc: 0.7873Restoring model weights from the end of the best epoch.\n",
      "383639/383639 [==============================] - 10s 27us/sample - loss: 0.3968 - auc: 0.7872 - val_loss: 0.4056 - val_auc: 0.7815\n",
      "Epoch 00007: early stopping\n",
      "validation AUC fold 3 : 0.78591\n",
      "Train on 383639 samples, validate on 95910 samples\n",
      "Epoch 1/50\n",
      "383639/383639 [==============================] - 14s 35us/sample - loss: 0.4340 - auc: 0.7219 - val_loss: 0.4034 - val_auc: 0.7825\n",
      "Epoch 2/50\n",
      "383639/383639 [==============================] - 10s 26us/sample - loss: 0.4098 - auc: 0.7676 - val_loss: 0.4019 - val_auc: 0.7833\n",
      "Epoch 3/50\n",
      "383639/383639 [==============================] - 10s 26us/sample - loss: 0.4066 - auc: 0.7729 - val_loss: 0.4024 - val_auc: 0.7829\n",
      "Epoch 4/50\n",
      "383639/383639 [==============================] - 10s 27us/sample - loss: 0.4038 - auc: 0.7773 - val_loss: 0.4052 - val_auc: 0.7809\n",
      "Epoch 5/50\n",
      "381952/383639 [============================>.] - ETA: 0s - loss: 0.4027 - auc: 0.7790\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "383639/383639 [==============================] - 11s 30us/sample - loss: 0.4026 - auc: 0.7789 - val_loss: 0.4015 - val_auc: 0.7810\n",
      "Epoch 6/50\n",
      "381952/383639 [============================>.] - ETA: 0s - loss: 0.3982 - auc: 0.7847Restoring model weights from the end of the best epoch.\n",
      "383639/383639 [==============================] - 11s 28us/sample - loss: 0.3982 - auc: 0.7848 - val_loss: 0.4066 - val_auc: 0.7800\n",
      "Epoch 00006: early stopping\n",
      "validation AUC fold 4 : 0.78226\n",
      "Train on 383640 samples, validate on 95909 samples\n",
      "Epoch 1/50\n",
      "383640/383640 [==============================] - 15s 39us/sample - loss: 0.4349 - auc: 0.7198 - val_loss: 0.4020 - val_auc: 0.7835\n",
      "Epoch 2/50\n",
      "383640/383640 [==============================] - 10s 26us/sample - loss: 0.4100 - auc: 0.7675 - val_loss: 0.3997 - val_auc: 0.7856\n",
      "Epoch 3/50\n",
      "383640/383640 [==============================] - 10s 26us/sample - loss: 0.4063 - auc: 0.7731 - val_loss: 0.4005 - val_auc: 0.7844\n",
      "Epoch 4/50\n",
      "383640/383640 [==============================] - 11s 27us/sample - loss: 0.4044 - auc: 0.7757 - val_loss: 0.4000 - val_auc: 0.7842\n",
      "Epoch 5/50\n",
      "382976/383640 [============================>.] - ETA: 0s - loss: 0.4025 - auc: 0.7787\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "383640/383640 [==============================] - 10s 27us/sample - loss: 0.4025 - auc: 0.7787 - val_loss: 0.4002 - val_auc: 0.7838\n",
      "Epoch 6/50\n",
      "383640/383640 [==============================] - 10s 25us/sample - loss: 0.3989 - auc: 0.7839 - val_loss: 0.4004 - val_auc: 0.7826\n",
      "Epoch 7/50\n",
      "381952/383640 [============================>.] - ETA: 0s - loss: 0.3975 - auc: 0.7858Restoring model weights from the end of the best epoch.\n",
      "383640/383640 [==============================] - 11s 30us/sample - loss: 0.3976 - auc: 0.7858 - val_loss: 0.4018 - val_auc: 0.7820\n",
      "Epoch 00007: early stopping\n",
      "validation AUC fold 5 : 0.78572\n"
     ]
    }
   ],
   "source": [
    "oof_pred_deepfm = run_cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF AUC : 0.78466\n"
     ]
    }
   ],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "#ln dim maxembdim 50 no BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF AUC : 0.78431\n"
     ]
    }
   ],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "#ln dim maxembdim 20 no BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF AUC : 0.78399\n"
     ]
    }
   ],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "#ln dim maxembdim 200 no BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF AUC : 0.78487\n"
     ]
    }
   ],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "#ln dim maxembdim 100 no BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF AUC : 0.78306\n"
     ]
    }
   ],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "# No BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF AUC : 0.78296\n"
     ]
    }
   ],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF AUC : 0.78375\n"
     ]
    }
   ],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.7838"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "del oof_pred_deepfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7595"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* test with nan_count\n",
    "* add dense features OK\n",
    "* test with ln instead of *0.5 OK\n",
    "* increase MAX_EMB_DIM OK\n",
    "* archi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['target']\n",
    "N_Splits = 5\n",
    "Verbose = 1\n",
    "Epochs = 50\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "DROPOUT = 0.3\n",
    "NNLAYERS = (300, 300)\n",
    "PATIENCE = 5\n",
    "\n",
    "MAX_EMB_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dense_features = features_enc\n",
    "all_features = features + features_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(data, catcols, densecols, dnn_layers, last_dense):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    dense_out = []\n",
    "    \n",
    "    # Cat cols\n",
    "    for c in catcols:\n",
    "        \n",
    "        num_unique_values = int(data[c].nunique())\n",
    "        embed_dim = int(min(np.ceil(math.log(num_unique_values)), MAX_EMB_DIM))\n",
    "        inp = layers.Input(shape=(1,))\n",
    "        out = layers.Embedding(num_unique_values + 1, embed_dim, name=c)(inp)\n",
    "        out = layers.SpatialDropout1D(DROPOUT)(out)\n",
    "        out = layers.Reshape(target_shape=(embed_dim, ))(out)\n",
    "        inputs.append(inp)\n",
    "        outputs.append(out)\n",
    "        \n",
    "#     First dense for embeddings\n",
    "#     outputs_emb = layers.Concatenate()(outputs_emb)\n",
    "#     outputs_emb = layers.Dense(last_dense, activation=\"relu\")(outputs_emb)\n",
    "#     outputs_emb = layers.Dropout(DROPOUT)(outputs_emb)\n",
    "\n",
    "#     outputs_emb = layers.Concatenate()(outputs_emb)\n",
    "#     outputs_emb = layers.Dense(last_dense, activation=\"relu\")(outputs_emb)\n",
    "#     outputs_emb = layers.Dropout(DROPOUT)(outputs_emb)\n",
    "        \n",
    "    # Dense cols\n",
    "    if densecols:\n",
    "        dense_inp = layers.Input(shape=(len(densecols),))\n",
    "        inputs.append(dense_inp)\n",
    "\n",
    "    dense_inp = layers.Dense(\n",
    "        last_dense, \n",
    "        activation=\"relu\",\n",
    "        kernel_regularizer=keras.regularizers.l2(0.01)\n",
    "    )(dense_inp)\n",
    "    dense_inp = layers.Dropout(0.5)(dense_inp)\n",
    "        \n",
    "    x = layers.Concatenate()(outputs)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # DNN layers\n",
    "    for size in dnn_layers:\n",
    "#         x = layers.Concatenate()([x, outputs_emb])\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(size, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(DROPOUT)(x)\n",
    "        \n",
    "    x = layers.Concatenate()([x, dense_inp])\n",
    "    \n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    y = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=y)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_cv():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    oof_pred_deepfm = np.zeros((len(train), ))\n",
    "    y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "\n",
    "    for fold, (tr_ind, val_ind) in enumerate(skf.split(train, train[target])):\n",
    "\n",
    "        # Split\n",
    "        X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "        y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "        train_model_input = [X_train.loc[:, sparse_features].values[:, k] \\\n",
    "                             for k in range(X_train.loc[:, sparse_features].values.shape[1])]\n",
    "        train_model_input += [X_train.loc[:, dense_features].values]\n",
    "        val_model_input = [X_val.loc[:, sparse_features].values[:, k] \\\n",
    "                           for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "        val_model_input += [X_val.loc[:, dense_features].values]\n",
    "        test_model_input = [test.loc[:, sparse_features].values[:, k] \\\n",
    "                            for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "        test_model_input += [test.loc[:, dense_features].values]\n",
    "\n",
    "\n",
    "        # Define model\n",
    "        model = create_model(data, sparse_features, dense_features, (300, 300), 32)\n",
    "        opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "\n",
    "        # Define callbacks\n",
    "        es = callbacks.EarlyStopping(\n",
    "            monitor='val_auc', \n",
    "            min_delta=0.001, \n",
    "            patience=PATIENCE, \n",
    "            verbose=Verbose, \n",
    "            mode='max', \n",
    "            baseline=None, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        #sb = callbacks.ModelCheckpoint(\n",
    "         #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "        #)\n",
    "    #     clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "    #                        step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "    #                        gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_auc', \n",
    "            mode='max',\n",
    "            factor=0.5,\n",
    "            patience=3, \n",
    "            min_lr=1e-6,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "    #     cb = TQDMNotebookCallback()\n",
    "    #     setattr(cb,'on_train_batch_begin',lambda x,y:None)\n",
    "    #     setattr(cb,'on_train_batch_end',lambda x,y:None)\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train_model_input, utils.to_categorical(y_train),\n",
    "            validation_data=(val_model_input, utils.to_categorical(y_val)),\n",
    "            batch_size=1024, \n",
    "            epochs=Epochs, \n",
    "            verbose=1,\n",
    "            callbacks=[reduce_lr, es]\n",
    "        )\n",
    "\n",
    "        # Predict\n",
    "        val_pred = model.predict(val_model_input, batch_size=512)[:, 1]\n",
    "        print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "        oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "        y_pred_deepfm += model.predict(test_model_input, batch_size=512)[:, 1].ravel() / (N_Splits)\n",
    "        K.clear_session()\n",
    "        \n",
    "    return oof_pred_deepfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 383639 samples, validate on 95910 samples\n",
      "Epoch 1/50\n",
      "383639/383639 [==============================] - 17s 45us/sample - loss: 0.5121 - auc: 0.7171 - val_loss: 0.4072 - val_auc: 0.7839\n",
      "Epoch 2/50\n",
      "383639/383639 [==============================] - 11s 28us/sample - loss: 0.4113 - auc: 0.7665 - val_loss: 0.3991 - val_auc: 0.7861\n",
      "Epoch 3/50\n",
      "383639/383639 [==============================] - 11s 28us/sample - loss: 0.4064 - auc: 0.7730 - val_loss: 0.4012 - val_auc: 0.7849\n",
      "Epoch 4/50\n",
      "383639/383639 [==============================] - 11s 28us/sample - loss: 0.4047 - auc: 0.7754 - val_loss: 0.4010 - val_auc: 0.7839\n",
      "Epoch 5/50\n",
      "382976/383639 [============================>.] - ETA: 0s - loss: 0.4025 - auc: 0.7788\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "383639/383639 [==============================] - 11s 28us/sample - loss: 0.4025 - auc: 0.7789 - val_loss: 0.4016 - val_auc: 0.7829\n",
      "Epoch 6/50\n",
      "383639/383639 [==============================] - 11s 28us/sample - loss: 0.3984 - auc: 0.7847 - val_loss: 0.4041 - val_auc: 0.7808\n",
      "Epoch 7/50\n",
      "381952/383639 [============================>.] - ETA: 0s - loss: 0.3974 - auc: 0.7863Restoring model weights from the end of the best epoch.\n",
      "383639/383639 [==============================] - 11s 28us/sample - loss: 0.3974 - auc: 0.7863 - val_loss: 0.4071 - val_auc: 0.7800\n",
      "Epoch 00007: early stopping\n",
      "validation AUC fold 1 : 0.78585\n",
      "Train on 383639 samples, validate on 95910 samples\n",
      "Epoch 1/50\n",
      "383639/383639 [==============================] - 18s 46us/sample - loss: 0.5094 - auc: 0.7204 - val_loss: 0.4088 - val_auc: 0.7840\n",
      "Epoch 2/50\n",
      "383639/383639 [==============================] - 11s 28us/sample - loss: 0.4108 - auc: 0.7677 - val_loss: 0.4014 - val_auc: 0.7856\n",
      "Epoch 3/50\n",
      "383639/383639 [==============================] - 11s 28us/sample - loss: 0.4061 - auc: 0.7736 - val_loss: 0.4004 - val_auc: 0.7852\n",
      "Epoch 4/50\n",
      "383639/383639 [==============================] - 11s 28us/sample - loss: 0.4041 - auc: 0.7765 - val_loss: 0.4027 - val_auc: 0.7845\n",
      "Epoch 5/50\n",
      "380928/383639 [============================>.] - ETA: 0s - loss: 0.4027 - auc: 0.7787\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "383639/383639 [==============================] - 11s 27us/sample - loss: 0.4027 - auc: 0.7786 - val_loss: 0.4013 - val_auc: 0.7831\n",
      "Epoch 6/50\n",
      "383639/383639 [==============================] - 9s 23us/sample - loss: 0.3986 - auc: 0.7846 - val_loss: 0.4019 - val_auc: 0.7820\n",
      "Epoch 7/50\n",
      "381952/383639 [============================>.] - ETA: 0s - loss: 0.3973 - auc: 0.7862Restoring model weights from the end of the best epoch.\n",
      "383639/383639 [==============================] - 9s 22us/sample - loss: 0.3974 - auc: 0.7860 - val_loss: 0.4041 - val_auc: 0.7807\n",
      "Epoch 00007: early stopping\n",
      "validation AUC fold 2 : 0.78582\n",
      "Train on 383639 samples, validate on 95910 samples\n",
      "Epoch 1/50\n",
      "383639/383639 [==============================] - 13s 34us/sample - loss: 0.5056 - auc: 0.7162 - val_loss: 0.4064 - val_auc: 0.7833\n",
      "Epoch 2/50\n",
      "383639/383639 [==============================] - 9s 24us/sample - loss: 0.4109 - auc: 0.7670 - val_loss: 0.3995 - val_auc: 0.7857\n",
      "Epoch 3/50\n",
      "383639/383639 [==============================] - 9s 22us/sample - loss: 0.4067 - auc: 0.7723 - val_loss: 0.4011 - val_auc: 0.7852\n",
      "Epoch 4/50\n",
      "383639/383639 [==============================] - 9s 23us/sample - loss: 0.4045 - auc: 0.7759 - val_loss: 0.4026 - val_auc: 0.7834\n",
      "Epoch 5/50\n",
      "381952/383639 [============================>.] - ETA: 0s - loss: 0.4027 - auc: 0.7787\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "383639/383639 [==============================] - 9s 23us/sample - loss: 0.4026 - auc: 0.7789 - val_loss: 0.4005 - val_auc: 0.7843\n",
      "Epoch 6/50\n",
      "383639/383639 [==============================] - 9s 23us/sample - loss: 0.3985 - auc: 0.7847 - val_loss: 0.4013 - val_auc: 0.7828\n",
      "Epoch 7/50\n",
      "382976/383639 [============================>.] - ETA: 0s - loss: 0.3973 - auc: 0.7864Restoring model weights from the end of the best epoch.\n",
      "383639/383639 [==============================] - 9s 23us/sample - loss: 0.3973 - auc: 0.7865 - val_loss: 0.4041 - val_auc: 0.7808\n",
      "Epoch 00007: early stopping\n",
      "validation AUC fold 3 : 0.78591\n",
      "Train on 383639 samples, validate on 95910 samples\n",
      "Epoch 1/50\n",
      "383639/383639 [==============================] - 12s 31us/sample - loss: 0.5065 - auc: 0.7217 - val_loss: 0.4057 - val_auc: 0.7820\n",
      "Epoch 2/50\n",
      "383639/383639 [==============================] - 9s 23us/sample - loss: 0.4102 - auc: 0.7682 - val_loss: 0.4010 - val_auc: 0.7827\n",
      "Epoch 3/50\n",
      "383639/383639 [==============================] - 9s 22us/sample - loss: 0.4056 - auc: 0.7745 - val_loss: 0.4005 - val_auc: 0.7822\n",
      "Epoch 4/50\n",
      "383639/383639 [==============================] - 9s 23us/sample - loss: 0.4037 - auc: 0.7770 - val_loss: 0.4013 - val_auc: 0.7813\n",
      "Epoch 5/50\n",
      "381952/383639 [============================>.] - ETA: 0s - loss: 0.4024 - auc: 0.7793\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "383639/383639 [==============================] - 9s 23us/sample - loss: 0.4024 - auc: 0.7794 - val_loss: 0.4033 - val_auc: 0.7806\n",
      "Epoch 6/50\n",
      "381952/383639 [============================>.] - ETA: 0s - loss: 0.3987 - auc: 0.7843Restoring model weights from the end of the best epoch.\n",
      "383639/383639 [==============================] - 9s 23us/sample - loss: 0.3986 - auc: 0.7843 - val_loss: 0.4033 - val_auc: 0.7793\n",
      "Epoch 00006: early stopping\n",
      "validation AUC fold 4 : 0.78179\n",
      "Train on 383640 samples, validate on 95909 samples\n",
      "Epoch 1/50\n",
      "383640/383640 [==============================] - 12s 32us/sample - loss: 0.5097 - auc: 0.7188 - val_loss: 0.4055 - val_auc: 0.7833\n",
      "Epoch 2/50\n",
      "383640/383640 [==============================] - 10s 27us/sample - loss: 0.4109 - auc: 0.7673 - val_loss: 0.4010 - val_auc: 0.7852\n",
      "Epoch 3/50\n",
      "383640/383640 [==============================] - 12s 32us/sample - loss: 0.4062 - auc: 0.7732 - val_loss: 0.3998 - val_auc: 0.7847\n",
      "Epoch 4/50\n",
      "383640/383640 [==============================] - 12s 31us/sample - loss: 0.4039 - auc: 0.7770 - val_loss: 0.4013 - val_auc: 0.7832\n",
      "Epoch 5/50\n",
      "382976/383640 [============================>.] - ETA: 0s - loss: 0.4019 - auc: 0.7795\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "383640/383640 [==============================] - 12s 32us/sample - loss: 0.4019 - auc: 0.7795 - val_loss: 0.4004 - val_auc: 0.7820\n",
      "Epoch 6/50\n",
      "383640/383640 [==============================] - 11s 29us/sample - loss: 0.3980 - auc: 0.7852 - val_loss: 0.4017 - val_auc: 0.7813\n",
      "Epoch 7/50\n",
      "381952/383640 [============================>.] - ETA: 0s - loss: 0.3970 - auc: 0.7866Restoring model weights from the end of the best epoch.\n",
      "383640/383640 [==============================] - 12s 32us/sample - loss: 0.3969 - auc: 0.7866 - val_loss: 0.4036 - val_auc: 0.7805\n",
      "Epoch 00007: early stopping\n",
      "validation AUC fold 5 : 0.78496\n"
     ]
    }
   ],
   "source": [
    "oof_pred_deepfm = run_cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF AUC : 0.78444\n"
     ]
    }
   ],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "#ln dim maxembdim 100 no BN (300, 300), 32 reg 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF AUC : 0.78476\n"
     ]
    }
   ],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "#ln dim maxembdim 100 no BN (300, 300), 128 reg 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF AUC : 0.78412\n"
     ]
    }
   ],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "#ln dim maxembdim 100 no BN (300, 300), 128 no reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* add nan_features OK\n",
    "* archi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['target']\n",
    "N_Splits = 5\n",
    "Verbose = 1\n",
    "Epochs = 50\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "DROPOUT = 0.3\n",
    "NNLAYERS = (300, 300)\n",
    "PATIENCE = 5\n",
    "\n",
    "MAX_EMB_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_enc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-1a825e55cdc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdense_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures_enc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mall_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mindicator_cols\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfeatures_enc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features_enc' is not defined"
     ]
    }
   ],
   "source": [
    "dense_features = features_enc\n",
    "all_features = features + indicator_cols + features_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features += indicator_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = features + ['nan_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(data, catcols, densecols, dnn_layers, last_dense):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    outputs_emb = []\n",
    "    \n",
    "    # Cat cols\n",
    "    for c in catcols:\n",
    "        \n",
    "        num_unique_values = int(data[c].nunique())\n",
    "        embed_dim = int(min(np.ceil(math.log(num_unique_values)), MAX_EMB_DIM))\n",
    "        inp = layers.Input(shape=(1,))\n",
    "        out = layers.Embedding(num_unique_values + 1, embed_dim, name=c)(inp)\n",
    "        out = layers.SpatialDropout1D(DROPOUT)(out)\n",
    "        out = layers.Reshape(target_shape=(embed_dim, ))(out)\n",
    "        inputs.append(inp)\n",
    "        outputs.append(out)\n",
    "        outputs_emb.append(out)\n",
    "        \n",
    "    # First dense for embeddings\n",
    "#     outputs_emb = layers.Concatenate()(outputs_emb)\n",
    "#     outputs_emb = layers.Dense(last_dense, activation=\"relu\")(outputs_emb)\n",
    "#     outputs_emb = layers.Dropout(DROPOUT)(outputs_emb)\n",
    "        \n",
    "    # Dense cols\n",
    "#     if densecols:\n",
    "#         dense_inp = layers.Input(shape=(len(densecols),))\n",
    "#         inputs.append(dense_inp)\n",
    "\n",
    "#         outputs.append(dense_inp)\n",
    "        \n",
    "    x = layers.Concatenate()(outputs)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # DNN layers\n",
    "    for size in dnn_layers:\n",
    "#         x = layers.Concatenate()([x, outputs_emb])\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(size, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(DROPOUT)(x)\n",
    "        \n",
    "#     x = layers.Concatenate()([x, outputs_emb])\n",
    "    \n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    y = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=y)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.nan_features = train.nan_features.replace({-1: 0})\n",
    "test.nan_features = test.nan_features.replace({-1: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features = sparse_features[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_cv():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    oof_pred_deepfm = np.zeros((len(train), ))\n",
    "    y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "\n",
    "    for fold, (tr_ind, val_ind) in enumerate(skf.split(train, train[target])):\n",
    "\n",
    "        # Split\n",
    "        X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "        y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "        train_model_input = [X_train.loc[:, sparse_features].values[:, k] \\\n",
    "                             for k in range(X_train.loc[:, sparse_features].values.shape[1])]\n",
    "    #     train_model_input += [X_train.loc[:, dense_features].values]\n",
    "        val_model_input = [X_val.loc[:, sparse_features].values[:, k] \\\n",
    "                           for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "    #     val_model_input += [X_val.loc[:, dense_features].values]\n",
    "        test_model_input = [test.loc[:, sparse_features].values[:, k] \\\n",
    "                            for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "    #     test_model_input += [test.loc[:, dense_features].values]\n",
    "\n",
    "\n",
    "        # Define model\n",
    "        model = create_model(data, sparse_features, ['id'], (300, 300), 256)\n",
    "        opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "\n",
    "        # Define callbacks\n",
    "        es = callbacks.EarlyStopping(\n",
    "            monitor='val_auc', \n",
    "            min_delta=0.001, \n",
    "            patience=PATIENCE, \n",
    "            verbose=Verbose, \n",
    "            mode='max', \n",
    "            baseline=None, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        #sb = callbacks.ModelCheckpoint(\n",
    "         #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "        #)\n",
    "    #     clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "    #                        step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "    #                        gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_auc', \n",
    "            mode='max',\n",
    "            factor=0.5,\n",
    "            patience=3, \n",
    "            min_lr=1e-6,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "    #     cb = TQDMNotebookCallback()\n",
    "    #     setattr(cb,'on_train_batch_begin',lambda x,y:None)\n",
    "    #     setattr(cb,'on_train_batch_end',lambda x,y:None)\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train_model_input, utils.to_categorical(y_train),\n",
    "            validation_data=(val_model_input, utils.to_categorical(y_val)),\n",
    "            batch_size=1024, \n",
    "            epochs=Epochs, \n",
    "            verbose=1,\n",
    "            callbacks=[reduce_lr, es]\n",
    "        )\n",
    "\n",
    "        # Predict\n",
    "        val_pred = model.predict(val_model_input, batch_size=512)[:, 1]\n",
    "        print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "        oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "        y_pred_deepfm += model.predict(test_model_input, batch_size=512)[:, 1].ravel() / (N_Splits)\n",
    "        K.clear_session()\n",
    "        \n",
    "    return oof_pred_deepfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 383639 samples, validate on 95910 samples\n",
      "Epoch 1/50\n",
      "383639/383639 [==============================] - 13s 33us/sample - loss: 0.4357 - auc: 0.7173 - val_loss: 0.4011 - val_auc: 0.7847\n",
      "Epoch 2/50\n",
      "383639/383639 [==============================] - 8s 22us/sample - loss: 0.4106 - auc: 0.7664 - val_loss: 0.3985 - val_auc: 0.7852\n",
      "Epoch 3/50\n",
      "383639/383639 [==============================] - 9s 24us/sample - loss: 0.4060 - auc: 0.7739 - val_loss: 0.3993 - val_auc: 0.7855\n",
      "Epoch 4/50\n",
      "383639/383639 [==============================] - 9s 23us/sample - loss: 0.4048 - auc: 0.7753 - val_loss: 0.4030 - val_auc: 0.7842\n",
      "Epoch 5/50\n",
      "383639/383639 [==============================] - 9s 24us/sample - loss: 0.4027 - auc: 0.7789 - val_loss: 0.4013 - val_auc: 0.7831\n",
      "Epoch 6/50\n",
      "382976/383639 [============================>.] - ETA: 0s - loss: 0.4009 - auc: 0.7810\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "383639/383639 [==============================] - 9s 23us/sample - loss: 0.4009 - auc: 0.7811 - val_loss: 0.4028 - val_auc: 0.7820\n",
      "Epoch 00006: early stopping\n",
      "validation AUC fold 1 : 0.78483\n",
      "Train on 383639 samples, validate on 95910 samples\n",
      "Epoch 1/50\n",
      "383639/383639 [==============================] - 13s 33us/sample - loss: 0.4338 - auc: 0.7240 - val_loss: 0.4014 - val_auc: 0.7837\n",
      "Epoch 2/50\n",
      "383639/383639 [==============================] - 9s 22us/sample - loss: 0.4103 - auc: 0.7664 - val_loss: 0.4025 - val_auc: 0.7855\n",
      "Epoch 3/50\n",
      "383639/383639 [==============================] - 9s 22us/sample - loss: 0.4062 - auc: 0.7730 - val_loss: 0.4030 - val_auc: 0.7848\n",
      "Epoch 4/50\n",
      "383639/383639 [==============================] - 9s 24us/sample - loss: 0.4044 - auc: 0.7764 - val_loss: 0.4022 - val_auc: 0.7837\n",
      "Epoch 5/50\n",
      "381952/383639 [============================>.] - ETA: 0s - loss: 0.4024 - auc: 0.7792\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "383639/383639 [==============================] - 12s 31us/sample - loss: 0.4023 - auc: 0.7791 - val_loss: 0.4029 - val_auc: 0.7823\n",
      "Epoch 6/50\n",
      "383639/383639 [==============================] - 12s 30us/sample - loss: 0.3985 - auc: 0.7843 - val_loss: 0.4046 - val_auc: 0.7818\n",
      "Epoch 7/50\n",
      "382976/383639 [============================>.] - ETA: 0s - loss: 0.3978 - auc: 0.7856Restoring model weights from the end of the best epoch.\n",
      "383639/383639 [==============================] - 12s 30us/sample - loss: 0.3978 - auc: 0.7855 - val_loss: 0.4038 - val_auc: 0.7806\n",
      "Epoch 00007: early stopping\n",
      "validation AUC fold 2 : 0.78542\n",
      "Train on 383639 samples, validate on 95910 samples\n",
      "Epoch 1/50\n",
      "383639/383639 [==============================] - 17s 43us/sample - loss: 0.4329 - auc: 0.7249 - val_loss: 0.4022 - val_auc: 0.7821\n",
      "Epoch 2/50\n",
      "383639/383639 [==============================] - 12s 32us/sample - loss: 0.4095 - auc: 0.7681 - val_loss: 0.4024 - val_auc: 0.7860\n",
      "Epoch 3/50\n",
      "383639/383639 [==============================] - 12s 32us/sample - loss: 0.4066 - auc: 0.7727 - val_loss: 0.4002 - val_auc: 0.7857\n",
      "Epoch 4/50\n",
      "383639/383639 [==============================] - 12s 30us/sample - loss: 0.4041 - auc: 0.7763 - val_loss: 0.3986 - val_auc: 0.7849\n",
      "Epoch 5/50\n",
      "381952/383639 [============================>.] - ETA: 0s - loss: 0.4026 - auc: 0.7788\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "383639/383639 [==============================] - 15s 40us/sample - loss: 0.4025 - auc: 0.7789 - val_loss: 0.4030 - val_auc: 0.7832\n",
      "Epoch 6/50\n",
      "383639/383639 [==============================] - 13s 35us/sample - loss: 0.3983 - auc: 0.7848 - val_loss: 0.4039 - val_auc: 0.7814\n",
      "Epoch 7/50\n",
      "380928/383639 [============================>.] - ETA: 0s - loss: 0.3967 - auc: 0.7871Restoring model weights from the end of the best epoch.\n",
      "383639/383639 [==============================] - 8s 20us/sample - loss: 0.3966 - auc: 0.7871 - val_loss: 0.4040 - val_auc: 0.7810\n",
      "Epoch 00007: early stopping\n",
      "validation AUC fold 3 : 0.78611\n",
      "Train on 383639 samples, validate on 95910 samples\n",
      "Epoch 1/50\n",
      "383639/383639 [==============================] - 11s 29us/sample - loss: 0.4333 - auc: 0.7219 - val_loss: 0.4025 - val_auc: 0.7819\n",
      "Epoch 2/50\n",
      "383639/383639 [==============================] - 8s 20us/sample - loss: 0.4101 - auc: 0.7673 - val_loss: 0.4008 - val_auc: 0.7839\n",
      "Epoch 3/50\n",
      "383639/383639 [==============================] - 8s 20us/sample - loss: 0.4063 - auc: 0.7738 - val_loss: 0.4029 - val_auc: 0.7826\n",
      "Epoch 4/50\n",
      "383639/383639 [==============================] - 9s 23us/sample - loss: 0.4036 - auc: 0.7774 - val_loss: 0.4010 - val_auc: 0.7830\n",
      "Epoch 5/50\n",
      "382976/383639 [============================>.] - ETA: 0s - loss: 0.4020 - auc: 0.7797\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "383639/383639 [==============================] - 9s 24us/sample - loss: 0.4020 - auc: 0.7797 - val_loss: 0.4022 - val_auc: 0.7798\n",
      "Epoch 6/50\n",
      "383639/383639 [==============================] - 9s 22us/sample - loss: 0.3982 - auc: 0.7855 - val_loss: 0.4056 - val_auc: 0.7793\n",
      "Epoch 7/50\n",
      "381952/383639 [============================>.] - ETA: 0s - loss: 0.3972 - auc: 0.7868Restoring model weights from the end of the best epoch.\n",
      "383639/383639 [==============================] - 8s 20us/sample - loss: 0.3973 - auc: 0.7866 - val_loss: 0.4071 - val_auc: 0.7779\n",
      "Epoch 00007: early stopping\n",
      "validation AUC fold 4 : 0.78375\n",
      "Train on 383640 samples, validate on 95909 samples\n",
      "Epoch 1/50\n",
      "383640/383640 [==============================] - 11s 28us/sample - loss: 0.4343 - auc: 0.7222 - val_loss: 0.4021 - val_auc: 0.7835\n",
      "Epoch 2/50\n",
      "383640/383640 [==============================] - 8s 20us/sample - loss: 0.4097 - auc: 0.7675 - val_loss: 0.3986 - val_auc: 0.7857\n",
      "Epoch 3/50\n",
      "383640/383640 [==============================] - 8s 21us/sample - loss: 0.4066 - auc: 0.7725 - val_loss: 0.3998 - val_auc: 0.7846\n",
      "Epoch 4/50\n",
      "383640/383640 [==============================] - 8s 20us/sample - loss: 0.4041 - auc: 0.7766 - val_loss: 0.4008 - val_auc: 0.7842\n",
      "Epoch 5/50\n",
      "381952/383640 [============================>.] - ETA: 0s - loss: 0.4030 - auc: 0.7779\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "383640/383640 [==============================] - 8s 20us/sample - loss: 0.4030 - auc: 0.7778 - val_loss: 0.3997 - val_auc: 0.7835\n",
      "Epoch 6/50\n",
      "383640/383640 [==============================] - 8s 21us/sample - loss: 0.3990 - auc: 0.7836 - val_loss: 0.4038 - val_auc: 0.7819\n",
      "Epoch 7/50\n",
      "381952/383640 [============================>.] - ETA: 0s - loss: 0.3976 - auc: 0.7856Restoring model weights from the end of the best epoch.\n",
      "383640/383640 [==============================] - 8s 20us/sample - loss: 0.3975 - auc: 0.7857 - val_loss: 0.4036 - val_auc: 0.7811\n",
      "Epoch 00007: early stopping\n",
      "validation AUC fold 5 : 0.7856\n"
     ]
    }
   ],
   "source": [
    "oof_pred_deepfm = run_cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF AUC : 0.78435\n"
     ]
    }
   ],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "#ln dim maxembdim 100 no BN (300, 300) nan features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['target']\n",
    "N_Splits = 5\n",
    "Verbose = 1\n",
    "Epochs = 50\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "DROPOUT = 0.2\n",
    "NNLAYERS = (256, 256, 256)\n",
    "PATIENCE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "oof_pred_deepfm = np.zeros((len(train), ))\n",
    "y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "\n",
    "for fold, (tr_ind, val_ind) in enumerate(skf.split(train, train[target])):\n",
    "    \n",
    "    # Split\n",
    "    X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "    y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "    train_model_input = {name:X_train[name] for name in feature_names}\n",
    "    val_model_input = {name:X_val[name] for name in feature_names}\n",
    "    test_model_input = {name:test[name] for name in feature_names}\n",
    "    \n",
    "    # Define model\n",
    "    model = DeepFM(sparse_feature_columns, sparse_feature_columns + dense_feature_columns,\n",
    "                   dnn_hidden_units=NNLAYERS, dnn_dropout=DROPOUT, dnn_use_bn=False, task='binary')\n",
    "    opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "    \n",
    "    # Define callbacks\n",
    "    es = callbacks.EarlyStopping(\n",
    "        monitor='val_auc', \n",
    "        min_delta=0.0, \n",
    "        patience=PATIENCE, \n",
    "        verbose=Verbose, \n",
    "        mode='max', \n",
    "        baseline=None, \n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    #sb = callbacks.ModelCheckpoint(\n",
    "     #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "    #)\n",
    "#     clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "#                        step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "#                        gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_auc', \n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=3, \n",
    "        min_lr=1e-7,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_model_input, y_train,\n",
    "        validation_data=(val_model_input, y_val),\n",
    "        batch_size=BATCH_SIZE, \n",
    "        epochs=Epochs, \n",
    "        verbose=Verbose,\n",
    "        callbacks=[reduce_lr, es]\n",
    "    )\n",
    "    \n",
    "    # Predict\n",
    "    val_pred = model.predict(val_model_input, batch_size=512)\n",
    "    print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "    oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "    y_pred_deepfm += model.predict(test_model_input, batch_size=512).ravel() / (N_Splits)\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Deeper\n",
    "* decrease LR\n",
    "* back to cyclic lr\n",
    "* decrease patience reduceLRplateau\n",
    "* add reg for DNN\n",
    "* dropout levels\n",
    "* treat emb dim\n",
    "* nunique + 1 ?\n",
    "* pseudo label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "grid_params = {\n",
    "    'dnn_dropout': [0.0, 0.1, 0.2, 0.3, 0.5],\n",
    "    'dnn_hidden_units': [(256,256, 256), (256, 256), (128, 128, 128), (256, 256, 256, 256)],\n",
    "    'emb_dim': [2, 3, 4, 8],\n",
    "    'lr': [1e-3, 1e-4],\n",
    "    'cyclic': [True, False],\n",
    "    'bn': [True, False],\n",
    "    'l2_reg_linear': [1e-05, 1e-04],\n",
    "    'l2_reg_embedding': [1e-05, 1e-04], \n",
    "    'l2_reg_dnn': [0.0, 1e-05, 1e-04]    \n",
    "}\n",
    "list_params = list(ParameterSampler(grid_params,\n",
    "                                    n_iter=50,\n",
    "                                    random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "\n",
    "model = DeepFM(sparse_feature_columns, sparse_feature_columns + dense_feature_columns,\n",
    "               dnn_hidden_units=param['dnn_hidden_units'], dnn_dropout=param['dnn_dropout'], \n",
    "               dnn_use_bn=param['bn'], task='binary',\n",
    "               l2_reg_linear=param['l2_reg_linear'], l2_reg_embedding=param['l2_reg_embedding'], \n",
    "               l2_reg_dnn=param['l2_reg_dnn'], init_std=0.0001,\n",
    "               seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [{'lr': 0.0001,\n",
    "  'l2_reg_linear': 0.0001,\n",
    "  'l2_reg_embedding': 1e-05,\n",
    "  'l2_reg_dnn': 0.0001,\n",
    "  'emb_dim': 4,\n",
    "  'dnn_hidden_units': (256, 256, 256),\n",
    "  'dnn_dropout': 0.0,\n",
    "  'cyclic': False,\n",
    "  'bn': False}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_perf = []\n",
    "\n",
    "for param in tqdm(params):\n",
    "    \n",
    "    oof_pred_deepfm = np.zeros((len(train), ))\n",
    "    y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    sparse_feature_columns = [SparseFeat(feat, train[feat].nunique() + 1, embedding_dim=param['emb_dim']) \n",
    "                          for feat in features]\n",
    "    dense_feature_columns = [DenseFeat(feat, 1) for feat in features_enc + indicator_cols]\n",
    "\n",
    "    feature_names = get_feature_names(sparse_feature_columns + dense_feature_columns)\n",
    "\n",
    "    all_features = features + features_enc + indicator_cols\n",
    "\n",
    "    for fold, (tr_ind, val_ind) in enumerate(skf.split(train, train[target])):\n",
    "\n",
    "        # Split\n",
    "        X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "        y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "        train_model_input = {name:X_train[name] for name in feature_names}\n",
    "        val_model_input = {name:X_val[name] for name in feature_names}\n",
    "        test_model_input = {name:test[name] for name in feature_names}\n",
    "\n",
    "        # Define model\n",
    "        model = DeepFM(sparse_feature_columns, sparse_feature_columns + dense_feature_columns,\n",
    "                       dnn_hidden_units=param['dnn_hidden_units'], dnn_dropout=param['dnn_dropout'], \n",
    "                       dnn_use_bn=param['bn'], task='binary',\n",
    "                       l2_reg_linear=param['l2_reg_linear'], l2_reg_embedding=param['l2_reg_embedding'], \n",
    "                       l2_reg_dnn=param['l2_reg_dnn'], init_std=0.0001,\n",
    "                       seed=SEED)\n",
    "        opt = keras.optimizers.Adam(learning_rate=param['lr'])\n",
    "        model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "\n",
    "        # Define callbacks\n",
    "        es = callbacks.EarlyStopping(\n",
    "            monitor='val_auc', \n",
    "            min_delta=0.0, \n",
    "            patience=PATIENCE, \n",
    "            verbose=Verbose, \n",
    "            mode='max', \n",
    "            baseline=None, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        #sb = callbacks.ModelCheckpoint(\n",
    "         #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "        #)\n",
    "        if param['cyclic']:\n",
    "            reduce_lr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "                           step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "                           gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "        else:\n",
    "            reduce_lr = ReduceLROnPlateau(\n",
    "                monitor='val_auc', \n",
    "                mode='max',\n",
    "                factor=0.5,\n",
    "                patience=3, \n",
    "                min_lr=1e-7,\n",
    "                verbose=True,\n",
    "            )\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train_model_input, y_train,\n",
    "            validation_data=(val_model_input, y_val),\n",
    "            batch_size=BATCH_SIZE, \n",
    "            epochs=Epochs, \n",
    "            verbose=Verbose,\n",
    "            callbacks=[reduce_lr, es]\n",
    "        )\n",
    "\n",
    "        # Predict\n",
    "        val_pred = model.predict(val_model_input, batch_size=512)\n",
    "        print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "        oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "        y_pred_deepfm += model.predict(test_model_input, batch_size=512).ravel() / (N_Splits)\n",
    "        K.clear_session()\n",
    "    cv_perf.append(round(roc_auc_score(train.target.values, oof_pred_deepfm), 5))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_ord = [feat for feat in features if 'ord' in feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_feature_columns = [SparseFeat(feat, train[feat].nunique() + 1, embedding_dim=4) \n",
    "                          for feat in features if 'ord' not in feat]\n",
    "dense_feature_columns = [DenseFeat(feat, 1) for feat in features_enc + indicator_cols + features_ord]\n",
    "\n",
    "feature_names = get_feature_names(sparse_feature_columns + dense_feature_columns)\n",
    "\n",
    "all_features = features + features_enc + indicator_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_perf = []\n",
    "\n",
    "for param in tqdm(list_params[:10]):\n",
    "    \n",
    "    oof_pred_deepfm = np.zeros((len(train), ))\n",
    "    y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    sparse_feature_columns = [SparseFeat(feat, train[feat].nunique() + 1, embedding_dim=param['emb_dim']) \n",
    "                          for feat in features]\n",
    "    dense_feature_columns = [DenseFeat(feat, 1) for feat in features_enc + indicator_cols]\n",
    "\n",
    "    feature_names = get_feature_names(sparse_feature_columns + dense_feature_columns)\n",
    "\n",
    "    all_features = features + features_enc + indicator_cols\n",
    "\n",
    "    for fold, (tr_ind, val_ind) in enumerate(skf.split(train, train[target])):\n",
    "\n",
    "        # Split\n",
    "        X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "        y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "        train_model_input = {name:X_train[name] for name in feature_names}\n",
    "        val_model_input = {name:X_val[name] for name in feature_names}\n",
    "        test_model_input = {name:test[name] for name in feature_names}\n",
    "\n",
    "        # Define model\n",
    "        model = DeepFM(sparse_feature_columns, sparse_feature_columns + dense_feature_columns,\n",
    "                       dnn_hidden_units=param['dnn_hidden_units'], dnn_dropout=param['dnn_dropout'], \n",
    "                       dnn_use_bn=param['bn'], task='binary',\n",
    "                       l2_reg_linear=param['l2_reg_linear'], l2_reg_embedding=param['l2_reg_embedding'], \n",
    "                       l2_reg_dnn=param['l2_reg_dnn'], init_std=0.0001,\n",
    "                       seed=SEED)\n",
    "        opt = keras.optimizers.Adam(learning_rate=param['lr'])\n",
    "        model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "\n",
    "        # Define callbacks\n",
    "        es = callbacks.EarlyStopping(\n",
    "            monitor='val_auc', \n",
    "            min_delta=0.0, \n",
    "            patience=PATIENCE, \n",
    "            verbose=Verbose, \n",
    "            mode='max', \n",
    "            baseline=None, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        #sb = callbacks.ModelCheckpoint(\n",
    "         #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "        #)\n",
    "        if param['cyclic']:\n",
    "            reduce_lr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "                           step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "                           gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "        else:\n",
    "            reduce_lr = ReduceLROnPlateau(\n",
    "                monitor='val_auc', \n",
    "                mode='max',\n",
    "                factor=0.5,\n",
    "                patience=3, \n",
    "                min_lr=1e-7,\n",
    "                verbose=True,\n",
    "            )\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train_model_input, y_train,\n",
    "            validation_data=(val_model_input, y_val),\n",
    "            batch_size=BATCH_SIZE, \n",
    "            epochs=Epochs, \n",
    "            verbose=Verbose,\n",
    "            callbacks=[reduce_lr, es]\n",
    "        )\n",
    "\n",
    "        # Predict\n",
    "        val_pred = model.predict(val_model_input, batch_size=512)\n",
    "        print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "        oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "        y_pred_deepfm += model.predict(test_model_input, batch_size=512).ravel() / (N_Splits)\n",
    "        K.clear_session()\n",
    "    cv_perf.append(round(roc_auc_score(train.target.values, oof_pred_deepfm), 5))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['target']\n",
    "N_Splits = 10\n",
    "Verbose = 1\n",
    "Epochs = 50\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "grid_params = {\n",
    "    'dnn_dropout': [0.0, 0.1, 0.2],\n",
    "    'dnn_hidden_units': [(256,), (512,), (256, 256), (512, 512)],\n",
    "    'linear': [linear_feature_columns, linear_feature_columns + dnn_feature_columns],\n",
    "    'sparse': [dnn_feature_columns, linear_feature_columns + dnn_feature_columns],\n",
    "    'batch_size': [128, 256, 512],\n",
    "}\n",
    "list_params = list(ParameterSampler(grid_params,\n",
    "                                    n_iter=20,\n",
    "                                    random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_perf = []\n",
    "\n",
    "for param in list_params:\n",
    "\n",
    "    oof_pred_deepfm = np.zeros((len(train), ))\n",
    "    y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "    for fold, (tr_ind, val_ind) in enumerate(skf.split(train, train[target])):\n",
    "        X_train, X_val = train[features+features_enc+indicator_cols].iloc[tr_ind], train[features+features_enc+indicator_cols].iloc[val_ind]\n",
    "        y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "        train_model_input = {name:X_train[name] for name in feature_names}\n",
    "        val_model_input = {name:X_val[name] for name in feature_names}\n",
    "        test_model_input = {name:test[name] for name in feature_names}\n",
    "        model = DeepFM(param['linear'], param['sparse'],\n",
    "                       dnn_hidden_units=param['dnn_hidden_units'], dnn_dropout=param['dnn_dropout'], dnn_use_bn=False, task='binary')\n",
    "        model.compile(\"adam\", \"binary_crossentropy\", metrics=[auc], )\n",
    "        es = callbacks.EarlyStopping(monitor='val_auc', min_delta=0.001, patience=4, verbose=Verbose, mode='max', baseline=None, restore_best_weights=True)\n",
    "        sb = callbacks.ModelCheckpoint('./nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose)\n",
    "        clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "                           step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "                           gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "        history = model.fit(train_model_input, y_train,\n",
    "                            validation_data=(val_model_input, y_val),\n",
    "                            batch_size=param['batch_size'], epochs=Epochs, verbose=Verbose,\n",
    "                            callbacks=[es, sb, clr],)\n",
    "        model.load_weights('./nn_model.w8')\n",
    "        val_pred = model.predict(val_model_input, batch_size=param['batch_size'])\n",
    "        print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "        oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "        y_pred_deepfm += model.predict(test_model_input, batch_size=param['batch_size']).ravel() / (N_Splits)\n",
    "        K.clear_session()\n",
    "    cv_perf.append(round(roc_auc_score(train.target.values, oof_pred_deepfm), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_params[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV 50 best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['target']\n",
    "N_Splits = 50\n",
    "Verbose = 1\n",
    "Epochs = 50\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_pred_deepfm = np.zeros((len(train), ))\n",
    "y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "\n",
    "for fold, (tr_ind, val_ind) in enumerate(skf.split(train, train[target])):\n",
    "    \n",
    "    # Split\n",
    "    X_train, X_val = train[features+features_enc+indicator_cols].iloc[tr_ind], train[features+features_enc+indicator_cols].iloc[val_ind]\n",
    "    y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "    train_model_input = {name:X_train[name] for name in feature_names}\n",
    "    val_model_input = {name:X_val[name] for name in feature_names}\n",
    "    test_model_input = {name:test[name] for name in feature_names}\n",
    "    \n",
    "    # Define model\n",
    "    model = DeepFM(linear_feature_columns, linear_feature_columns + dnn_feature_columns,\n",
    "                   dnn_hidden_units=(512, 512), dnn_dropout=0.0, dnn_use_bn=False, task='binary')\n",
    "    model.compile(\"adam\", \"binary_crossentropy\", metrics=[auc])\n",
    "    \n",
    "    # Define callbacks\n",
    "    es = callbacks.EarlyStopping(monitor='val_auc', min_delta=0.001, patience=4, verbose=Verbose, mode='max', baseline=None, restore_best_weights=True)\n",
    "    sb = callbacks.ModelCheckpoint('./nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose)\n",
    "#     clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "#                        step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "#                        gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_auc', factor=0.2,\n",
    "                                  patience=5, min_lr=0.00001)\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(train_model_input, y_train,\n",
    "                        validation_data=(val_model_input, y_val),\n",
    "                        batch_size=BATCH_SIZE, epochs=Epochs, verbose=Verbose,\n",
    "                        callbacks=[es, sb, reduce_lr],)\n",
    "    model.load_weights('./nn_model.w8')\n",
    "    \n",
    "    # Predict\n",
    "    val_pred = model.predict(val_model_input, batch_size=512)\n",
    "    print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "    oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "    y_pred_deepfm += model.predict(test_model_input, batch_size=512).ravel() / (N_Splits)\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = test.id.values\n",
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test_idx,\n",
    "    'target': y_pred_deepfm\n",
    "})\n",
    "submission.to_csv(\"submission_deepfm_cv_50.csv\", index=False)\n",
    "print(\"Submission file saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('oof_pred_deepfm_cv_50.npy',oof_pred_deepfm)\n",
    "np.save('y_pred_deepfm_cv_50.npy',    y_pred_deepfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "sandbox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
