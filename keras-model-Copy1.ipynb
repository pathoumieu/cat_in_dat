{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 666"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import MissingIndicator, SimpleImputer, IterativeImputer, KNNImputer\n",
    "from category_encoders.cat_boost import CatBoostEncoder\n",
    "from deepctr.inputs import  SparseFeat, DenseFeat, get_feature_names\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam,RMSprop\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "import gc\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from deepctr.models import DeepFM\n",
    "import tensorflow.keras as keras\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import re\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import utils\n",
    "import math\n",
    "from sklearn.metrics import precision_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('cat-in-the-dat-ii/train.csv')\n",
    "test = pd.read_csv('cat-in-the-dat-ii/test.csv')\n",
    "\n",
    "test[\"target\"] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_features = ['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4']\n",
    "nom_features = ['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']\n",
    "ord_features = ['ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5']\n",
    "other_features = ['day', 'month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train, test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features = [feat for feat in train.columns if feat not in ['id', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['nan_feature'] = data[sparse_features].isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_features += ['nan_feature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features = bin_features + nom_features + other_features\n",
    "dense_features = ord_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label encode and fillna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_numeric(df):\n",
    "    \n",
    "    bin_3_mapping = {'T':1 , 'F':0}\n",
    "    bin_4_mapping = {'Y':1 , 'N':0}\n",
    "    nom_0_mapping = {'Red' : 0, 'Blue' : 1, 'Green' : 2}\n",
    "    nom_1_mapping = {'Trapezoid' : 0, 'Star' : 1, 'Circle': 2, 'Triangle' : 3, 'Polygon' : 4}\n",
    "    nom_2_mapping = {'Hamster' : 0 , 'Axolotl' : 1, 'Lion' : 2, 'Dog' : 3, 'Cat' : 4, 'Snake' : 5}\n",
    "    nom_3_mapping = {'Russia' : 0, 'Canada' : 1, 'Finland' : 2, 'Costa Rica' : 3, 'China' : 4, 'India' : 5}\n",
    "    nom_4_mapping = {'Bassoon' : 0, 'Theremin' : 1, 'Oboe' : 2, 'Piano' : 3}\n",
    "    nom_5_mapping = dict(zip((df.nom_5.dropna().unique()), range(len((df.nom_5.dropna().unique())))))\n",
    "    nom_6_mapping = dict(zip((df.nom_6.dropna().unique()), range(len((df.nom_6.dropna().unique())))))\n",
    "    nom_7_mapping = dict(zip((df.nom_7.dropna().unique()), range(len((df.nom_7.dropna().unique())))))\n",
    "    nom_8_mapping = dict(zip((df.nom_8.dropna().unique()), range(len((df.nom_8.dropna().unique())))))\n",
    "    nom_9_mapping = dict(zip((df.nom_9.dropna().unique()), range(len((df.nom_9.dropna().unique())))))\n",
    "    ord_1_mapping = {'Novice' : 0, 'Contributor' : 1, 'Expert' : 2, 'Master': 3, 'Grandmaster': 4}\n",
    "    ord_2_mapping = { 'Freezing': 0, 'Cold': 1, 'Warm' : 2, 'Hot': 3, 'Boiling Hot' : 4, 'Lava Hot' : 5}\n",
    "    ord_3_mapping = {'a':0, 'b':1, 'c':2 ,'d':3 ,'e':4, 'f':5, 'g':6, 'h':7, 'i':8, 'j':9, 'k':10, 'l':11, 'm':12, 'n':13, 'o':14}\n",
    "    ord_4_mapping = {'A':0, 'B':1, 'C':2, 'D':3, 'E':4, 'F':5, 'G':6, 'H':7, 'I':8, 'J':9, 'K':10,'L':11,'M':12,\n",
    "                 'N':13,'O':14,'P':15,'Q':16,'R':17,'S':18,'T':19,'U':20,'V':21,'W':22,'X':23,'Y':24,'Z':25}\n",
    "    sorted_ord_5 = sorted(df.ord_5.dropna().unique())\n",
    "    ord_5_mapping = dict(zip(sorted_ord_5, range(len(sorted_ord_5))))\n",
    "\n",
    "    df['bin_3'] = df.loc[df.bin_3.notnull(), 'bin_3'].map(bin_3_mapping)\n",
    "    df['bin_4'] = df.loc[df.bin_4.notnull(), 'bin_4'].map(bin_4_mapping)\n",
    "    df['nom_0'] = df.loc[df.nom_0.notnull(), 'nom_0'].map(nom_0_mapping)\n",
    "    df['nom_1'] = df.loc[df.nom_1.notnull(), 'nom_1'].map(nom_1_mapping)\n",
    "    df['nom_2'] = df.loc[df.nom_2.notnull(), 'nom_2'].map(nom_2_mapping)\n",
    "    df['nom_3'] = df.loc[df.nom_3.notnull(), 'nom_3'].map(nom_3_mapping)\n",
    "    df['nom_4'] = df.loc[df.nom_4.notnull(), 'nom_4'].map(nom_4_mapping)\n",
    "    df['nom_5'] = df.loc[df.nom_5.notnull(), 'nom_5'].map(nom_5_mapping)\n",
    "    df['nom_6'] = df.loc[df.nom_6.notnull(), 'nom_6'].map(nom_6_mapping)\n",
    "    df['nom_7'] = df.loc[df.nom_7.notnull(), 'nom_7'].map(nom_7_mapping)\n",
    "    df['nom_8'] = df.loc[df.nom_8.notnull(), 'nom_8'].map(nom_8_mapping)\n",
    "    df['nom_9'] = df.loc[df.nom_9.notnull(), 'nom_9'].map(nom_9_mapping)\n",
    "    df['ord_1'] = df.loc[df.ord_1.notnull(), 'ord_1'].map(ord_1_mapping)\n",
    "    df['ord_2'] = df.loc[df.ord_2.notnull(), 'ord_2'].map(ord_2_mapping)\n",
    "    df['ord_3'] = df.loc[df.ord_3.notnull(), 'ord_3'].map(ord_3_mapping)\n",
    "    df['ord_4'] = df.loc[df.ord_4.notnull(), 'ord_4'].map(ord_4_mapping)\n",
    "    df['ord_5'] = df.loc[df.ord_5.notnull(), 'ord_5'].map(ord_5_mapping)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define sparse features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NaN count\n",
    "# data['nan_count'] = data.isnull().sum(axis=1) Problem with nan count\n",
    "\n",
    "features = [feat for feat in data.columns if feat not in ['id', 'target']]#, 'nan_feature']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep categories present in train AND test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[data.target != -1]\n",
    "test = data[data.target == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 values in nom_5, {'b3ad70fcb'} Replaced with nan\n",
      "4 values in nom_6, {'a885aacec', 'ee6983c6d', '3a121fefb', 'f0732a795'} Replaced with nan\n",
      "2 values in nom_9, {'1065f10dd', '3d19cd31d'} Replaced with nan\n"
     ]
    }
   ],
   "source": [
    "for col in features:\n",
    "    train_unique_values = set(train[col].dropna().unique())\n",
    "    test_unique_values  = set(test[col].dropna().unique())\n",
    "\n",
    "    symmetric_difference_values = train_unique_values.symmetric_difference(test_unique_values)\n",
    "    if symmetric_difference_values:\n",
    "        print(f'{len(symmetric_difference_values)} values in {col}, {symmetric_difference_values} Replaced with nan')\n",
    "        data.loc[data[col].isin(symmetric_difference_values), col] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fillna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "features = [feat for feat in data.columns if feat not in ['target','id']]\n",
    "data[features] = convert_data_to_numeric(data[features])\n",
    "data[features] = data[features].astype('category')\n",
    "imp = IterativeImputer(max_iter=500, initial_strategy='most_frequent', random_state=SEED, add_indicator=True)\n",
    "indicator_cols = [feat + '_ind' for feat in features]\n",
    "for col in indicator_cols:\n",
    "    data[col] = 0\n",
    "data[features+indicator_cols] = imp.fit_transform(data[features])\n",
    "data[features] = data[features].round(0).astype(np.int16)\n",
    "data[indicator_cols] = data[indicator_cols].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[features] = convert_data_to_numeric(data[features])\n",
    "data = data.fillna(-1)\n",
    "data[features] = data[features] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[data.target != -1].reset_index(drop=True)\n",
    "test  = data[data.target == -1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard scale dense_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std = StandardScaler()\n",
    "\n",
    "train[dense_features] = std.fit_transform(train[dense_features])\n",
    "test[dense_features] = std.transform(test[dense_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(y_true, y_pred):\n",
    "    def fallback_auc(y_true, y_pred):\n",
    "        try:\n",
    "            return roc_auc_score(y_true, y_pred)\n",
    "        except:\n",
    "            return 0.5\n",
    "    return tf.py_function(fallback_auc, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicLR(keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1 / (2. ** (x - 1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma ** (x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "\n",
    "    def clr(self):\n",
    "        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n",
    "        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x)) * self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x)) * self.scale_fn(\n",
    "                self.clr_iterations)\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "\n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['target']\n",
    "N_Splits = 5\n",
    "Verbose = 1\n",
    "Epochs = 50\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "DROPOUT = 0.3\n",
    "NNLAYERS = (300, 300)\n",
    "PATIENCE = 5\n",
    "\n",
    "MAX_EMB_DIM = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(data, catcols, densecols, max_emb_dim, dnn_layers, last_dense):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    outputs_emb = []\n",
    "    \n",
    "    # Cat cols\n",
    "    for c in catcols:\n",
    "        \n",
    "        num_unique_values = int(data[c].nunique())\n",
    "        embed_dim = int(min(np.ceil(num_unique_values*0.5), max_emb_dim))\n",
    "        inp = layers.Input(shape=(1,))\n",
    "        out = layers.Embedding(\n",
    "            num_unique_values + 1, \n",
    "            embed_dim, \n",
    "            name=c, \n",
    "#             activity_regularizer=regularizers.l1(0.01)\n",
    "        )(inp)\n",
    "        out = layers.SpatialDropout1D(DROPOUT)(out)\n",
    "        out = layers.Reshape(target_shape=(embed_dim, ))(out)\n",
    "        inputs.append(inp)\n",
    "        outputs.append(out)\n",
    "        \n",
    "    # First dense for embeddings\n",
    "#     outputs_emb = layers.Concatenate()(outputs_emb)\n",
    "#     outputs_emb = layers.Dense(last_dense, activation=\"relu\")(outputs_emb)\n",
    "#     outputs_emb = layers.Dropout(DROPOUT)(outputs_emb)\n",
    "        \n",
    "    # Dense cols\n",
    "    if densecols:\n",
    "        dense_inp = layers.Input(shape=(len(densecols),))\n",
    "        inputs.append(dense_inp)\n",
    "        outputs.append(dense_inp)\n",
    "        \n",
    "    x = layers.Concatenate()(outputs)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # DNN layers\n",
    "    for size in dnn_layers:\n",
    "#         x = layers.Concatenate()([x, outputs_emb])\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(size, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(DROPOUT)(x)\n",
    "        \n",
    "#     x = layers.Concatenate()([x, outputs_emb])\n",
    "    \n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    y = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=y)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_features = dense_features + sparse_features + ['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = features + ['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_cv(\n",
    "    sparse_features, \n",
    "    dense_features, \n",
    "    max_emb_dim, \n",
    "    nn_layers=(300, 300), \n",
    "    last_dense=256, \n",
    "    n_splits=N_Splits,\n",
    "    verbose=Verbose\n",
    "):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    random.seed(SEED)\n",
    "\n",
    "    tf.random.set_seed(SEED)\n",
    "    \n",
    "    oof_pred_deepfm = np.zeros((len(train), ))\n",
    "    y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    all_features = sparse_features + dense_features + ['target']\n",
    "\n",
    "    for fold, (tr_ind, val_ind) in enumerate(skf.split(train, train[target])):\n",
    "\n",
    "        # Split\n",
    "        X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "        y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "        \n",
    "        train_model_input = [X_train.loc[:, sparse_features].values[:, k] \\\n",
    "                             for k in range(X_train.loc[:, sparse_features].values.shape[1])]\n",
    "        if dense_features:\n",
    "            train_model_input += [X_train.loc[:, dense_features].values]\n",
    "        \n",
    "        val_model_input = [X_val.loc[:, sparse_features].values[:, k] \\\n",
    "                           for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "        if dense_features:\n",
    "            val_model_input += [X_val.loc[:, dense_features].values]\n",
    "        \n",
    "        test_model_input = [test.loc[:, sparse_features].values[:, k] \\\n",
    "                            for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "        if dense_features:\n",
    "            test_model_input += [test.loc[:, dense_features].values]\n",
    "\n",
    "\n",
    "        # Define model\n",
    "        model = create_model(data, sparse_features, dense_features, max_emb_dim, nn_layers, last_dense)\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "\n",
    "        # Define callbacks\n",
    "        es = callbacks.EarlyStopping(\n",
    "            monitor='val_auc', \n",
    "            min_delta=0.001, \n",
    "            patience=PATIENCE, \n",
    "            verbose=verbose, \n",
    "            mode='max', \n",
    "            baseline=None, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        #sb = callbacks.ModelCheckpoint(\n",
    "         #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "        #)\n",
    "    #     clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "    #                        step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "    #                        gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_auc', \n",
    "            mode='max',\n",
    "            factor=0.5,\n",
    "            patience=3, \n",
    "            min_lr=1e-6,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "    #     cb = TQDMNotebookCallback()\n",
    "    #     setattr(cb,'on_train_batch_begin',lambda x,y:None)\n",
    "    #     setattr(cb,'on_train_batch_end',lambda x,y:None)\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train_model_input, utils.to_categorical(y_train),\n",
    "            validation_data=(val_model_input, utils.to_categorical(y_val)),\n",
    "            batch_size=1024, \n",
    "            epochs=Epochs, \n",
    "            verbose=verbose,\n",
    "            callbacks=[reduce_lr, es]\n",
    "        )\n",
    "\n",
    "        # Predict\n",
    "        val_pred = model.predict(val_model_input, batch_size=512)[:, 1]\n",
    "        print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "        oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "        y_pred_deepfm += model.predict(test_model_input, batch_size=512)[:, 1].ravel() / (n_splits)\n",
    "        K.clear_session()\n",
    "        gc.collect()\n",
    "    gc.collect()        \n",
    "    print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "        \n",
    "    return oof_pred_deepfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 480000 samples, validate on 120000 samples\n",
      "Epoch 1/50\n",
      "480000/480000 [==============================] - 23s 48us/sample - loss: 0.4223 - auc: 0.7425 - val_loss: 0.3983 - val_auc: 0.7859\n",
      "Epoch 2/50\n",
      "480000/480000 [==============================] - 21s 44us/sample - loss: 0.4019 - auc: 0.7796 - val_loss: 0.3997 - val_auc: 0.7840\n",
      "Epoch 3/50\n",
      "480000/480000 [==============================] - 22s 45us/sample - loss: 0.3978 - auc: 0.7849 - val_loss: 0.4013 - val_auc: 0.7838\n",
      "Epoch 4/50\n",
      "479232/480000 [============================>.] - ETA: 0s - loss: 0.3928 - auc: 0.7917\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "480000/480000 [==============================] - 17s 36us/sample - loss: 0.3928 - auc: 0.7916 - val_loss: 0.4025 - val_auc: 0.7819\n",
      "Epoch 5/50\n",
      "480000/480000 [==============================] - 17s 36us/sample - loss: 0.3807 - auc: 0.8070 - val_loss: 0.4106 - val_auc: 0.7770\n",
      "Epoch 6/50\n",
      "478208/480000 [============================>.] - ETA: 0s - loss: 0.3719 - auc: 0.8180Restoring model weights from the end of the best epoch.\n",
      "480000/480000 [==============================] - 17s 36us/sample - loss: 0.3719 - auc: 0.8180 - val_loss: 0.4271 - val_auc: 0.7703\n",
      "Epoch 00006: early stopping\n",
      "validation AUC fold 1 : 0.78594\n",
      "Train on 480000 samples, validate on 120000 samples\n",
      "Epoch 1/50\n",
      "480000/480000 [==============================] - 21s 43us/sample - loss: 0.4215 - auc: 0.7448 - val_loss: 0.4001 - val_auc: 0.7847\n",
      "Epoch 2/50\n",
      "480000/480000 [==============================] - 17s 35us/sample - loss: 0.4024 - auc: 0.7793 - val_loss: 0.3979 - val_auc: 0.7852\n",
      "Epoch 3/50\n",
      "480000/480000 [==============================] - 17s 35us/sample - loss: 0.3978 - auc: 0.7854 - val_loss: 0.3999 - val_auc: 0.7840\n",
      "Epoch 4/50\n",
      "480000/480000 [==============================] - 17s 35us/sample - loss: 0.3930 - auc: 0.7919 - val_loss: 0.4032 - val_auc: 0.7817\n",
      "Epoch 5/50\n",
      "478208/480000 [============================>.] - ETA: 0s - loss: 0.3861 - auc: 0.8004\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "480000/480000 [==============================] - 17s 35us/sample - loss: 0.3862 - auc: 0.8004 - val_loss: 0.4074 - val_auc: 0.7752\n",
      "Epoch 6/50\n",
      "478208/480000 [============================>.] - ETA: 0s - loss: 0.3704 - auc: 0.8201Restoring model weights from the end of the best epoch.\n",
      "480000/480000 [==============================] - 17s 35us/sample - loss: 0.3703 - auc: 0.8201 - val_loss: 0.4213 - val_auc: 0.7678\n",
      "Epoch 00006: early stopping\n",
      "validation AUC fold 2 : 0.78456\n",
      "Train on 480000 samples, validate on 120000 samples\n",
      "Epoch 1/50\n",
      "480000/480000 [==============================] - 21s 43us/sample - loss: 0.4224 - auc: 0.7442 - val_loss: 0.3998 - val_auc: 0.7828\n",
      "Epoch 2/50\n",
      "480000/480000 [==============================] - 17s 35us/sample - loss: 0.4019 - auc: 0.7798 - val_loss: 0.3999 - val_auc: 0.7838\n",
      "Epoch 3/50\n",
      "480000/480000 [==============================] - 17s 35us/sample - loss: 0.3974 - auc: 0.7857 - val_loss: 0.4001 - val_auc: 0.7830\n",
      "Epoch 4/50\n",
      "480000/480000 [==============================] - 17s 36us/sample - loss: 0.3925 - auc: 0.7927 - val_loss: 0.4053 - val_auc: 0.7799\n",
      "Epoch 5/50\n",
      "479232/480000 [============================>.] - ETA: 0s - loss: 0.3856 - auc: 0.8017\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "480000/480000 [==============================] - 17s 36us/sample - loss: 0.3856 - auc: 0.8017 - val_loss: 0.4081 - val_auc: 0.7755\n",
      "Epoch 6/50\n",
      "480000/480000 [==============================] - 17s 35us/sample - loss: 0.3703 - auc: 0.8206 - val_loss: 0.4206 - val_auc: 0.7661\n",
      "Epoch 7/50\n",
      "479232/480000 [============================>.] - ETA: 0s - loss: 0.3610 - auc: 0.8315Restoring model weights from the end of the best epoch.\n",
      "480000/480000 [==============================] - 19s 40us/sample - loss: 0.3610 - auc: 0.8314 - val_loss: 0.4316 - val_auc: 0.7603\n",
      "Epoch 00007: early stopping\n",
      "validation AUC fold 3 : 0.78395\n",
      "Train on 480000 samples, validate on 120000 samples\n",
      "Epoch 1/50\n",
      "480000/480000 [==============================] - 21s 43us/sample - loss: 0.4230 - auc: 0.7420 - val_loss: 0.4008 - val_auc: 0.7841\n",
      "Epoch 2/50\n",
      "480000/480000 [==============================] - 17s 36us/sample - loss: 0.4019 - auc: 0.7794 - val_loss: 0.3998 - val_auc: 0.7831\n",
      "Epoch 3/50\n",
      "480000/480000 [==============================] - 17s 35us/sample - loss: 0.3978 - auc: 0.7851 - val_loss: 0.4006 - val_auc: 0.7824\n",
      "Epoch 4/50\n",
      "478208/480000 [============================>.] - ETA: 0s - loss: 0.3924 - auc: 0.7927\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "480000/480000 [==============================] - 17s 35us/sample - loss: 0.3924 - auc: 0.7926 - val_loss: 0.4028 - val_auc: 0.7799\n",
      "Epoch 5/50\n",
      "480000/480000 [==============================] - 17s 35us/sample - loss: 0.3800 - auc: 0.8085 - val_loss: 0.4094 - val_auc: 0.7726\n",
      "Epoch 6/50\n",
      "478208/480000 [============================>.] - ETA: 0s - loss: 0.3713 - auc: 0.8191Restoring model weights from the end of the best epoch.\n",
      "480000/480000 [==============================] - 17s 36us/sample - loss: 0.3713 - auc: 0.8191 - val_loss: 0.4207 - val_auc: 0.7666\n",
      "Epoch 00006: early stopping\n",
      "validation AUC fold 4 : 0.78367\n",
      "Train on 480000 samples, validate on 120000 samples\n",
      "Epoch 1/50\n",
      "480000/480000 [==============================] - 21s 43us/sample - loss: 0.4211 - auc: 0.7464 - val_loss: 0.4003 - val_auc: 0.7840\n",
      "Epoch 2/50\n",
      "480000/480000 [==============================] - 17s 35us/sample - loss: 0.4025 - auc: 0.7788 - val_loss: 0.4000 - val_auc: 0.7834\n",
      "Epoch 3/50\n",
      "480000/480000 [==============================] - 17s 36us/sample - loss: 0.3975 - auc: 0.7859 - val_loss: 0.4017 - val_auc: 0.7827\n",
      "Epoch 4/50\n",
      "479232/480000 [============================>.] - ETA: 0s - loss: 0.3928 - auc: 0.7919\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "480000/480000 [==============================] - 17s 35us/sample - loss: 0.3928 - auc: 0.7919 - val_loss: 0.4050 - val_auc: 0.7801\n",
      "Epoch 5/50\n",
      "480000/480000 [==============================] - 17s 35us/sample - loss: 0.3802 - auc: 0.8081 - val_loss: 0.4097 - val_auc: 0.7746\n",
      "Epoch 6/50\n",
      "478208/480000 [============================>.] - ETA: 0s - loss: 0.3711 - auc: 0.8191Restoring model weights from the end of the best epoch.\n",
      "480000/480000 [==============================] - 17s 35us/sample - loss: 0.3712 - auc: 0.8190 - val_loss: 0.4183 - val_auc: 0.7680\n",
      "Epoch 00006: early stopping\n",
      "validation AUC fold 5 : 0.78412\n",
      "OOF AUC : 0.7843\n"
     ]
    }
   ],
   "source": [
    "oof_pred_deepfm = run_cv(sparse_features=sparse_features, dense_features=[], max_emb_dim=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "* Max emb dim 50 = 100 OOF AUC : 0.78486 (ln(dim))\n",
    "* Max emb dim 50 OOF AUC : 0.7851 (dim*0.5)\n",
    "* Max emb dim 100 OOF AUC : 0.7851 (dim*0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "oof_pred_deepfm = run_cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "# Max emb dim 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "# Max emb dim 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "# Max emb dim 100 std scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "# Max emb dim 50 std scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* fillna median + std scale ord (not nan feature) + create indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "#ln dim maxembdim 50 no BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "#ln dim maxembdim 20 no BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "#ln dim maxembdim 200 no BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "#ln dim maxembdim 100 no BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "# No BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.7848 - 0.7852"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* test with nan_count\n",
    "* add dense features OK\n",
    "* test with ln instead of *0.5 OK\n",
    "* increase MAX_EMB_DIM OK\n",
    "* archi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['target']\n",
    "N_Splits = 5\n",
    "Verbose = 1\n",
    "Epochs = 50\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "DROPOUT = 0.3\n",
    "NNLAYERS = (300, 300)\n",
    "PATIENCE = 5\n",
    "\n",
    "MAX_EMB_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dense_features = features_enc\n",
    "all_features = features + features_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(data, catcols, densecols, dnn_layers, last_dense):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    dense_out = []\n",
    "    \n",
    "    # Cat cols\n",
    "    for c in catcols:\n",
    "        \n",
    "        num_unique_values = int(data[c].nunique())\n",
    "        embed_dim = int(min(np.ceil(math.log(num_unique_values)), MAX_EMB_DIM))\n",
    "        inp = layers.Input(shape=(1,))\n",
    "        out = layers.Embedding(num_unique_values + 1, embed_dim, name=c)(inp)\n",
    "        out = layers.SpatialDropout1D(DROPOUT)(out)\n",
    "        out = layers.Reshape(target_shape=(embed_dim, ))(out)\n",
    "        inputs.append(inp)\n",
    "        outputs.append(out)\n",
    "        \n",
    "#     First dense for embeddings\n",
    "#     outputs_emb = layers.Concatenate()(outputs_emb)\n",
    "#     outputs_emb = layers.Dense(last_dense, activation=\"relu\")(outputs_emb)\n",
    "#     outputs_emb = layers.Dropout(DROPOUT)(outputs_emb)\n",
    "\n",
    "#     outputs_emb = layers.Concatenate()(outputs_emb)\n",
    "#     outputs_emb = layers.Dense(last_dense, activation=\"relu\")(outputs_emb)\n",
    "#     outputs_emb = layers.Dropout(DROPOUT)(outputs_emb)\n",
    "        \n",
    "    # Dense cols\n",
    "    if densecols:\n",
    "        dense_inp = layers.Input(shape=(len(densecols),))\n",
    "        inputs.append(dense_inp)\n",
    "\n",
    "    dense_inp = layers.Dense(\n",
    "        last_dense, \n",
    "        activation=\"relu\",\n",
    "        kernel_regularizer=keras.regularizers.l2(0.01)\n",
    "    )(dense_inp)\n",
    "    dense_inp = layers.Dropout(0.5)(dense_inp)\n",
    "        \n",
    "    x = layers.Concatenate()(outputs)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # DNN layers\n",
    "    for size in dnn_layers:\n",
    "#         x = layers.Concatenate()([x, outputs_emb])\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(size, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(DROPOUT)(x)\n",
    "        \n",
    "    x = layers.Concatenate()([x, dense_inp])\n",
    "    \n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    y = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=y)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_cv():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    oof_pred_deepfm = np.zeros((len(train), ))\n",
    "    y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "\n",
    "    for fold, (tr_ind, val_ind) in enumerate(skf.split(train, train[target])):\n",
    "\n",
    "        # Split\n",
    "        X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "        y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "        train_model_input = [X_train.loc[:, sparse_features].values[:, k] \\\n",
    "                             for k in range(X_train.loc[:, sparse_features].values.shape[1])]\n",
    "        train_model_input += [X_train.loc[:, dense_features].values]\n",
    "        val_model_input = [X_val.loc[:, sparse_features].values[:, k] \\\n",
    "                           for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "        val_model_input += [X_val.loc[:, dense_features].values]\n",
    "        test_model_input = [test.loc[:, sparse_features].values[:, k] \\\n",
    "                            for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "        test_model_input += [test.loc[:, dense_features].values]\n",
    "\n",
    "\n",
    "        # Define model\n",
    "        model = create_model(data, sparse_features, dense_features, (300, 300), 32)\n",
    "        opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "\n",
    "        # Define callbacks\n",
    "        es = callbacks.EarlyStopping(\n",
    "            monitor='val_auc', \n",
    "            min_delta=0.001, \n",
    "            patience=PATIENCE, \n",
    "            verbose=Verbose, \n",
    "            mode='max', \n",
    "            baseline=None, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        #sb = callbacks.ModelCheckpoint(\n",
    "         #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "        #)\n",
    "    #     clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "    #                        step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "    #                        gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_auc', \n",
    "            mode='max',\n",
    "            factor=0.5,\n",
    "            patience=3, \n",
    "            min_lr=1e-6,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "    #     cb = TQDMNotebookCallback()\n",
    "    #     setattr(cb,'on_train_batch_begin',lambda x,y:None)\n",
    "    #     setattr(cb,'on_train_batch_end',lambda x,y:None)\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train_model_input, utils.to_categorical(y_train),\n",
    "            validation_data=(val_model_input, utils.to_categorical(y_val)),\n",
    "            batch_size=1024, \n",
    "            epochs=Epochs, \n",
    "            verbose=1,\n",
    "            callbacks=[reduce_lr, es]\n",
    "        )\n",
    "\n",
    "        # Predict\n",
    "        val_pred = model.predict(val_model_input, batch_size=512)[:, 1]\n",
    "        print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "        oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "        y_pred_deepfm += model.predict(test_model_input, batch_size=512)[:, 1].ravel() / (N_Splits)\n",
    "        K.clear_session()\n",
    "        \n",
    "    return oof_pred_deepfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_pred_deepfm = run_cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "#ln dim maxembdim 100 no BN (300, 300), 32 reg 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "#ln dim maxembdim 100 no BN (300, 300), 128 reg 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "#ln dim maxembdim 100 no BN (300, 300), 128 no reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* add nan_features OK\n",
    "* archi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['target']\n",
    "N_Splits = 5\n",
    "Verbose = 1\n",
    "Epochs = 50\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "DROPOUT = 0.3\n",
    "NNLAYERS = (300, 300)\n",
    "PATIENCE = 5\n",
    "\n",
    "MAX_EMB_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dense_features = features_enc\n",
    "all_features = features + indicator_cols + features_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features += indicator_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = features + ['nan_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(data, catcols, densecols, dnn_layers, last_dense):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    outputs_emb = []\n",
    "    \n",
    "    # Cat cols\n",
    "    for c in catcols:\n",
    "        \n",
    "        num_unique_values = int(data[c].nunique())\n",
    "        embed_dim = int(min(np.ceil(math.log(num_unique_values)), MAX_EMB_DIM))\n",
    "        inp = layers.Input(shape=(1,))\n",
    "        out = layers.Embedding(num_unique_values + 1, embed_dim, name=c)(inp)\n",
    "        out = layers.SpatialDropout1D(DROPOUT)(out)\n",
    "        out = layers.Reshape(target_shape=(embed_dim, ))(out)\n",
    "        inputs.append(inp)\n",
    "        outputs.append(out)\n",
    "        outputs_emb.append(out)\n",
    "        \n",
    "    # First dense for embeddings\n",
    "#     outputs_emb = layers.Concatenate()(outputs_emb)\n",
    "#     outputs_emb = layers.Dense(last_dense, activation=\"relu\")(outputs_emb)\n",
    "#     outputs_emb = layers.Dropout(DROPOUT)(outputs_emb)\n",
    "        \n",
    "    # Dense cols\n",
    "#     if densecols:\n",
    "#         dense_inp = layers.Input(shape=(len(densecols),))\n",
    "#         inputs.append(dense_inp)\n",
    "\n",
    "#         outputs.append(dense_inp)\n",
    "        \n",
    "    x = layers.Concatenate()(outputs)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # DNN layers\n",
    "    for size in dnn_layers:\n",
    "#         x = layers.Concatenate()([x, outputs_emb])\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(size, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(DROPOUT)(x)\n",
    "        \n",
    "#     x = layers.Concatenate()([x, outputs_emb])\n",
    "    \n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    y = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=y)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.nan_features = train.nan_features.replace({-1: 0})\n",
    "test.nan_features = test.nan_features.replace({-1: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features = sparse_features[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_cv():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    oof_pred_deepfm = np.zeros((len(train), ))\n",
    "    y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "\n",
    "    for fold, (tr_ind, val_ind) in enumerate(skf.split(train, train[target])):\n",
    "\n",
    "        # Split\n",
    "        X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "        y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "        train_model_input = [X_train.loc[:, sparse_features].values[:, k] \\\n",
    "                             for k in range(X_train.loc[:, sparse_features].values.shape[1])]\n",
    "    #     train_model_input += [X_train.loc[:, dense_features].values]\n",
    "        val_model_input = [X_val.loc[:, sparse_features].values[:, k] \\\n",
    "                           for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "    #     val_model_input += [X_val.loc[:, dense_features].values]\n",
    "        test_model_input = [test.loc[:, sparse_features].values[:, k] \\\n",
    "                            for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "    #     test_model_input += [test.loc[:, dense_features].values]\n",
    "\n",
    "\n",
    "        # Define model\n",
    "        model = create_model(data, sparse_features, ['id'], (300, 300), 256)\n",
    "        opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "\n",
    "        # Define callbacks\n",
    "        es = callbacks.EarlyStopping(\n",
    "            monitor='val_auc', \n",
    "            min_delta=0.001, \n",
    "            patience=PATIENCE, \n",
    "            verbose=Verbose, \n",
    "            mode='max', \n",
    "            baseline=None, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        #sb = callbacks.ModelCheckpoint(\n",
    "         #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "        #)\n",
    "    #     clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "    #                        step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "    #                        gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_auc', \n",
    "            mode='max',\n",
    "            factor=0.5,\n",
    "            patience=3, \n",
    "            min_lr=1e-6,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "    #     cb = TQDMNotebookCallback()\n",
    "    #     setattr(cb,'on_train_batch_begin',lambda x,y:None)\n",
    "    #     setattr(cb,'on_train_batch_end',lambda x,y:None)\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train_model_input, utils.to_categorical(y_train),\n",
    "            validation_data=(val_model_input, utils.to_categorical(y_val)),\n",
    "            batch_size=1024, \n",
    "            epochs=Epochs, \n",
    "            verbose=1,\n",
    "            callbacks=[reduce_lr, es]\n",
    "        )\n",
    "\n",
    "        # Predict\n",
    "        val_pred = model.predict(val_model_input, batch_size=512)[:, 1]\n",
    "        print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "        oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "        y_pred_deepfm += model.predict(test_model_input, batch_size=512)[:, 1].ravel() / (N_Splits)\n",
    "        K.clear_session()\n",
    "        \n",
    "    return oof_pred_deepfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "oof_pred_deepfm = run_cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "#ln dim maxembdim 100 no BN (300, 300) nan features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['target']\n",
    "N_Splits = 50\n",
    "Verbose = 0\n",
    "Epochs = 50\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "DROPOUT = 0.3\n",
    "NNLAYERS = (300, 300)\n",
    "PATIENCE = 5\n",
    "\n",
    "MAX_EMB_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(data, catcols, densecols, dnn_layers, last_dense):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    outputs_emb = []\n",
    "    \n",
    "    # Cat cols\n",
    "    for c in catcols:\n",
    "        \n",
    "        num_unique_values = int(data[c].nunique())\n",
    "        embed_dim = int(min(np.ceil(math.log(num_unique_values)), MAX_EMB_DIM))\n",
    "        inp = layers.Input(shape=(1,))\n",
    "        out = layers.Embedding(num_unique_values + 1, embed_dim, name=c)(inp)\n",
    "        out = layers.SpatialDropout1D(DROPOUT)(out)\n",
    "        out = layers.Reshape(target_shape=(embed_dim, ))(out)\n",
    "        inputs.append(inp)\n",
    "        outputs.append(out)\n",
    "        outputs_emb.append(out)\n",
    "        \n",
    "    # First dense for embeddings\n",
    "#     outputs_emb = layers.Concatenate()(outputs_emb)\n",
    "#     outputs_emb = layers.Dense(last_dense, activation=\"relu\")(outputs_emb)\n",
    "#     outputs_emb = layers.Dropout(DROPOUT)(outputs_emb)\n",
    "        \n",
    "    # Dense cols\n",
    "#     if densecols:\n",
    "#         dense_inp = layers.Input(shape=(len(densecols),))\n",
    "#         inputs.append(dense_inp)\n",
    "\n",
    "#         outputs.append(dense_inp)\n",
    "        \n",
    "    x = layers.Concatenate()(outputs)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # DNN layers\n",
    "    for size in dnn_layers:\n",
    "#         x = layers.Concatenate()([x, outputs_emb])\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(size, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(DROPOUT)(x)\n",
    "        \n",
    "#     x = layers.Concatenate()([x, outputs_emb])\n",
    "    \n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    y = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=y)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "oof_pred_deepfm = np.zeros((len(train), ))\n",
    "y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "\n",
    "for fold, (tr_ind, val_ind) in tqdm(enumerate(skf.split(train, train[target]))):\n",
    "\n",
    "    # Split\n",
    "    X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "    y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "    train_model_input = [X_train.loc[:, sparse_features].values[:, k] \\\n",
    "                         for k in range(X_train.loc[:, sparse_features].values.shape[1])]\n",
    "#     train_model_input += [X_train.loc[:, dense_features].values]\n",
    "    val_model_input = [X_val.loc[:, sparse_features].values[:, k] \\\n",
    "                       for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "#     val_model_input += [X_val.loc[:, dense_features].values]\n",
    "    test_model_input = [test.loc[:, sparse_features].values[:, k] \\\n",
    "                        for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "#     test_model_input += [test.loc[:, dense_features].values]\n",
    "\n",
    "\n",
    "    # Define model\n",
    "    model = create_model(data, sparse_features, ['id'], (300, 300), 256)\n",
    "    opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "\n",
    "    # Define callbacks\n",
    "    es = callbacks.EarlyStopping(\n",
    "        monitor='val_auc', \n",
    "        min_delta=0.001, \n",
    "        patience=PATIENCE, \n",
    "        verbose=Verbose, \n",
    "        mode='max', \n",
    "        baseline=None, \n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    #sb = callbacks.ModelCheckpoint(\n",
    "     #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "    #)\n",
    "#     clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "#                        step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "#                        gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_auc', \n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=3, \n",
    "        min_lr=1e-6,\n",
    "        verbose=Verbose,\n",
    "    )\n",
    "\n",
    "#     cb = TQDMNotebookCallback()\n",
    "#     setattr(cb,'on_train_batch_begin',lambda x,y:None)\n",
    "#     setattr(cb,'on_train_batch_end',lambda x,y:None)\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_model_input, utils.to_categorical(y_train),\n",
    "        validation_data=(val_model_input, utils.to_categorical(y_val)),\n",
    "        batch_size=1024, \n",
    "        epochs=Epochs, \n",
    "        verbose=Verbose,\n",
    "        callbacks=[reduce_lr, es]\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    val_pred = model.predict(val_model_input, batch_size=512)[:, 1]\n",
    "    print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "    oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "    y_pred_deepfm += model.predict(test_model_input, batch_size=512)[:, 1].ravel() / (N_Splits)\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = test.id.values\n",
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test_idx,\n",
    "    'target': y_pred_deepfm\n",
    "})\n",
    "submission.to_csv(\"submission_kerasemb_cv_50.csv\", index=False)\n",
    "print(\"Submission file saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('oof_pred_kerasemb_cv_50.npy',oof_pred_deepfm)\n",
    "np.save('y_pred_kerasemb_cv_50.npy',    y_pred_deepfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_pred_deepfm = np.load('oof_pred_kerasemb_cv_50.npy')\n",
    "y_pred_deepfm = np.load('y_pred_kerasemb_cv_50.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percs, bins = np.histogram(oof_pred_deepfm, bins=500)\n",
    "precs = [precision_score(train.target.values, (oof_pred_deepfm > thresh).astype(int)) for thresh in bins]\n",
    "pops = (percs[::-1].cumsum() / 600000)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create traces\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=pops, y=precs[:-1],\n",
    "                    mode='lines',\n",
    "                    name='lines'))\n",
    "#fig.add_trace(go.Scatter(y=bins[:-1], x=precs[:-1],\n",
    " #                   mode='lines',\n",
    "  #                  name='lines'))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percs, bins = np.histogram(1 - oof_pred_deepfm, bins=500)\n",
    "precs = [precision_score(1 - train.target.values, ((1 - oof_pred_deepfm) > thresh).astype(int)) for thresh in bins]\n",
    "\n",
    "pops = (percs[::-1].cumsum() / 600000)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create traces\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=pops, y=precs[:-1],\n",
    "                    mode='lines',\n",
    "                    name='lines'))\n",
    "#fig.add_trace(go.Scatter(y=bins[:-1], x=precs[:-1],\n",
    " #                   mode='lines',\n",
    "  #                  name='lines'))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(oof_pred_deepfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pseudo_index(oof_pred, y_pred, pos_frac=0.01, neg_frac=0.08):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    idx_neg = int(neg_frac * len(oof_pred))\n",
    "    neg_thresh = np.sort(oof_pred)[idx_neg]\n",
    "    \n",
    "    idx_pos = int(pos_frac * len(oof_pred))\n",
    "    pos_thresh = np.sort(oof_pred)[::-1][idx_pos]\n",
    "    \n",
    "    neg_indices = np.argwhere(y_pred < neg_thresh)\n",
    "    pos_indices = np.argwhere(y_pred > pos_thresh)\n",
    "    \n",
    "    print(f'Selected {len(pos_indices) * 100. / len(y_pred)}% ({len(pos_indices)}) of test as 1, for threshold {pos_thresh}')\n",
    "    print(f'Selected {len(neg_indices) * 100. / len(y_pred)}% ({len(neg_indices)}) of test as 0, for threshold {neg_thresh}')\n",
    "    \n",
    "    return pos_indices.flatten(), neg_indices.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_ind, neg_ind = get_pseudo_index(oof_pred_deepfm, y_pred_deepfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV 50 pseudo labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['target']\n",
    "N_Splits = 50\n",
    "Verbose = 0\n",
    "Epochs = 50\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "DROPOUT = 0.3\n",
    "NNLAYERS = (300, 300)\n",
    "PATIENCE = 5\n",
    "\n",
    "MAX_EMB_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(data, catcols, densecols, dnn_layers, last_dense):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    outputs_emb = []\n",
    "    \n",
    "    # Cat cols\n",
    "    for c in catcols:\n",
    "        \n",
    "        num_unique_values = int(data[c].nunique())\n",
    "        embed_dim = int(min(np.ceil(math.log(num_unique_values)), MAX_EMB_DIM))\n",
    "        inp = layers.Input(shape=(1,))\n",
    "        out = layers.Embedding(num_unique_values + 1, embed_dim, name=c)(inp)\n",
    "        out = layers.SpatialDropout1D(DROPOUT)(out)\n",
    "        out = layers.Reshape(target_shape=(embed_dim, ))(out)\n",
    "        inputs.append(inp)\n",
    "        outputs.append(out)\n",
    "        outputs_emb.append(out)\n",
    "        \n",
    "    # First dense for embeddings\n",
    "#     outputs_emb = layers.Concatenate()(outputs_emb)\n",
    "#     outputs_emb = layers.Dense(last_dense, activation=\"relu\")(outputs_emb)\n",
    "#     outputs_emb = layers.Dropout(DROPOUT)(outputs_emb)\n",
    "        \n",
    "    # Dense cols\n",
    "#     if densecols:\n",
    "#         dense_inp = layers.Input(shape=(len(densecols),))\n",
    "#         inputs.append(dense_inp)\n",
    "\n",
    "#         outputs.append(dense_inp)\n",
    "        \n",
    "    x = layers.Concatenate()(outputs)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # DNN layers\n",
    "    for size in dnn_layers:\n",
    "#         x = layers.Concatenate()([x, outputs_emb])\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(size, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(DROPOUT)(x)\n",
    "        \n",
    "#     x = layers.Concatenate()([x, outputs_emb])\n",
    "    \n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    y = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=y)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bak = train.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = test.iloc[pos_ind]\n",
    "pos_df.target = 1\n",
    "\n",
    "neg_df = test.iloc[neg_ind]\n",
    "neg_df.target = 0\n",
    "\n",
    "selected_test = pd.concat([pos_df, neg_df], axis=0).sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train, selected_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_pred_deepfm = np.zeros((len(train), ))\n",
    "y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "for fold, (tr_ind, val_ind) in tqdm(enumerate(skf.split(train, train[target]))):\n",
    "\n",
    "    # Split\n",
    "    X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "    y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "    train_model_input = [X_train.loc[:, sparse_features].values[:, k] \\\n",
    "                         for k in range(X_train.loc[:, sparse_features].values.shape[1])]\n",
    "#     train_model_input += [X_train.loc[:, dense_features].values]\n",
    "    val_model_input = [X_val.loc[:, sparse_features].values[:, k] \\\n",
    "                       for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "#     val_model_input += [X_val.loc[:, dense_features].values]\n",
    "    test_model_input = [test.loc[:, sparse_features].values[:, k] \\\n",
    "                        for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "#     test_model_input += [test.loc[:, dense_features].values]\n",
    "\n",
    "\n",
    "    # Define model\n",
    "    model = create_model(data, sparse_features, ['id'], (300, 300), 256)\n",
    "    opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "\n",
    "    # Define callbacks\n",
    "    es = callbacks.EarlyStopping(\n",
    "        monitor='val_auc', \n",
    "        min_delta=0.001, \n",
    "        patience=PATIENCE, \n",
    "        verbose=Verbose, \n",
    "        mode='max', \n",
    "        baseline=None, \n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    #sb = callbacks.ModelCheckpoint(\n",
    "     #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "    #)\n",
    "#     clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "#                        step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "#                        gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_auc', \n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=3, \n",
    "        min_lr=1e-6,\n",
    "        verbose=Verbose,\n",
    "    )\n",
    "\n",
    "#     cb = TQDMNotebookCallback()\n",
    "#     setattr(cb,'on_train_batch_begin',lambda x,y:None)\n",
    "#     setattr(cb,'on_train_batch_end',lambda x,y:None)\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_model_input, utils.to_categorical(y_train),\n",
    "        validation_data=(val_model_input, utils.to_categorical(y_val)),\n",
    "        batch_size=1024, \n",
    "        epochs=Epochs, \n",
    "        verbose=Verbose,\n",
    "        callbacks=[reduce_lr, es]\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    val_pred = model.predict(val_model_input, batch_size=512)[:, 1]\n",
    "    print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "    oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "    y_pred_deepfm += model.predict(test_model_input, batch_size=512)[:, 1].ravel() / (N_Splits)\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values[:600000], oof_pred_deepfm[:600000]), 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = test.id.values\n",
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test_idx,\n",
    "    'target': y_pred_deepfm\n",
    "})\n",
    "submission.to_csv(\"submission_kerasemb_pseudo_cv_50.csv\", index=False)\n",
    "print(\"Submission file saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('oof_pred_kerasemb_pseudo_cv_50.npy',oof_pred_deepfm)\n",
    "np.save('y_pred_kerasemb_pseudo_cv_50.npy',    y_pred_deepfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* weight loss\n",
    "* pseudo labelling with only pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_ind, neg_ind = get_pseudo_index(oof_pred_deepfm, y_pred_deepfm, pos_frac=0.005, neg_frac=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = test.iloc[pos_ind]\n",
    "pos_df.target = 1\n",
    "\n",
    "neg_df = test.iloc[neg_ind]\n",
    "neg_df.target = 0\n",
    "\n",
    "selected_test = pd.concat([pos_df, neg_df], axis=0).sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train_bak\n",
    "                   , selected_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bak.target.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_pred_deepfm = np.zeros((len(train), ))\n",
    "y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "for fold, (tr_ind, val_ind) in tqdm(enumerate(skf.split(train, train[target]))):\n",
    "\n",
    "    # Split\n",
    "    X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "    y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "    train_model_input = [X_train.loc[:, sparse_features].values[:, k] \\\n",
    "                         for k in range(X_train.loc[:, sparse_features].values.shape[1])]\n",
    "#     train_model_input += [X_train.loc[:, dense_features].values]\n",
    "    val_model_input = [X_val.loc[:, sparse_features].values[:, k] \\\n",
    "                       for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "#     val_model_input += [X_val.loc[:, dense_features].values]\n",
    "    test_model_input = [test.loc[:, sparse_features].values[:, k] \\\n",
    "                        for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "#     test_model_input += [test.loc[:, dense_features].values]\n",
    "\n",
    "\n",
    "    # Define model\n",
    "    model = create_model(data, sparse_features, ['id'], (300, 300), 256)\n",
    "    opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "\n",
    "    # Define callbacks\n",
    "    es = callbacks.EarlyStopping(\n",
    "        monitor='val_auc', \n",
    "        min_delta=0.001, \n",
    "        patience=PATIENCE, \n",
    "        verbose=Verbose, \n",
    "        mode='max', \n",
    "        baseline=None, \n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    #sb = callbacks.ModelCheckpoint(\n",
    "     #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "    #)\n",
    "#     clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "#                        step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "#                        gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_auc', \n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=3, \n",
    "        min_lr=1e-6,\n",
    "        verbose=Verbose,\n",
    "    )\n",
    "\n",
    "#     cb = TQDMNotebookCallback()\n",
    "#     setattr(cb,'on_train_batch_begin',lambda x,y:None)\n",
    "#     setattr(cb,'on_train_batch_end',lambda x,y:None)\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_model_input, utils.to_categorical(y_train),\n",
    "        validation_data=(val_model_input, utils.to_categorical(y_val)),\n",
    "        batch_size=1024, \n",
    "        epochs=Epochs, \n",
    "        verbose=Verbose,\n",
    "        callbacks=[reduce_lr, es]\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    val_pred = model.predict(val_model_input, batch_size=512)[:, 1]\n",
    "    print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "    oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "    y_pred_deepfm += model.predict(test_model_input, batch_size=512)[:, 1].ravel() / (N_Splits)\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values[:600000], oof_pred_deepfm[:600000]), 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = test.id.values\n",
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test_idx,\n",
    "    'target': y_pred_deepfm\n",
    "})\n",
    "submission.to_csv(\"submission_kerasemb_pseudo005_cv_50.csv\", index=False)\n",
    "print(\"Submission file saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('oof_pred_kerasemb_pseudo005_cv_50.npy',oof_pred_deepfm)\n",
    "np.save('y_pred_kerasemb_pseudo005_cv_50.npy',    y_pred_deepfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_bak.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_pred_deepfm = np.zeros((len(train), ))\n",
    "y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "for fold, (tr_ind, val_ind) in tqdm(enumerate(skf.split(train, train[target]))):\n",
    "\n",
    "    # Split\n",
    "    X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "    y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "    train_model_input = [X_train.loc[:, sparse_features].values[:, k] \\\n",
    "                         for k in range(X_train.loc[:, sparse_features].values.shape[1])]\n",
    "#     train_model_input += [X_train.loc[:, dense_features].values]\n",
    "    val_model_input = [X_val.loc[:, sparse_features].values[:, k] \\\n",
    "                       for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "#     val_model_input += [X_val.loc[:, dense_features].values]\n",
    "    test_model_input = [test.loc[:, sparse_features].values[:, k] \\\n",
    "                        for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "#     test_model_input += [test.loc[:, dense_features].values]\n",
    "\n",
    "\n",
    "    # Define model\n",
    "    model = create_model(data, sparse_features, ['id'], (300, 300), 256)\n",
    "    opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "\n",
    "    # Define callbacks\n",
    "    es = callbacks.EarlyStopping(\n",
    "        monitor='val_auc', \n",
    "        min_delta=0.001, \n",
    "        patience=PATIENCE, \n",
    "        verbose=Verbose, \n",
    "        mode='max', \n",
    "        baseline=None, \n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    #sb = callbacks.ModelCheckpoint(\n",
    "     #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "    #)\n",
    "#     clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "#                        step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "#                        gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_auc', \n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=3, \n",
    "        min_lr=1e-6,\n",
    "        verbose=Verbose,\n",
    "    )\n",
    "\n",
    "#     cb = TQDMNotebookCallback()\n",
    "#     setattr(cb,'on_train_batch_begin',lambda x,y:None)\n",
    "#     setattr(cb,'on_train_batch_end',lambda x,y:None)\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_model_input, utils.to_categorical(y_train),\n",
    "        validation_data=(val_model_input, utils.to_categorical(y_val)),\n",
    "        batch_size=1024, \n",
    "        epochs=Epochs, \n",
    "        verbose=Verbose,\n",
    "        callbacks=[reduce_lr, es],\n",
    "        class_weight={0: 1, 1: 5.3417},\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    val_pred = model.predict(val_model_input, batch_size=512)[:, 1]\n",
    "    print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "    oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "    y_pred_deepfm += model.predict(test_model_input, batch_size=512)[:, 1].ravel() / (N_Splits)\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values[:600000], oof_pred_deepfm[:600000]), 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = test.id.values\n",
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test_idx,\n",
    "    'target': y_pred_deepfm\n",
    "})\n",
    "submission.to_csv(\"submission_kerasemb_weighted_cv_50.csv\", index=False)\n",
    "print(\"Submission file saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('oof_pred_kerasemb_weighted_cv_50.npy',oof_pred_deepfm)\n",
    "np.save('y_pred_kerasemb_weighted_cv_50.npy',    y_pred_deepfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['target']\n",
    "N_Splits = 5\n",
    "Verbose = 1\n",
    "Epochs = 50\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "DROPOUT = 0.2\n",
    "NNLAYERS = (256, 256, 256)\n",
    "PATIENCE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "oof_pred_deepfm = np.zeros((len(train), ))\n",
    "y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "\n",
    "for fold, (tr_ind, val_ind) in enumerate(skf.split(train, train[target])):\n",
    "    \n",
    "    # Split\n",
    "    X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "    y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "    train_model_input = {name:X_train[name] for name in feature_names}\n",
    "    val_model_input = {name:X_val[name] for name in feature_names}\n",
    "    test_model_input = {name:test[name] for name in feature_names}\n",
    "    \n",
    "    # Define model\n",
    "    model = DeepFM(sparse_feature_columns, sparse_feature_columns + dense_feature_columns,\n",
    "                   dnn_hidden_units=NNLAYERS, dnn_dropout=DROPOUT, dnn_use_bn=False, task='binary')\n",
    "    opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "    \n",
    "    # Define callbacks\n",
    "    es = callbacks.EarlyStopping(\n",
    "        monitor='val_auc', \n",
    "        min_delta=0.0, \n",
    "        patience=PATIENCE, \n",
    "        verbose=Verbose, \n",
    "        mode='max', \n",
    "        baseline=None, \n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    #sb = callbacks.ModelCheckpoint(\n",
    "     #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "    #)\n",
    "#     clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "#                        step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "#                        gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_auc', \n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=3, \n",
    "        min_lr=1e-7,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_model_input, y_train,\n",
    "        validation_data=(val_model_input, y_val),\n",
    "        batch_size=BATCH_SIZE, \n",
    "        epochs=Epochs, \n",
    "        verbose=Verbose,\n",
    "        callbacks=[reduce_lr, es]\n",
    "    )\n",
    "    \n",
    "    # Predict\n",
    "    val_pred = model.predict(val_model_input, batch_size=512)\n",
    "    print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "    oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "    y_pred_deepfm += model.predict(test_model_input, batch_size=512).ravel() / (N_Splits)\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Deeper\n",
    "* decrease LR\n",
    "* back to cyclic lr\n",
    "* decrease patience reduceLRplateau\n",
    "* add reg for DNN\n",
    "* dropout levels\n",
    "* treat emb dim\n",
    "* nunique + 1 ?\n",
    "* pseudo label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "grid_params = {\n",
    "    'dnn_dropout': [0.0, 0.1, 0.2, 0.3, 0.5],\n",
    "    'dnn_hidden_units': [(256,256, 256), (256, 256), (128, 128, 128), (256, 256, 256, 256)],\n",
    "    'emb_dim': [2, 3, 4, 8],\n",
    "    'lr': [1e-3, 1e-4],\n",
    "    'cyclic': [True, False],\n",
    "    'bn': [True, False],\n",
    "    'l2_reg_linear': [1e-05, 1e-04],\n",
    "    'l2_reg_embedding': [1e-05, 1e-04], \n",
    "    'l2_reg_dnn': [0.0, 1e-05, 1e-04]    \n",
    "}\n",
    "list_params = list(ParameterSampler(grid_params,\n",
    "                                    n_iter=50,\n",
    "                                    random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "\n",
    "model = DeepFM(sparse_feature_columns, sparse_feature_columns + dense_feature_columns,\n",
    "               dnn_hidden_units=param['dnn_hidden_units'], dnn_dropout=param['dnn_dropout'], \n",
    "               dnn_use_bn=param['bn'], task='binary',\n",
    "               l2_reg_linear=param['l2_reg_linear'], l2_reg_embedding=param['l2_reg_embedding'], \n",
    "               l2_reg_dnn=param['l2_reg_dnn'], init_std=0.0001,\n",
    "               seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [{'lr': 0.0001,\n",
    "  'l2_reg_linear': 0.0001,\n",
    "  'l2_reg_embedding': 1e-05,\n",
    "  'l2_reg_dnn': 0.0001,\n",
    "  'emb_dim': 4,\n",
    "  'dnn_hidden_units': (256, 256, 256),\n",
    "  'dnn_dropout': 0.0,\n",
    "  'cyclic': False,\n",
    "  'bn': False}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_perf = []\n",
    "\n",
    "for param in tqdm(params):\n",
    "    \n",
    "    oof_pred_deepfm = np.zeros((len(train), ))\n",
    "    y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    sparse_feature_columns = [SparseFeat(feat, train[feat].nunique() + 1, embedding_dim=param['emb_dim']) \n",
    "                          for feat in features]\n",
    "    dense_feature_columns = [DenseFeat(feat, 1) for feat in features_enc + indicator_cols]\n",
    "\n",
    "    feature_names = get_feature_names(sparse_feature_columns + dense_feature_columns)\n",
    "\n",
    "    all_features = features + features_enc + indicator_cols\n",
    "\n",
    "    for fold, (tr_ind, val_ind) in enumerate(skf.split(train, train[target])):\n",
    "\n",
    "        # Split\n",
    "        X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "        y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "        train_model_input = {name:X_train[name] for name in feature_names}\n",
    "        val_model_input = {name:X_val[name] for name in feature_names}\n",
    "        test_model_input = {name:test[name] for name in feature_names}\n",
    "\n",
    "        # Define model\n",
    "        model = DeepFM(sparse_feature_columns, sparse_feature_columns + dense_feature_columns,\n",
    "                       dnn_hidden_units=param['dnn_hidden_units'], dnn_dropout=param['dnn_dropout'], \n",
    "                       dnn_use_bn=param['bn'], task='binary',\n",
    "                       l2_reg_linear=param['l2_reg_linear'], l2_reg_embedding=param['l2_reg_embedding'], \n",
    "                       l2_reg_dnn=param['l2_reg_dnn'], init_std=0.0001,\n",
    "                       seed=SEED)\n",
    "        opt = keras.optimizers.Adam(learning_rate=param['lr'])\n",
    "        model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "\n",
    "        # Define callbacks\n",
    "        es = callbacks.EarlyStopping(\n",
    "            monitor='val_auc', \n",
    "            min_delta=0.0, \n",
    "            patience=PATIENCE, \n",
    "            verbose=Verbose, \n",
    "            mode='max', \n",
    "            baseline=None, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        #sb = callbacks.ModelCheckpoint(\n",
    "         #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "        #)\n",
    "        if param['cyclic']:\n",
    "            reduce_lr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "                           step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "                           gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "        else:\n",
    "            reduce_lr = ReduceLROnPlateau(\n",
    "                monitor='val_auc', \n",
    "                mode='max',\n",
    "                factor=0.5,\n",
    "                patience=3, \n",
    "                min_lr=1e-7,\n",
    "                verbose=True,\n",
    "            )\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train_model_input, y_train,\n",
    "            validation_data=(val_model_input, y_val),\n",
    "            batch_size=BATCH_SIZE, \n",
    "            epochs=Epochs, \n",
    "            verbose=Verbose,\n",
    "            callbacks=[reduce_lr, es]\n",
    "        )\n",
    "\n",
    "        # Predict\n",
    "        val_pred = model.predict(val_model_input, batch_size=512)\n",
    "        print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "        oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "        y_pred_deepfm += model.predict(test_model_input, batch_size=512).ravel() / (N_Splits)\n",
    "        K.clear_session()\n",
    "    cv_perf.append(round(roc_auc_score(train.target.values, oof_pred_deepfm), 5))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_ord = [feat for feat in features if 'ord' in feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_feature_columns = [SparseFeat(feat, train[feat].nunique() + 1, embedding_dim=4) \n",
    "                          for feat in features if 'ord' not in feat]\n",
    "dense_feature_columns = [DenseFeat(feat, 1) for feat in features_enc + indicator_cols + features_ord]\n",
    "\n",
    "feature_names = get_feature_names(sparse_feature_columns + dense_feature_columns)\n",
    "\n",
    "all_features = features + features_enc + indicator_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_perf = []\n",
    "\n",
    "for param in tqdm(list_params[:10]):\n",
    "    \n",
    "    oof_pred_deepfm = np.zeros((len(train), ))\n",
    "    y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    sparse_feature_columns = [SparseFeat(feat, train[feat].nunique() + 1, embedding_dim=param['emb_dim']) \n",
    "                          for feat in features]\n",
    "    dense_feature_columns = [DenseFeat(feat, 1) for feat in features_enc + indicator_cols]\n",
    "\n",
    "    feature_names = get_feature_names(sparse_feature_columns + dense_feature_columns)\n",
    "\n",
    "    all_features = features + features_enc + indicator_cols\n",
    "\n",
    "    for fold, (tr_ind, val_ind) in enumerate(skf.split(train, train[target])):\n",
    "\n",
    "        # Split\n",
    "        X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "        y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "        train_model_input = {name:X_train[name] for name in feature_names}\n",
    "        val_model_input = {name:X_val[name] for name in feature_names}\n",
    "        test_model_input = {name:test[name] for name in feature_names}\n",
    "\n",
    "        # Define model\n",
    "        model = DeepFM(sparse_feature_columns, sparse_feature_columns + dense_feature_columns,\n",
    "                       dnn_hidden_units=param['dnn_hidden_units'], dnn_dropout=param['dnn_dropout'], \n",
    "                       dnn_use_bn=param['bn'], task='binary',\n",
    "                       l2_reg_linear=param['l2_reg_linear'], l2_reg_embedding=param['l2_reg_embedding'], \n",
    "                       l2_reg_dnn=param['l2_reg_dnn'], init_std=0.0001,\n",
    "                       seed=SEED)\n",
    "        opt = keras.optimizers.Adam(learning_rate=param['lr'])\n",
    "        model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "\n",
    "        # Define callbacks\n",
    "        es = callbacks.EarlyStopping(\n",
    "            monitor='val_auc', \n",
    "            min_delta=0.0, \n",
    "            patience=PATIENCE, \n",
    "            verbose=Verbose, \n",
    "            mode='max', \n",
    "            baseline=None, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        #sb = callbacks.ModelCheckpoint(\n",
    "         #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "        #)\n",
    "        if param['cyclic']:\n",
    "            reduce_lr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "                           step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "                           gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "        else:\n",
    "            reduce_lr = ReduceLROnPlateau(\n",
    "                monitor='val_auc', \n",
    "                mode='max',\n",
    "                factor=0.5,\n",
    "                patience=3, \n",
    "                min_lr=1e-7,\n",
    "                verbose=True,\n",
    "            )\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train_model_input, y_train,\n",
    "            validation_data=(val_model_input, y_val),\n",
    "            batch_size=BATCH_SIZE, \n",
    "            epochs=Epochs, \n",
    "            verbose=Verbose,\n",
    "            callbacks=[reduce_lr, es]\n",
    "        )\n",
    "\n",
    "        # Predict\n",
    "        val_pred = model.predict(val_model_input, batch_size=512)\n",
    "        print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "        oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "        y_pred_deepfm += model.predict(test_model_input, batch_size=512).ravel() / (N_Splits)\n",
    "        K.clear_session()\n",
    "    cv_perf.append(round(roc_auc_score(train.target.values, oof_pred_deepfm), 5))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['target']\n",
    "N_Splits = 10\n",
    "Verbose = 1\n",
    "Epochs = 50\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "grid_params = {\n",
    "    'dnn_dropout': [0.0, 0.1, 0.2],\n",
    "    'dnn_hidden_units': [(256,), (512,), (256, 256), (512, 512)],\n",
    "    'linear': [linear_feature_columns, linear_feature_columns + dnn_feature_columns],\n",
    "    'sparse': [dnn_feature_columns, linear_feature_columns + dnn_feature_columns],\n",
    "    'batch_size': [128, 256, 512],\n",
    "}\n",
    "list_params = list(ParameterSampler(grid_params,\n",
    "                                    n_iter=20,\n",
    "                                    random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_perf = []\n",
    "\n",
    "for param in list_params:\n",
    "\n",
    "    oof_pred_deepfm = np.zeros((len(train), ))\n",
    "    y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "    for fold, (tr_ind, val_ind) in enumerate(skf.split(train, train[target])):\n",
    "        X_train, X_val = train[features+features_enc+indicator_cols].iloc[tr_ind], train[features+features_enc+indicator_cols].iloc[val_ind]\n",
    "        y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "        train_model_input = {name:X_train[name] for name in feature_names}\n",
    "        val_model_input = {name:X_val[name] for name in feature_names}\n",
    "        test_model_input = {name:test[name] for name in feature_names}\n",
    "        model = DeepFM(param['linear'], param['sparse'],\n",
    "                       dnn_hidden_units=param['dnn_hidden_units'], dnn_dropout=param['dnn_dropout'], dnn_use_bn=False, task='binary')\n",
    "        model.compile(\"adam\", \"binary_crossentropy\", metrics=[auc], )\n",
    "        es = callbacks.EarlyStopping(monitor='val_auc', min_delta=0.001, patience=4, verbose=Verbose, mode='max', baseline=None, restore_best_weights=True)\n",
    "        sb = callbacks.ModelCheckpoint('./nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose)\n",
    "        clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "                           step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "                           gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "        history = model.fit(train_model_input, y_train,\n",
    "                            validation_data=(val_model_input, y_val),\n",
    "                            batch_size=param['batch_size'], epochs=Epochs, verbose=Verbose,\n",
    "                            callbacks=[es, sb, clr],)\n",
    "        model.load_weights('./nn_model.w8')\n",
    "        val_pred = model.predict(val_model_input, batch_size=param['batch_size'])\n",
    "        print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "        oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "        y_pred_deepfm += model.predict(test_model_input, batch_size=param['batch_size']).ravel() / (N_Splits)\n",
    "        K.clear_session()\n",
    "    cv_perf.append(round(roc_auc_score(train.target.values, oof_pred_deepfm), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_params[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV 50 best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['target']\n",
    "N_Splits = 50\n",
    "Verbose = 1\n",
    "Epochs = 50\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_pred_deepfm = np.zeros((len(train), ))\n",
    "y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "\n",
    "for fold, (tr_ind, val_ind) in enumerate(skf.split(train, train[target])):\n",
    "    \n",
    "    # Split\n",
    "    X_train, X_val = train[features+features_enc+indicator_cols].iloc[tr_ind], train[features+features_enc+indicator_cols].iloc[val_ind]\n",
    "    y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "    train_model_input = {name:X_train[name] for name in feature_names}\n",
    "    val_model_input = {name:X_val[name] for name in feature_names}\n",
    "    test_model_input = {name:test[name] for name in feature_names}\n",
    "    \n",
    "    # Define model\n",
    "    model = DeepFM(linear_feature_columns, linear_feature_columns + dnn_feature_columns,\n",
    "                   dnn_hidden_units=(512, 512), dnn_dropout=0.0, dnn_use_bn=False, task='binary')\n",
    "    model.compile(\"adam\", \"binary_crossentropy\", metrics=[auc])\n",
    "    \n",
    "    # Define callbacks\n",
    "    es = callbacks.EarlyStopping(monitor='val_auc', min_delta=0.001, patience=4, verbose=Verbose, mode='max', baseline=None, restore_best_weights=True)\n",
    "    sb = callbacks.ModelCheckpoint('./nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose)\n",
    "#     clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "#                        step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "#                        gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_auc', factor=0.2,\n",
    "                                  patience=5, min_lr=0.00001)\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(train_model_input, y_train,\n",
    "                        validation_data=(val_model_input, y_val),\n",
    "                        batch_size=BATCH_SIZE, epochs=Epochs, verbose=Verbose,\n",
    "                        callbacks=[es, sb, reduce_lr],)\n",
    "    model.load_weights('./nn_model.w8')\n",
    "    \n",
    "    # Predict\n",
    "    val_pred = model.predict(val_model_input, batch_size=512)\n",
    "    print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "    oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "    y_pred_deepfm += model.predict(test_model_input, batch_size=512).ravel() / (N_Splits)\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = test.id.values\n",
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test_idx,\n",
    "    'target': y_pred_deepfm\n",
    "})\n",
    "submission.to_csv(\"submission_deepfm_cv_50.csv\", index=False)\n",
    "print(\"Submission file saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('oof_pred_deepfm_cv_50.npy',oof_pred_deepfm)\n",
    "np.save('y_pred_deepfm_cv_50.npy',    y_pred_deepfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "sandbox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
