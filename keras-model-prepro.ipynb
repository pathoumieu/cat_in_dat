{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 666"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import MissingIndicator, SimpleImputer, IterativeImputer, KNNImputer\n",
    "from category_encoders.cat_boost import CatBoostEncoder\n",
    "from deepctr.inputs import  SparseFeat, DenseFeat, get_feature_names\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam,RMSprop\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "import gc\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from deepctr.models import DeepFM\n",
    "import tensorflow.keras as keras\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import re\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import utils\n",
    "import math\n",
    "from sklearn.metrics import precision_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('cat-in-the-dat-ii/train.csv')\n",
    "test = pd.read_csv('cat-in-the-dat-ii/test.csv')\n",
    "\n",
    "test[\"target\"] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_features = ['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4']\n",
    "nom_features = ['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']\n",
    "ord_features = ['ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5']\n",
    "other_features = ['day', 'month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train, test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features = [feat for feat in train.columns if feat not in ['id', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['nan_feature'] = data[sparse_features].isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features = bin_features + nom_features + other_features + ['nan_feature']\n",
    "dense_features = ord_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label encode and fillna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_numeric(df):\n",
    "    \n",
    "    bin_3_mapping = {'T':1 , 'F':0}\n",
    "    bin_4_mapping = {'Y':1 , 'N':0}\n",
    "    nom_0_mapping = {'Red' : 0, 'Blue' : 1, 'Green' : 2}\n",
    "    nom_1_mapping = {'Trapezoid' : 0, 'Star' : 1, 'Circle': 2, 'Triangle' : 3, 'Polygon' : 4}\n",
    "    nom_2_mapping = {'Hamster' : 0 , 'Axolotl' : 1, 'Lion' : 2, 'Dog' : 3, 'Cat' : 4, 'Snake' : 5}\n",
    "    nom_3_mapping = {'Russia' : 0, 'Canada' : 1, 'Finland' : 2, 'Costa Rica' : 3, 'China' : 4, 'India' : 5}\n",
    "    nom_4_mapping = {'Bassoon' : 0, 'Theremin' : 1, 'Oboe' : 2, 'Piano' : 3}\n",
    "    nom_5_mapping = dict(zip((df.nom_5.dropna().unique()), range(len((df.nom_5.dropna().unique())))))\n",
    "    nom_6_mapping = dict(zip((df.nom_6.dropna().unique()), range(len((df.nom_6.dropna().unique())))))\n",
    "    nom_7_mapping = dict(zip((df.nom_7.dropna().unique()), range(len((df.nom_7.dropna().unique())))))\n",
    "    nom_8_mapping = dict(zip((df.nom_8.dropna().unique()), range(len((df.nom_8.dropna().unique())))))\n",
    "    nom_9_mapping = dict(zip((df.nom_9.dropna().unique()), range(len((df.nom_9.dropna().unique())))))\n",
    "    ord_1_mapping = {'Novice' : 0, 'Contributor' : 1, 'Expert' : 2, 'Master': 3, 'Grandmaster': 4}\n",
    "    ord_2_mapping = { 'Freezing': 0, 'Cold': 1, 'Warm' : 2, 'Hot': 3, 'Boiling Hot' : 4, 'Lava Hot' : 5}\n",
    "    ord_3_mapping = {'a':0, 'b':1, 'c':2 ,'d':3 ,'e':4, 'f':5, 'g':6, 'h':7, 'i':8, 'j':9, 'k':10, 'l':11, 'm':12, 'n':13, 'o':14}\n",
    "    ord_4_mapping = {'A':0, 'B':1, 'C':2, 'D':3, 'E':4, 'F':5, 'G':6, 'H':7, 'I':8, 'J':9, 'K':10,'L':11,'M':12,\n",
    "                 'N':13,'O':14,'P':15,'Q':16,'R':17,'S':18,'T':19,'U':20,'V':21,'W':22,'X':23,'Y':24,'Z':25}\n",
    "    sorted_ord_5 = sorted(df.ord_5.dropna().unique())\n",
    "    ord_5_mapping = dict(zip(sorted_ord_5, range(len(sorted_ord_5))))\n",
    "\n",
    "    df['bin_3'] = df.loc[df.bin_3.notnull(), 'bin_3'].map(bin_3_mapping)\n",
    "    df['bin_4'] = df.loc[df.bin_4.notnull(), 'bin_4'].map(bin_4_mapping)\n",
    "    df['nom_0'] = df.loc[df.nom_0.notnull(), 'nom_0'].map(nom_0_mapping)\n",
    "    df['nom_1'] = df.loc[df.nom_1.notnull(), 'nom_1'].map(nom_1_mapping)\n",
    "    df['nom_2'] = df.loc[df.nom_2.notnull(), 'nom_2'].map(nom_2_mapping)\n",
    "    df['nom_3'] = df.loc[df.nom_3.notnull(), 'nom_3'].map(nom_3_mapping)\n",
    "    df['nom_4'] = df.loc[df.nom_4.notnull(), 'nom_4'].map(nom_4_mapping)\n",
    "    df['nom_5'] = df.loc[df.nom_5.notnull(), 'nom_5'].map(nom_5_mapping)\n",
    "    df['nom_6'] = df.loc[df.nom_6.notnull(), 'nom_6'].map(nom_6_mapping)\n",
    "    df['nom_7'] = df.loc[df.nom_7.notnull(), 'nom_7'].map(nom_7_mapping)\n",
    "    df['nom_8'] = df.loc[df.nom_8.notnull(), 'nom_8'].map(nom_8_mapping)\n",
    "    df['nom_9'] = df.loc[df.nom_9.notnull(), 'nom_9'].map(nom_9_mapping)\n",
    "    df['ord_1'] = df.loc[df.ord_1.notnull(), 'ord_1'].map(ord_1_mapping)\n",
    "    df['ord_2'] = df.loc[df.ord_2.notnull(), 'ord_2'].map(ord_2_mapping)\n",
    "    df['ord_3'] = df.loc[df.ord_3.notnull(), 'ord_3'].map(ord_3_mapping)\n",
    "    df['ord_4'] = df.loc[df.ord_4.notnull(), 'ord_4'].map(ord_4_mapping)\n",
    "    df['ord_5'] = df.loc[df.ord_5.notnull(), 'ord_5'].map(ord_5_mapping)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define sparse features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NaN count\n",
    "# data['nan_count'] = data.isnull().sum(axis=1) Problem with nan count\n",
    "\n",
    "features = [feat for feat in data.columns if feat not in ['id', 'target']]#, 'nan_feature']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep categories present in train AND test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[data.target != -1]\n",
    "test = data[data.target == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 values in nom_5, {'b3ad70fcb'} Replaced with nan\n",
      "4 values in nom_6, {'a885aacec', 'ee6983c6d', '3a121fefb', 'f0732a795'} Replaced with nan\n",
      "2 values in nom_9, {'1065f10dd', '3d19cd31d'} Replaced with nan\n",
      "1 values in nan_feature, {7} Replaced with nan\n"
     ]
    }
   ],
   "source": [
    "for col in features:\n",
    "    train_unique_values = set(train[col].dropna().unique())\n",
    "    test_unique_values  = set(test[col].dropna().unique())\n",
    "\n",
    "    symmetric_difference_values = train_unique_values.symmetric_difference(test_unique_values)\n",
    "    if symmetric_difference_values:\n",
    "        print(f'{len(symmetric_difference_values)} values in {col}, {symmetric_difference_values} Replaced with nan')\n",
    "        data.loc[data[col].isin(symmetric_difference_values), col] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fillna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "features = [feat for feat in data.columns if feat not in ['target','id']]\n",
    "data[features] = convert_data_to_numeric(data[features])\n",
    "data[features] = data[features].astype('category')\n",
    "imp = IterativeImputer(max_iter=500, initial_strategy='most_frequent', random_state=SEED, add_indicator=True)\n",
    "indicator_cols = [feat + '_ind' for feat in features]\n",
    "for col in indicator_cols:\n",
    "    data[col] = 0\n",
    "data[features+indicator_cols] = imp.fit_transform(data[features])\n",
    "data[features] = data[features].round(0).astype(np.int16)\n",
    "data[indicator_cols] = data[indicator_cols].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[features] = convert_data_to_numeric(data[features])\n",
    "data[sparse_features] = data[sparse_features].fillna(-1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[data.target != -1].reset_index(drop=True)\n",
    "test  = data[data.target == -1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(strategy='median')\n",
    "train[dense_features] = imp.fit_transform(train[dense_features])\n",
    "test[dense_features] = imp.transform(test[dense_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard scale dense_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std = StandardScaler()\n",
    "\n",
    "train[dense_features] = std.fit_transform(train[dense_features])\n",
    "test[dense_features] = std.transform(test[dense_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treat days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(y_true, y_pred):\n",
    "    def fallback_auc(y_true, y_pred):\n",
    "        try:\n",
    "            return roc_auc_score(y_true, y_pred)\n",
    "        except:\n",
    "            return 0.5\n",
    "    return tf.py_function(fallback_auc, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicLR(keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1 / (2. ** (x - 1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma ** (x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "\n",
    "    def clr(self):\n",
    "        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n",
    "        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x)) * self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x)) * self.scale_fn(\n",
    "                self.clr_iterations)\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "\n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['target']\n",
    "N_Splits = 5\n",
    "Verbose = 1\n",
    "Epochs = 50\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "DROPOUT = 0.3\n",
    "NNLAYERS = (300, 300)\n",
    "PATIENCE = 5\n",
    "\n",
    "MAX_EMB_DIM = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(data, catcols, densecols, sparse_features_emb, max_emb_dim, dnn_layers, last_dense):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    outputs_emb = []\n",
    "    \n",
    "    # Cat cols\n",
    "    for c in catcols:\n",
    "        \n",
    "        num_unique_values = int(data[c].nunique())\n",
    "        \n",
    "        if sparse_features_emb[c]:\n",
    "            embed_dim = sparse_features_emb[c]\n",
    "        else:\n",
    "            embed_dim = int(min(np.ceil(num_unique_values*0.5), max_emb_dim))\n",
    "            \n",
    "        inp = layers.Input(shape=(1,))\n",
    "        out = layers.Embedding(\n",
    "            num_unique_values + 1, \n",
    "            embed_dim, \n",
    "            name=c, \n",
    "#             activity_regularizer=regularizers.l1(0.01)\n",
    "        )(inp)\n",
    "        out = layers.SpatialDropout1D(DROPOUT)(out)\n",
    "        out = layers.Reshape(target_shape=(embed_dim, ))(out)\n",
    "        inputs.append(inp)\n",
    "        outputs.append(out)\n",
    "        \n",
    "    # First dense for embeddings\n",
    "#     outputs_emb = layers.Concatenate()(outputs_emb)\n",
    "#     outputs_emb = layers.Dense(last_dense, activation=\"relu\")(outputs_emb)\n",
    "#     outputs_emb = layers.Dropout(DROPOUT)(outputs_emb)\n",
    "        \n",
    "    # Dense cols\n",
    "    if densecols:\n",
    "        dense_inp = layers.Input(shape=(len(densecols),))\n",
    "        inputs.append(dense_inp)\n",
    "        outputs.append(dense_inp)\n",
    "        \n",
    "    x = layers.Concatenate()(outputs)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # DNN layers\n",
    "    for size in dnn_layers:\n",
    "#         x = layers.Concatenate()([x, outputs_emb])\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(size, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(DROPOUT)(x)\n",
    "        \n",
    "#     x = layers.Concatenate()([x, outputs_emb])\n",
    "    \n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    y = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=y)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_features = dense_features + sparse_features + ['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = features + ['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_cv(\n",
    "    sparse_features, \n",
    "    dense_features,\n",
    "    sparse_features_emb,\n",
    "    max_emb_dim, \n",
    "    nn_layers=(300, 300), \n",
    "    last_dense=256, \n",
    "    n_splits=N_Splits,\n",
    "    verbose=Verbose\n",
    "):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    random.seed(SEED)\n",
    "\n",
    "    tf.random.set_seed(SEED)\n",
    "    \n",
    "    oof_pred_deepfm = np.zeros((len(train), ))\n",
    "    y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    all_features = sparse_features + dense_features + ['target']\n",
    "\n",
    "    for fold, (tr_ind, val_ind) in enumerate(skf.split(train, train[target])):\n",
    "\n",
    "        # Split\n",
    "        X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "        y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "        \n",
    "        train_model_input = [X_train.loc[:, sparse_features].values[:, k] \\\n",
    "                             for k in range(X_train.loc[:, sparse_features].values.shape[1])]\n",
    "        if dense_features:\n",
    "            train_model_input += [X_train.loc[:, dense_features].values]\n",
    "        \n",
    "        val_model_input = [X_val.loc[:, sparse_features].values[:, k] \\\n",
    "                           for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "        if dense_features:\n",
    "            val_model_input += [X_val.loc[:, dense_features].values]\n",
    "        \n",
    "        test_model_input = [test.loc[:, sparse_features].values[:, k] \\\n",
    "                            for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "        if dense_features:\n",
    "            test_model_input += [test.loc[:, dense_features].values]\n",
    "\n",
    "\n",
    "        # Define model\n",
    "        model = create_model(data, sparse_features, dense_features, sparse_features_emb, \n",
    "                             max_emb_dim, nn_layers, last_dense)\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "\n",
    "        # Define callbacks\n",
    "        es = callbacks.EarlyStopping(\n",
    "            monitor='val_auc', \n",
    "            min_delta=0.001, \n",
    "            patience=PATIENCE, \n",
    "            verbose=verbose, \n",
    "            mode='max', \n",
    "            baseline=None, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        #sb = callbacks.ModelCheckpoint(\n",
    "         #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "        #)\n",
    "    #     clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "    #                        step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "    #                        gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_auc', \n",
    "            mode='max',\n",
    "            factor=0.5,\n",
    "            patience=3, \n",
    "            min_lr=1e-6,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "    #     cb = TQDMNotebookCallback()\n",
    "    #     setattr(cb,'on_train_batch_begin',lambda x,y:None)\n",
    "    #     setattr(cb,'on_train_batch_end',lambda x,y:None)\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train_model_input, utils.to_categorical(y_train),\n",
    "            validation_data=(val_model_input, utils.to_categorical(y_val)),\n",
    "            batch_size=1024, \n",
    "            epochs=Epochs, \n",
    "            verbose=verbose,\n",
    "            callbacks=[reduce_lr, es]\n",
    "        )\n",
    "\n",
    "        # Predict\n",
    "        val_pred = model.predict(val_model_input, batch_size=512)[:, 1]\n",
    "        print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "        oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "        y_pred_deepfm += model.predict(test_model_input, batch_size=512)[:, 1].ravel() / (n_splits)\n",
    "        K.clear_session()\n",
    "        gc.collect()\n",
    "    gc.collect()        \n",
    "    print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "        \n",
    "    return oof_pred_deepfm, y_pred_deepfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 480000 samples, validate on 120000 samples\n",
      "Epoch 1/50\n",
      "480000/480000 [==============================] - 15s 30us/sample - loss: 0.4165 - auc: 0.7547 - val_loss: 0.3967 - val_auc: 0.7873\n",
      "Epoch 2/50\n",
      "480000/480000 [==============================] - 12s 26us/sample - loss: 0.3988 - auc: 0.7842 - val_loss: 0.3964 - val_auc: 0.7870\n",
      "Epoch 3/50\n",
      "480000/480000 [==============================] - 12s 25us/sample - loss: 0.3960 - auc: 0.7884 - val_loss: 0.3971 - val_auc: 0.7873\n",
      "Epoch 4/50\n",
      "478208/480000 [============================>.] - ETA: 0s - loss: 0.3931 - auc: 0.7925\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "480000/480000 [==============================] - 17s 36us/sample - loss: 0.3931 - auc: 0.7925 - val_loss: 0.3979 - val_auc: 0.7859\n",
      "Epoch 5/50\n",
      "480000/480000 [==============================] - 15s 32us/sample - loss: 0.3882 - auc: 0.7989 - val_loss: 0.3993 - val_auc: 0.7845\n",
      "Epoch 6/50\n",
      "478208/480000 [============================>.] - ETA: 0s - loss: 0.3858 - auc: 0.8023Restoring model weights from the end of the best epoch.\n",
      "480000/480000 [==============================] - 13s 27us/sample - loss: 0.3858 - auc: 0.8023 - val_loss: 0.4013 - val_auc: 0.7818\n",
      "Epoch 00006: early stopping\n",
      "validation AUC fold 1 : 0.7874\n",
      "Train on 480000 samples, validate on 120000 samples\n",
      "Epoch 1/50\n",
      "480000/480000 [==============================] - 23s 48us/sample - loss: 0.4162 - auc: 0.7552 - val_loss: 0.3976 - val_auc: 0.7861\n",
      "Epoch 2/50\n",
      "480000/480000 [==============================] - 13s 28us/sample - loss: 0.3990 - auc: 0.7845 - val_loss: 0.3962 - val_auc: 0.7874\n",
      "Epoch 3/50\n",
      "480000/480000 [==============================] - 14s 30us/sample - loss: 0.3957 - auc: 0.7893 - val_loss: 0.3964 - val_auc: 0.7870\n",
      "Epoch 4/50\n",
      "480000/480000 [==============================] - 14s 29us/sample - loss: 0.3935 - auc: 0.7924 - val_loss: 0.3974 - val_auc: 0.7857\n",
      "Epoch 5/50\n",
      "479232/480000 [============================>.] - ETA: 0s - loss: 0.3907 - auc: 0.7961\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "480000/480000 [==============================] - 14s 29us/sample - loss: 0.3907 - auc: 0.7962 - val_loss: 0.3994 - val_auc: 0.7826\n",
      "Epoch 6/50\n",
      "480000/480000 [==============================] - 14s 29us/sample - loss: 0.3849 - auc: 0.8036 - val_loss: 0.4015 - val_auc: 0.7797\n",
      "Epoch 7/50\n",
      "479232/480000 [============================>.] - ETA: 0s - loss: 0.3831 - auc: 0.8061Restoring model weights from the end of the best epoch.\n",
      "480000/480000 [==============================] - 14s 29us/sample - loss: 0.3831 - auc: 0.8062 - val_loss: 0.4025 - val_auc: 0.7786\n",
      "Epoch 00007: early stopping\n",
      "validation AUC fold 2 : 0.78734\n",
      "Train on 480000 samples, validate on 120000 samples\n",
      "Epoch 1/50\n",
      "480000/480000 [==============================] - 17s 36us/sample - loss: 0.4155 - auc: 0.7569 - val_loss: 0.3984 - val_auc: 0.7854\n",
      "Epoch 2/50\n",
      "480000/480000 [==============================] - 14s 30us/sample - loss: 0.3992 - auc: 0.7839 - val_loss: 0.3972 - val_auc: 0.7866\n",
      "Epoch 3/50\n",
      "480000/480000 [==============================] - 14s 29us/sample - loss: 0.3958 - auc: 0.7891 - val_loss: 0.3979 - val_auc: 0.7860\n",
      "Epoch 4/50\n",
      "480000/480000 [==============================] - 14s 29us/sample - loss: 0.3936 - auc: 0.7920 - val_loss: 0.3979 - val_auc: 0.7855\n",
      "Epoch 5/50\n",
      "478208/480000 [============================>.] - ETA: 0s - loss: 0.3905 - auc: 0.7962\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "480000/480000 [==============================] - 14s 29us/sample - loss: 0.3905 - auc: 0.7962 - val_loss: 0.3998 - val_auc: 0.7828\n",
      "Epoch 6/50\n",
      "480000/480000 [==============================] - 14s 29us/sample - loss: 0.3850 - auc: 0.8037 - val_loss: 0.4016 - val_auc: 0.7789\n",
      "Epoch 7/50\n",
      "478208/480000 [============================>.] - ETA: 0s - loss: 0.3830 - auc: 0.8063Restoring model weights from the end of the best epoch.\n",
      "480000/480000 [==============================] - 14s 29us/sample - loss: 0.3830 - auc: 0.8063 - val_loss: 0.4031 - val_auc: 0.7782\n",
      "Epoch 00007: early stopping\n",
      "validation AUC fold 3 : 0.78666\n",
      "Train on 480000 samples, validate on 120000 samples\n",
      "Epoch 1/50\n",
      "480000/480000 [==============================] - 16s 34us/sample - loss: 0.4148 - auc: 0.7569 - val_loss: 0.3978 - val_auc: 0.7852\n",
      "Epoch 2/50\n",
      "480000/480000 [==============================] - 14s 29us/sample - loss: 0.3986 - auc: 0.7843 - val_loss: 0.3977 - val_auc: 0.7857\n",
      "Epoch 3/50\n",
      "480000/480000 [==============================] - 14s 30us/sample - loss: 0.3950 - auc: 0.7901 - val_loss: 0.3986 - val_auc: 0.7851\n",
      "Epoch 4/50\n",
      "480000/480000 [==============================] - 14s 30us/sample - loss: 0.3923 - auc: 0.7938 - val_loss: 0.3996 - val_auc: 0.7831\n",
      "Epoch 5/50\n",
      "478208/480000 [============================>.] - ETA: 0s - loss: 0.3889 - auc: 0.7985\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "480000/480000 [==============================] - 15s 31us/sample - loss: 0.3888 - auc: 0.7985 - val_loss: 0.4022 - val_auc: 0.7803\n",
      "Epoch 6/50\n",
      "479232/480000 [============================>.] - ETA: 0s - loss: 0.3832 - auc: 0.8060Restoring model weights from the end of the best epoch.\n",
      "480000/480000 [==============================] - 14s 28us/sample - loss: 0.3832 - auc: 0.8060 - val_loss: 0.4041 - val_auc: 0.7767\n",
      "Epoch 00006: early stopping\n",
      "validation AUC fold 4 : 0.78573\n",
      "Train on 480000 samples, validate on 120000 samples\n",
      "Epoch 1/50\n",
      "480000/480000 [==============================] - 17s 36us/sample - loss: 0.4162 - auc: 0.7549 - val_loss: 0.3984 - val_auc: 0.7852\n",
      "Epoch 2/50\n",
      "480000/480000 [==============================] - 14s 28us/sample - loss: 0.3990 - auc: 0.7842 - val_loss: 0.3972 - val_auc: 0.7859\n",
      "Epoch 3/50\n",
      "480000/480000 [==============================] - 14s 29us/sample - loss: 0.3959 - auc: 0.7889 - val_loss: 0.3971 - val_auc: 0.7864\n",
      "Epoch 4/50\n",
      "480000/480000 [==============================] - 15s 30us/sample - loss: 0.3931 - auc: 0.7925 - val_loss: 0.3981 - val_auc: 0.7850\n",
      "Epoch 5/50\n",
      "480000/480000 [==============================] - 14s 29us/sample - loss: 0.3899 - auc: 0.7970 - val_loss: 0.4004 - val_auc: 0.7822\n",
      "Epoch 6/50\n",
      "479232/480000 [============================>.] - ETA: 0s - loss: 0.3871 - auc: 0.8012\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "480000/480000 [==============================] - 14s 29us/sample - loss: 0.3871 - auc: 0.8011 - val_loss: 0.4015 - val_auc: 0.7798\n",
      "Epoch 7/50\n",
      "480000/480000 [==============================] - 15s 31us/sample - loss: 0.3815 - auc: 0.8086 - val_loss: 0.4043 - val_auc: 0.7755\n",
      "Epoch 8/50\n",
      "479232/480000 [============================>.] - ETA: 0s - loss: 0.3794 - auc: 0.8113Restoring model weights from the end of the best epoch.\n",
      "480000/480000 [==============================] - 13s 28us/sample - loss: 0.3794 - auc: 0.8113 - val_loss: 0.4069 - val_auc: 0.7737\n",
      "Epoch 00008: early stopping\n",
      "validation AUC fold 5 : 0.78621\n",
      "OOF AUC : 0.78652\n"
     ]
    }
   ],
   "source": [
    "sparse_features_emb = {\n",
    "    'bin_0': None,\n",
    "    'bin_1': None,\n",
    "    'bin_2': None,\n",
    "    'bin_3': None,\n",
    "    'bin_4': None,\n",
    "    'nom_0': None,\n",
    "    'nom_1': None,\n",
    "    'nom_2': None,\n",
    "    'nom_3': None,\n",
    "    'nom_4': None,\n",
    "    'nom_5': None,\n",
    "    'nom_6': None,\n",
    "    'nom_7': None,\n",
    "    'nom_8': None,\n",
    "    'nom_9': None,\n",
    "    'day': None,\n",
    "    'month': None,\n",
    "    'nan_feature': None\n",
    "}\n",
    "\n",
    "oof_pred_deepfm = run_cv(\n",
    "    sparse_features=sparse_features, \n",
    "    dense_features=dense_features,\n",
    "    sparse_features_emb=sparse_features_emb,\n",
    "    max_emb_dim=15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "* Max emb dim 50 = 100 OOF AUC : 0.78486 (ln(dim))\n",
    "* Max emb dim 50 OOF AUC : 0.7851 (dim*0.5)\n",
    "* Max emb dim 100 OOF AUC : 0.7843 (dim*0.5)\n",
    "* Max emb dim 200 OOF AUC : 0.78327 (dim*0.5)\n",
    "* Max emb dim 20 OOF AUC : 0.78576 (dim*0.5)\n",
    "* Max emb dim 10 OOF AUC : 0.78557 (dim*0.5)\n",
    "* Max emb dim 20 OOF AUC : 0.78576 (dim*0.5) + std ord + nan_feature\n",
    "* Max emb dim 10 OOF AUC : 0.7864 (dim*0.5) + std ord + nan_feature\n",
    "* Max emb dim 15 OOF AUC : 0.78652 (dim*0.5) + std ord + nan_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "sparse_emb_dims = {\n",
    "    'bin_0': [None],\n",
    "    'bin_1': [None],\n",
    "    'bin_2': [None],\n",
    "    'bin_3': [None],\n",
    "    'bin_4': [None],\n",
    "    'nom_0': [None],\n",
    "    'nom_1': [None],\n",
    "    'nom_2': [None],\n",
    "    'nom_3': [None],\n",
    "    'nom_4': [None],\n",
    "    'nom_5': [5, 10, 15, 20],\n",
    "    'nom_6': [5, 10, 15, 20],\n",
    "    'nom_7': [5, 10, 15, 20],\n",
    "    'nom_8': [5, 10, 15, 20],\n",
    "    'nom_9': [5, 10, 15, 20],\n",
    "    'day': [None],\n",
    "    'month': [None],\n",
    "    'nan_feature': [None]    \n",
    "}\n",
    "list_params = list(ParameterSampler(sparse_emb_dims,\n",
    "                                    n_iter=10,\n",
    "                                    random_state=SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation AUC fold 1 : 0.78674\n",
      "validation AUC fold 2 : 0.78727\n",
      "validation AUC fold 3 : 0.78664\n",
      "validation AUC fold 4 : 0.78524\n",
      "validation AUC fold 5 : 0.78683\n",
      "OOF AUC : 0.78629\n",
      "validation AUC fold 1 : 0.78754\n",
      "validation AUC fold 2 : 0.78719\n",
      "validation AUC fold 3 : 0.78671\n",
      "validation AUC fold 4 : 0.78539\n",
      "validation AUC fold 5 : 0.78615\n",
      "OOF AUC : 0.7862\n",
      "validation AUC fold 1 : 0.78746\n",
      "validation AUC fold 2 : 0.78772\n",
      "validation AUC fold 3 : 0.78662\n",
      "validation AUC fold 4 : 0.78592\n",
      "validation AUC fold 5 : 0.78686\n",
      "OOF AUC : 0.78664\n",
      "validation AUC fold 1 : 0.78769\n",
      "validation AUC fold 2 : 0.78767\n",
      "validation AUC fold 3 : 0.78672\n",
      "validation AUC fold 4 : 0.78542\n",
      "validation AUC fold 5 : 0.78576\n",
      "OOF AUC : 0.78625\n",
      "validation AUC fold 1 : 0.78776\n",
      "validation AUC fold 2 : 0.78764\n",
      "validation AUC fold 3 : 0.78682\n",
      "validation AUC fold 4 : 0.78533\n",
      "validation AUC fold 5 : 0.78595\n",
      "OOF AUC : 0.78647\n",
      "validation AUC fold 1 : 0.78742\n",
      "validation AUC fold 2 : 0.78732\n",
      "validation AUC fold 3 : 0.78659\n",
      "validation AUC fold 4 : 0.78587\n",
      "validation AUC fold 5 : 0.78674\n",
      "OOF AUC : 0.78655\n",
      "validation AUC fold 1 : 0.78754\n",
      "validation AUC fold 2 : 0.78752\n",
      "validation AUC fold 3 : 0.78654\n",
      "validation AUC fold 4 : 0.7854\n",
      "validation AUC fold 5 : 0.7859\n",
      "OOF AUC : 0.78638\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-9195f570512f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msparse_features_emb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist_param\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmax_emb_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-80-9ce5c9384186>\u001b[0m in \u001b[0;36mrun_cv\u001b[0;34m(sparse_features, dense_features, sparse_features_emb, max_emb_dim, nn_layers, last_dense, n_splits, verbose)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEpochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreduce_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         )\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/sandbox/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.virtualenvs/sandbox/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.virtualenvs/sandbox/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/sandbox/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3508\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3509\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3510\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3512\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/sandbox/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    571\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 572\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/sandbox/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/sandbox/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    443\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    444\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 445\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    446\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/sandbox/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for list_param in list_params:\n",
    "    oof_pred_deepfm = run_cv(\n",
    "    sparse_features=sparse_features, \n",
    "    dense_features=dense_features,\n",
    "    sparse_features_emb=list_param,\n",
    "    max_emb_dim=15,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nom_9': 5,\n",
       " 'nom_8': 20,\n",
       " 'nom_7': 15,\n",
       " 'nom_6': 20,\n",
       " 'nom_5': 5,\n",
       " 'nom_4': None,\n",
       " 'nom_3': None,\n",
       " 'nom_2': None,\n",
       " 'nom_1': None,\n",
       " 'nom_0': None,\n",
       " 'nan_feature': None,\n",
       " 'month': None,\n",
       " 'day': None,\n",
       " 'bin_4': None,\n",
       " 'bin_3': None,\n",
       " 'bin_2': None,\n",
       " 'bin_1': None,\n",
       " 'bin_0': None}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_params[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features_emb = {'nom_9': 5,\n",
    " 'nom_8': 20,\n",
    " 'nom_7': 15,\n",
    " 'nom_6': 20,\n",
    " 'nom_5': 5,\n",
    " 'nom_4': None,\n",
    " 'nom_3': None,\n",
    " 'nom_2': None,\n",
    " 'nom_1': None,\n",
    " 'nom_0': None,\n",
    " 'nan_feature': None,\n",
    " 'month': None,\n",
    " 'day': None,\n",
    " 'bin_4': None,\n",
    " 'bin_3': None,\n",
    " 'bin_2': None,\n",
    " 'bin_1': None,\n",
    " 'bin_0': None}\n",
    "\n",
    "oof_pred_deepfm, y_pred_deepfm = run_cv(\n",
    "    sparse_features=sparse_features, \n",
    "    dense_features=dense_features,\n",
    "    sparse_features_emb=sparse_features_emb,\n",
    "    max_emb_dim=15,\n",
    "    n_splits=50,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = test.id.values\n",
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test_idx,\n",
    "    'target': y_pred_deepfm\n",
    "})\n",
    "submission.to_csv(\"submission_kerasembprepro01_cv_50.csv\", index=False)\n",
    "print(\"Submission file saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('oof_pred_kerasembprepro01_cv_50.npy',oof_pred_deepfm)\n",
    "np.save('y_pred_kerasembprepro01_cv_50.npy',    y_pred_deepfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "oof_pred_deepfm = run_cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "# Max emb dim 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "# Max emb dim 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "# Max emb dim 100 std scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "# Max emb dim 50 std scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* fillna median + std scale ord (not nan feature) + create indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "#ln dim maxembdim 50 no BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "#ln dim maxembdim 20 no BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "#ln dim maxembdim 200 no BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "#ln dim maxembdim 100 no BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "# No BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.7848 - 0.7852"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* test with nan_count\n",
    "* add dense features OK\n",
    "* test with ln instead of *0.5 OK\n",
    "* increase MAX_EMB_DIM OK\n",
    "* archi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['target']\n",
    "N_Splits = 5\n",
    "Verbose = 1\n",
    "Epochs = 50\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "DROPOUT = 0.3\n",
    "NNLAYERS = (300, 300)\n",
    "PATIENCE = 5\n",
    "\n",
    "MAX_EMB_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dense_features = features_enc\n",
    "all_features = features + features_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(data, catcols, densecols, dnn_layers, last_dense):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    dense_out = []\n",
    "    \n",
    "    # Cat cols\n",
    "    for c in catcols:\n",
    "        \n",
    "        num_unique_values = int(data[c].nunique())\n",
    "        embed_dim = int(min(np.ceil(math.log(num_unique_values)), MAX_EMB_DIM))\n",
    "        inp = layers.Input(shape=(1,))\n",
    "        out = layers.Embedding(num_unique_values + 1, embed_dim, name=c)(inp)\n",
    "        out = layers.SpatialDropout1D(DROPOUT)(out)\n",
    "        out = layers.Reshape(target_shape=(embed_dim, ))(out)\n",
    "        inputs.append(inp)\n",
    "        outputs.append(out)\n",
    "        \n",
    "#     First dense for embeddings\n",
    "#     outputs_emb = layers.Concatenate()(outputs_emb)\n",
    "#     outputs_emb = layers.Dense(last_dense, activation=\"relu\")(outputs_emb)\n",
    "#     outputs_emb = layers.Dropout(DROPOUT)(outputs_emb)\n",
    "\n",
    "#     outputs_emb = layers.Concatenate()(outputs_emb)\n",
    "#     outputs_emb = layers.Dense(last_dense, activation=\"relu\")(outputs_emb)\n",
    "#     outputs_emb = layers.Dropout(DROPOUT)(outputs_emb)\n",
    "        \n",
    "    # Dense cols\n",
    "    if densecols:\n",
    "        dense_inp = layers.Input(shape=(len(densecols),))\n",
    "        inputs.append(dense_inp)\n",
    "\n",
    "    dense_inp = layers.Dense(\n",
    "        last_dense, \n",
    "        activation=\"relu\",\n",
    "        kernel_regularizer=keras.regularizers.l2(0.01)\n",
    "    )(dense_inp)\n",
    "    dense_inp = layers.Dropout(0.5)(dense_inp)\n",
    "        \n",
    "    x = layers.Concatenate()(outputs)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # DNN layers\n",
    "    for size in dnn_layers:\n",
    "#         x = layers.Concatenate()([x, outputs_emb])\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(size, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(DROPOUT)(x)\n",
    "        \n",
    "    x = layers.Concatenate()([x, dense_inp])\n",
    "    \n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    y = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=y)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_cv():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    oof_pred_deepfm = np.zeros((len(train), ))\n",
    "    y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "\n",
    "    for fold, (tr_ind, val_ind) in enumerate(skf.split(train, train[target])):\n",
    "\n",
    "        # Split\n",
    "        X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "        y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "        train_model_input = [X_train.loc[:, sparse_features].values[:, k] \\\n",
    "                             for k in range(X_train.loc[:, sparse_features].values.shape[1])]\n",
    "        train_model_input += [X_train.loc[:, dense_features].values]\n",
    "        val_model_input = [X_val.loc[:, sparse_features].values[:, k] \\\n",
    "                           for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "        val_model_input += [X_val.loc[:, dense_features].values]\n",
    "        test_model_input = [test.loc[:, sparse_features].values[:, k] \\\n",
    "                            for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "        test_model_input += [test.loc[:, dense_features].values]\n",
    "\n",
    "\n",
    "        # Define model\n",
    "        model = create_model(data, sparse_features, dense_features, (300, 300), 32)\n",
    "        opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "\n",
    "        # Define callbacks\n",
    "        es = callbacks.EarlyStopping(\n",
    "            monitor='val_auc', \n",
    "            min_delta=0.001, \n",
    "            patience=PATIENCE, \n",
    "            verbose=Verbose, \n",
    "            mode='max', \n",
    "            baseline=None, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        #sb = callbacks.ModelCheckpoint(\n",
    "         #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "        #)\n",
    "    #     clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "    #                        step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "    #                        gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_auc', \n",
    "            mode='max',\n",
    "            factor=0.5,\n",
    "            patience=3, \n",
    "            min_lr=1e-6,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "    #     cb = TQDMNotebookCallback()\n",
    "    #     setattr(cb,'on_train_batch_begin',lambda x,y:None)\n",
    "    #     setattr(cb,'on_train_batch_end',lambda x,y:None)\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train_model_input, utils.to_categorical(y_train),\n",
    "            validation_data=(val_model_input, utils.to_categorical(y_val)),\n",
    "            batch_size=1024, \n",
    "            epochs=Epochs, \n",
    "            verbose=1,\n",
    "            callbacks=[reduce_lr, es]\n",
    "        )\n",
    "\n",
    "        # Predict\n",
    "        val_pred = model.predict(val_model_input, batch_size=512)[:, 1]\n",
    "        print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "        oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "        y_pred_deepfm += model.predict(test_model_input, batch_size=512)[:, 1].ravel() / (N_Splits)\n",
    "        K.clear_session()\n",
    "        \n",
    "    return oof_pred_deepfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_pred_deepfm = run_cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "#ln dim maxembdim 100 no BN (300, 300), 32 reg 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "#ln dim maxembdim 100 no BN (300, 300), 128 reg 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "#ln dim maxembdim 100 no BN (300, 300), 128 no reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* add nan_features OK\n",
    "* archi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['target']\n",
    "N_Splits = 5\n",
    "Verbose = 1\n",
    "Epochs = 50\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "DROPOUT = 0.3\n",
    "NNLAYERS = (300, 300)\n",
    "PATIENCE = 5\n",
    "\n",
    "MAX_EMB_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dense_features = features_enc\n",
    "all_features = features + indicator_cols + features_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features += indicator_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = features + ['nan_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(data, catcols, densecols, dnn_layers, last_dense):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    outputs_emb = []\n",
    "    \n",
    "    # Cat cols\n",
    "    for c in catcols:\n",
    "        \n",
    "        num_unique_values = int(data[c].nunique())\n",
    "        embed_dim = int(min(np.ceil(math.log(num_unique_values)), MAX_EMB_DIM))\n",
    "        inp = layers.Input(shape=(1,))\n",
    "        out = layers.Embedding(num_unique_values + 1, embed_dim, name=c)(inp)\n",
    "        out = layers.SpatialDropout1D(DROPOUT)(out)\n",
    "        out = layers.Reshape(target_shape=(embed_dim, ))(out)\n",
    "        inputs.append(inp)\n",
    "        outputs.append(out)\n",
    "        outputs_emb.append(out)\n",
    "        \n",
    "    # First dense for embeddings\n",
    "#     outputs_emb = layers.Concatenate()(outputs_emb)\n",
    "#     outputs_emb = layers.Dense(last_dense, activation=\"relu\")(outputs_emb)\n",
    "#     outputs_emb = layers.Dropout(DROPOUT)(outputs_emb)\n",
    "        \n",
    "    # Dense cols\n",
    "#     if densecols:\n",
    "#         dense_inp = layers.Input(shape=(len(densecols),))\n",
    "#         inputs.append(dense_inp)\n",
    "\n",
    "#         outputs.append(dense_inp)\n",
    "        \n",
    "    x = layers.Concatenate()(outputs)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # DNN layers\n",
    "    for size in dnn_layers:\n",
    "#         x = layers.Concatenate()([x, outputs_emb])\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(size, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(DROPOUT)(x)\n",
    "        \n",
    "#     x = layers.Concatenate()([x, outputs_emb])\n",
    "    \n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    y = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=y)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.nan_features = train.nan_features.replace({-1: 0})\n",
    "test.nan_features = test.nan_features.replace({-1: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features = sparse_features[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_cv():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    oof_pred_deepfm = np.zeros((len(train), ))\n",
    "    y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "\n",
    "    for fold, (tr_ind, val_ind) in enumerate(skf.split(train, train[target])):\n",
    "\n",
    "        # Split\n",
    "        X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "        y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "        train_model_input = [X_train.loc[:, sparse_features].values[:, k] \\\n",
    "                             for k in range(X_train.loc[:, sparse_features].values.shape[1])]\n",
    "    #     train_model_input += [X_train.loc[:, dense_features].values]\n",
    "        val_model_input = [X_val.loc[:, sparse_features].values[:, k] \\\n",
    "                           for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "    #     val_model_input += [X_val.loc[:, dense_features].values]\n",
    "        test_model_input = [test.loc[:, sparse_features].values[:, k] \\\n",
    "                            for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "    #     test_model_input += [test.loc[:, dense_features].values]\n",
    "\n",
    "\n",
    "        # Define model\n",
    "        model = create_model(data, sparse_features, ['id'], (300, 300), 256)\n",
    "        opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "\n",
    "        # Define callbacks\n",
    "        es = callbacks.EarlyStopping(\n",
    "            monitor='val_auc', \n",
    "            min_delta=0.001, \n",
    "            patience=PATIENCE, \n",
    "            verbose=Verbose, \n",
    "            mode='max', \n",
    "            baseline=None, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        #sb = callbacks.ModelCheckpoint(\n",
    "         #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "        #)\n",
    "    #     clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "    #                        step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "    #                        gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_auc', \n",
    "            mode='max',\n",
    "            factor=0.5,\n",
    "            patience=3, \n",
    "            min_lr=1e-6,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "    #     cb = TQDMNotebookCallback()\n",
    "    #     setattr(cb,'on_train_batch_begin',lambda x,y:None)\n",
    "    #     setattr(cb,'on_train_batch_end',lambda x,y:None)\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train_model_input, utils.to_categorical(y_train),\n",
    "            validation_data=(val_model_input, utils.to_categorical(y_val)),\n",
    "            batch_size=1024, \n",
    "            epochs=Epochs, \n",
    "            verbose=1,\n",
    "            callbacks=[reduce_lr, es]\n",
    "        )\n",
    "\n",
    "        # Predict\n",
    "        val_pred = model.predict(val_model_input, batch_size=512)[:, 1]\n",
    "        print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "        oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "        y_pred_deepfm += model.predict(test_model_input, batch_size=512)[:, 1].ravel() / (N_Splits)\n",
    "        K.clear_session()\n",
    "        \n",
    "    return oof_pred_deepfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "oof_pred_deepfm = run_cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "#ln dim maxembdim 100 no BN (300, 300) nan features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['target']\n",
    "N_Splits = 50\n",
    "Verbose = 0\n",
    "Epochs = 50\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "DROPOUT = 0.3\n",
    "NNLAYERS = (300, 300)\n",
    "PATIENCE = 5\n",
    "\n",
    "MAX_EMB_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(data, catcols, densecols, dnn_layers, last_dense):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    outputs_emb = []\n",
    "    \n",
    "    # Cat cols\n",
    "    for c in catcols:\n",
    "        \n",
    "        num_unique_values = int(data[c].nunique())\n",
    "        embed_dim = int(min(np.ceil(math.log(num_unique_values)), MAX_EMB_DIM))\n",
    "        inp = layers.Input(shape=(1,))\n",
    "        out = layers.Embedding(num_unique_values + 1, embed_dim, name=c)(inp)\n",
    "        out = layers.SpatialDropout1D(DROPOUT)(out)\n",
    "        out = layers.Reshape(target_shape=(embed_dim, ))(out)\n",
    "        inputs.append(inp)\n",
    "        outputs.append(out)\n",
    "        outputs_emb.append(out)\n",
    "        \n",
    "    # First dense for embeddings\n",
    "#     outputs_emb = layers.Concatenate()(outputs_emb)\n",
    "#     outputs_emb = layers.Dense(last_dense, activation=\"relu\")(outputs_emb)\n",
    "#     outputs_emb = layers.Dropout(DROPOUT)(outputs_emb)\n",
    "        \n",
    "    # Dense cols\n",
    "#     if densecols:\n",
    "#         dense_inp = layers.Input(shape=(len(densecols),))\n",
    "#         inputs.append(dense_inp)\n",
    "\n",
    "#         outputs.append(dense_inp)\n",
    "        \n",
    "    x = layers.Concatenate()(outputs)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # DNN layers\n",
    "    for size in dnn_layers:\n",
    "#         x = layers.Concatenate()([x, outputs_emb])\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(size, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(DROPOUT)(x)\n",
    "        \n",
    "#     x = layers.Concatenate()([x, outputs_emb])\n",
    "    \n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    y = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=y)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "oof_pred_deepfm = np.zeros((len(train), ))\n",
    "y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "\n",
    "for fold, (tr_ind, val_ind) in tqdm(enumerate(skf.split(train, train[target]))):\n",
    "\n",
    "    # Split\n",
    "    X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "    y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "    train_model_input = [X_train.loc[:, sparse_features].values[:, k] \\\n",
    "                         for k in range(X_train.loc[:, sparse_features].values.shape[1])]\n",
    "#     train_model_input += [X_train.loc[:, dense_features].values]\n",
    "    val_model_input = [X_val.loc[:, sparse_features].values[:, k] \\\n",
    "                       for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "#     val_model_input += [X_val.loc[:, dense_features].values]\n",
    "    test_model_input = [test.loc[:, sparse_features].values[:, k] \\\n",
    "                        for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "#     test_model_input += [test.loc[:, dense_features].values]\n",
    "\n",
    "\n",
    "    # Define model\n",
    "    model = create_model(data, sparse_features, ['id'], (300, 300), 256)\n",
    "    opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "\n",
    "    # Define callbacks\n",
    "    es = callbacks.EarlyStopping(\n",
    "        monitor='val_auc', \n",
    "        min_delta=0.001, \n",
    "        patience=PATIENCE, \n",
    "        verbose=Verbose, \n",
    "        mode='max', \n",
    "        baseline=None, \n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    #sb = callbacks.ModelCheckpoint(\n",
    "     #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "    #)\n",
    "#     clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "#                        step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "#                        gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_auc', \n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=3, \n",
    "        min_lr=1e-6,\n",
    "        verbose=Verbose,\n",
    "    )\n",
    "\n",
    "#     cb = TQDMNotebookCallback()\n",
    "#     setattr(cb,'on_train_batch_begin',lambda x,y:None)\n",
    "#     setattr(cb,'on_train_batch_end',lambda x,y:None)\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_model_input, utils.to_categorical(y_train),\n",
    "        validation_data=(val_model_input, utils.to_categorical(y_val)),\n",
    "        batch_size=1024, \n",
    "        epochs=Epochs, \n",
    "        verbose=Verbose,\n",
    "        callbacks=[reduce_lr, es]\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    val_pred = model.predict(val_model_input, batch_size=512)[:, 1]\n",
    "    print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "    oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "    y_pred_deepfm += model.predict(test_model_input, batch_size=512)[:, 1].ravel() / (N_Splits)\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = test.id.values\n",
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test_idx,\n",
    "    'target': y_pred_deepfm\n",
    "})\n",
    "submission.to_csv(\"submission_kerasemb_cv_50.csv\", index=False)\n",
    "print(\"Submission file saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('oof_pred_kerasemb_cv_50.npy',oof_pred_deepfm)\n",
    "np.save('y_pred_kerasemb_cv_50.npy',    y_pred_deepfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_pred_deepfm = np.load('oof_pred_kerasemb_cv_50.npy')\n",
    "y_pred_deepfm = np.load('y_pred_kerasemb_cv_50.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percs, bins = np.histogram(oof_pred_deepfm, bins=500)\n",
    "precs = [precision_score(train.target.values, (oof_pred_deepfm > thresh).astype(int)) for thresh in bins]\n",
    "pops = (percs[::-1].cumsum() / 600000)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create traces\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=pops, y=precs[:-1],\n",
    "                    mode='lines',\n",
    "                    name='lines'))\n",
    "#fig.add_trace(go.Scatter(y=bins[:-1], x=precs[:-1],\n",
    " #                   mode='lines',\n",
    "  #                  name='lines'))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percs, bins = np.histogram(1 - oof_pred_deepfm, bins=500)\n",
    "precs = [precision_score(1 - train.target.values, ((1 - oof_pred_deepfm) > thresh).astype(int)) for thresh in bins]\n",
    "\n",
    "pops = (percs[::-1].cumsum() / 600000)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create traces\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=pops, y=precs[:-1],\n",
    "                    mode='lines',\n",
    "                    name='lines'))\n",
    "#fig.add_trace(go.Scatter(y=bins[:-1], x=precs[:-1],\n",
    " #                   mode='lines',\n",
    "  #                  name='lines'))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(oof_pred_deepfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pseudo_index(oof_pred, y_pred, pos_frac=0.01, neg_frac=0.08):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    idx_neg = int(neg_frac * len(oof_pred))\n",
    "    neg_thresh = np.sort(oof_pred)[idx_neg]\n",
    "    \n",
    "    idx_pos = int(pos_frac * len(oof_pred))\n",
    "    pos_thresh = np.sort(oof_pred)[::-1][idx_pos]\n",
    "    \n",
    "    neg_indices = np.argwhere(y_pred < neg_thresh)\n",
    "    pos_indices = np.argwhere(y_pred > pos_thresh)\n",
    "    \n",
    "    print(f'Selected {len(pos_indices) * 100. / len(y_pred)}% ({len(pos_indices)}) of test as 1, for threshold {pos_thresh}')\n",
    "    print(f'Selected {len(neg_indices) * 100. / len(y_pred)}% ({len(neg_indices)}) of test as 0, for threshold {neg_thresh}')\n",
    "    \n",
    "    return pos_indices.flatten(), neg_indices.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_ind, neg_ind = get_pseudo_index(oof_pred_deepfm, y_pred_deepfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV 50 pseudo labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['target']\n",
    "N_Splits = 50\n",
    "Verbose = 0\n",
    "Epochs = 50\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "DROPOUT = 0.3\n",
    "NNLAYERS = (300, 300)\n",
    "PATIENCE = 5\n",
    "\n",
    "MAX_EMB_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(data, catcols, densecols, dnn_layers, last_dense):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    outputs_emb = []\n",
    "    \n",
    "    # Cat cols\n",
    "    for c in catcols:\n",
    "        \n",
    "        num_unique_values = int(data[c].nunique())\n",
    "        embed_dim = int(min(np.ceil(math.log(num_unique_values)), MAX_EMB_DIM))\n",
    "        inp = layers.Input(shape=(1,))\n",
    "        out = layers.Embedding(num_unique_values + 1, embed_dim, name=c)(inp)\n",
    "        out = layers.SpatialDropout1D(DROPOUT)(out)\n",
    "        out = layers.Reshape(target_shape=(embed_dim, ))(out)\n",
    "        inputs.append(inp)\n",
    "        outputs.append(out)\n",
    "        outputs_emb.append(out)\n",
    "        \n",
    "    # First dense for embeddings\n",
    "#     outputs_emb = layers.Concatenate()(outputs_emb)\n",
    "#     outputs_emb = layers.Dense(last_dense, activation=\"relu\")(outputs_emb)\n",
    "#     outputs_emb = layers.Dropout(DROPOUT)(outputs_emb)\n",
    "        \n",
    "    # Dense cols\n",
    "#     if densecols:\n",
    "#         dense_inp = layers.Input(shape=(len(densecols),))\n",
    "#         inputs.append(dense_inp)\n",
    "\n",
    "#         outputs.append(dense_inp)\n",
    "        \n",
    "    x = layers.Concatenate()(outputs)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # DNN layers\n",
    "    for size in dnn_layers:\n",
    "#         x = layers.Concatenate()([x, outputs_emb])\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(size, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(DROPOUT)(x)\n",
    "        \n",
    "#     x = layers.Concatenate()([x, outputs_emb])\n",
    "    \n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    y = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=y)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bak = train.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = test.iloc[pos_ind]\n",
    "pos_df.target = 1\n",
    "\n",
    "neg_df = test.iloc[neg_ind]\n",
    "neg_df.target = 0\n",
    "\n",
    "selected_test = pd.concat([pos_df, neg_df], axis=0).sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train, selected_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_pred_deepfm = np.zeros((len(train), ))\n",
    "y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "for fold, (tr_ind, val_ind) in tqdm(enumerate(skf.split(train, train[target]))):\n",
    "\n",
    "    # Split\n",
    "    X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "    y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "    train_model_input = [X_train.loc[:, sparse_features].values[:, k] \\\n",
    "                         for k in range(X_train.loc[:, sparse_features].values.shape[1])]\n",
    "#     train_model_input += [X_train.loc[:, dense_features].values]\n",
    "    val_model_input = [X_val.loc[:, sparse_features].values[:, k] \\\n",
    "                       for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "#     val_model_input += [X_val.loc[:, dense_features].values]\n",
    "    test_model_input = [test.loc[:, sparse_features].values[:, k] \\\n",
    "                        for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "#     test_model_input += [test.loc[:, dense_features].values]\n",
    "\n",
    "\n",
    "    # Define model\n",
    "    model = create_model(data, sparse_features, ['id'], (300, 300), 256)\n",
    "    opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "\n",
    "    # Define callbacks\n",
    "    es = callbacks.EarlyStopping(\n",
    "        monitor='val_auc', \n",
    "        min_delta=0.001, \n",
    "        patience=PATIENCE, \n",
    "        verbose=Verbose, \n",
    "        mode='max', \n",
    "        baseline=None, \n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    #sb = callbacks.ModelCheckpoint(\n",
    "     #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "    #)\n",
    "#     clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "#                        step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "#                        gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_auc', \n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=3, \n",
    "        min_lr=1e-6,\n",
    "        verbose=Verbose,\n",
    "    )\n",
    "\n",
    "#     cb = TQDMNotebookCallback()\n",
    "#     setattr(cb,'on_train_batch_begin',lambda x,y:None)\n",
    "#     setattr(cb,'on_train_batch_end',lambda x,y:None)\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_model_input, utils.to_categorical(y_train),\n",
    "        validation_data=(val_model_input, utils.to_categorical(y_val)),\n",
    "        batch_size=1024, \n",
    "        epochs=Epochs, \n",
    "        verbose=Verbose,\n",
    "        callbacks=[reduce_lr, es]\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    val_pred = model.predict(val_model_input, batch_size=512)[:, 1]\n",
    "    print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "    oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "    y_pred_deepfm += model.predict(test_model_input, batch_size=512)[:, 1].ravel() / (N_Splits)\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values[:600000], oof_pred_deepfm[:600000]), 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = test.id.values\n",
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test_idx,\n",
    "    'target': y_pred_deepfm\n",
    "})\n",
    "submission.to_csv(\"submission_kerasemb_pseudo_cv_50.csv\", index=False)\n",
    "print(\"Submission file saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('oof_pred_kerasemb_pseudo_cv_50.npy',oof_pred_deepfm)\n",
    "np.save('y_pred_kerasemb_pseudo_cv_50.npy',    y_pred_deepfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* weight loss\n",
    "* pseudo labelling with only pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_ind, neg_ind = get_pseudo_index(oof_pred_deepfm, y_pred_deepfm, pos_frac=0.005, neg_frac=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = test.iloc[pos_ind]\n",
    "pos_df.target = 1\n",
    "\n",
    "neg_df = test.iloc[neg_ind]\n",
    "neg_df.target = 0\n",
    "\n",
    "selected_test = pd.concat([pos_df, neg_df], axis=0).sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train_bak\n",
    "                   , selected_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bak.target.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_pred_deepfm = np.zeros((len(train), ))\n",
    "y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "for fold, (tr_ind, val_ind) in tqdm(enumerate(skf.split(train, train[target]))):\n",
    "\n",
    "    # Split\n",
    "    X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "    y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "    train_model_input = [X_train.loc[:, sparse_features].values[:, k] \\\n",
    "                         for k in range(X_train.loc[:, sparse_features].values.shape[1])]\n",
    "#     train_model_input += [X_train.loc[:, dense_features].values]\n",
    "    val_model_input = [X_val.loc[:, sparse_features].values[:, k] \\\n",
    "                       for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "#     val_model_input += [X_val.loc[:, dense_features].values]\n",
    "    test_model_input = [test.loc[:, sparse_features].values[:, k] \\\n",
    "                        for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "#     test_model_input += [test.loc[:, dense_features].values]\n",
    "\n",
    "\n",
    "    # Define model\n",
    "    model = create_model(data, sparse_features, ['id'], (300, 300), 256)\n",
    "    opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "\n",
    "    # Define callbacks\n",
    "    es = callbacks.EarlyStopping(\n",
    "        monitor='val_auc', \n",
    "        min_delta=0.001, \n",
    "        patience=PATIENCE, \n",
    "        verbose=Verbose, \n",
    "        mode='max', \n",
    "        baseline=None, \n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    #sb = callbacks.ModelCheckpoint(\n",
    "     #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "    #)\n",
    "#     clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "#                        step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "#                        gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_auc', \n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=3, \n",
    "        min_lr=1e-6,\n",
    "        verbose=Verbose,\n",
    "    )\n",
    "\n",
    "#     cb = TQDMNotebookCallback()\n",
    "#     setattr(cb,'on_train_batch_begin',lambda x,y:None)\n",
    "#     setattr(cb,'on_train_batch_end',lambda x,y:None)\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_model_input, utils.to_categorical(y_train),\n",
    "        validation_data=(val_model_input, utils.to_categorical(y_val)),\n",
    "        batch_size=1024, \n",
    "        epochs=Epochs, \n",
    "        verbose=Verbose,\n",
    "        callbacks=[reduce_lr, es]\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    val_pred = model.predict(val_model_input, batch_size=512)[:, 1]\n",
    "    print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "    oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "    y_pred_deepfm += model.predict(test_model_input, batch_size=512)[:, 1].ravel() / (N_Splits)\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values[:600000], oof_pred_deepfm[:600000]), 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = test.id.values\n",
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test_idx,\n",
    "    'target': y_pred_deepfm\n",
    "})\n",
    "submission.to_csv(\"submission_kerasemb_pseudo005_cv_50.csv\", index=False)\n",
    "print(\"Submission file saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('oof_pred_kerasemb_pseudo005_cv_50.npy',oof_pred_deepfm)\n",
    "np.save('y_pred_kerasemb_pseudo005_cv_50.npy',    y_pred_deepfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_bak.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_pred_deepfm = np.zeros((len(train), ))\n",
    "y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "for fold, (tr_ind, val_ind) in tqdm(enumerate(skf.split(train, train[target]))):\n",
    "\n",
    "    # Split\n",
    "    X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "    y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "    train_model_input = [X_train.loc[:, sparse_features].values[:, k] \\\n",
    "                         for k in range(X_train.loc[:, sparse_features].values.shape[1])]\n",
    "#     train_model_input += [X_train.loc[:, dense_features].values]\n",
    "    val_model_input = [X_val.loc[:, sparse_features].values[:, k] \\\n",
    "                       for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "#     val_model_input += [X_val.loc[:, dense_features].values]\n",
    "    test_model_input = [test.loc[:, sparse_features].values[:, k] \\\n",
    "                        for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "#     test_model_input += [test.loc[:, dense_features].values]\n",
    "\n",
    "\n",
    "    # Define model\n",
    "    model = create_model(data, sparse_features, ['id'], (300, 300), 256)\n",
    "    opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "\n",
    "    # Define callbacks\n",
    "    es = callbacks.EarlyStopping(\n",
    "        monitor='val_auc', \n",
    "        min_delta=0.001, \n",
    "        patience=PATIENCE, \n",
    "        verbose=Verbose, \n",
    "        mode='max', \n",
    "        baseline=None, \n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    #sb = callbacks.ModelCheckpoint(\n",
    "     #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "    #)\n",
    "#     clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "#                        step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "#                        gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_auc', \n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=3, \n",
    "        min_lr=1e-6,\n",
    "        verbose=Verbose,\n",
    "    )\n",
    "\n",
    "#     cb = TQDMNotebookCallback()\n",
    "#     setattr(cb,'on_train_batch_begin',lambda x,y:None)\n",
    "#     setattr(cb,'on_train_batch_end',lambda x,y:None)\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_model_input, utils.to_categorical(y_train),\n",
    "        validation_data=(val_model_input, utils.to_categorical(y_val)),\n",
    "        batch_size=1024, \n",
    "        epochs=Epochs, \n",
    "        verbose=Verbose,\n",
    "        callbacks=[reduce_lr, es],\n",
    "        class_weight={0: 1, 1: 5.3417},\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    val_pred = model.predict(val_model_input, batch_size=512)[:, 1]\n",
    "    print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "    oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "    y_pred_deepfm += model.predict(test_model_input, batch_size=512)[:, 1].ravel() / (N_Splits)\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values[:600000], oof_pred_deepfm[:600000]), 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = test.id.values\n",
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test_idx,\n",
    "    'target': y_pred_deepfm\n",
    "})\n",
    "submission.to_csv(\"submission_kerasemb_weighted_cv_50.csv\", index=False)\n",
    "print(\"Submission file saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('oof_pred_kerasemb_weighted_cv_50.npy',oof_pred_deepfm)\n",
    "np.save('y_pred_kerasemb_weighted_cv_50.npy',    y_pred_deepfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['target']\n",
    "N_Splits = 5\n",
    "Verbose = 1\n",
    "Epochs = 50\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "DROPOUT = 0.2\n",
    "NNLAYERS = (256, 256, 256)\n",
    "PATIENCE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "oof_pred_deepfm = np.zeros((len(train), ))\n",
    "y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "\n",
    "for fold, (tr_ind, val_ind) in enumerate(skf.split(train, train[target])):\n",
    "    \n",
    "    # Split\n",
    "    X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "    y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "    train_model_input = {name:X_train[name] for name in feature_names}\n",
    "    val_model_input = {name:X_val[name] for name in feature_names}\n",
    "    test_model_input = {name:test[name] for name in feature_names}\n",
    "    \n",
    "    # Define model\n",
    "    model = DeepFM(sparse_feature_columns, sparse_feature_columns + dense_feature_columns,\n",
    "                   dnn_hidden_units=NNLAYERS, dnn_dropout=DROPOUT, dnn_use_bn=False, task='binary')\n",
    "    opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "    \n",
    "    # Define callbacks\n",
    "    es = callbacks.EarlyStopping(\n",
    "        monitor='val_auc', \n",
    "        min_delta=0.0, \n",
    "        patience=PATIENCE, \n",
    "        verbose=Verbose, \n",
    "        mode='max', \n",
    "        baseline=None, \n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    #sb = callbacks.ModelCheckpoint(\n",
    "     #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "    #)\n",
    "#     clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "#                        step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "#                        gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_auc', \n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=3, \n",
    "        min_lr=1e-7,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_model_input, y_train,\n",
    "        validation_data=(val_model_input, y_val),\n",
    "        batch_size=BATCH_SIZE, \n",
    "        epochs=Epochs, \n",
    "        verbose=Verbose,\n",
    "        callbacks=[reduce_lr, es]\n",
    "    )\n",
    "    \n",
    "    # Predict\n",
    "    val_pred = model.predict(val_model_input, batch_size=512)\n",
    "    print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "    oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "    y_pred_deepfm += model.predict(test_model_input, batch_size=512).ravel() / (N_Splits)\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Deeper\n",
    "* decrease LR\n",
    "* back to cyclic lr\n",
    "* decrease patience reduceLRplateau\n",
    "* add reg for DNN\n",
    "* dropout levels\n",
    "* treat emb dim\n",
    "* nunique + 1 ?\n",
    "* pseudo label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "grid_params = {\n",
    "    'dnn_dropout': [0.0, 0.1, 0.2, 0.3, 0.5],\n",
    "    'dnn_hidden_units': [(256,256, 256), (256, 256), (128, 128, 128), (256, 256, 256, 256)],\n",
    "    'emb_dim': [2, 3, 4, 8],\n",
    "    'lr': [1e-3, 1e-4],\n",
    "    'cyclic': [True, False],\n",
    "    'bn': [True, False],\n",
    "    'l2_reg_linear': [1e-05, 1e-04],\n",
    "    'l2_reg_embedding': [1e-05, 1e-04], \n",
    "    'l2_reg_dnn': [0.0, 1e-05, 1e-04]    \n",
    "}\n",
    "list_params = list(ParameterSampler(grid_params,\n",
    "                                    n_iter=50,\n",
    "                                    random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "\n",
    "model = DeepFM(sparse_feature_columns, sparse_feature_columns + dense_feature_columns,\n",
    "               dnn_hidden_units=param['dnn_hidden_units'], dnn_dropout=param['dnn_dropout'], \n",
    "               dnn_use_bn=param['bn'], task='binary',\n",
    "               l2_reg_linear=param['l2_reg_linear'], l2_reg_embedding=param['l2_reg_embedding'], \n",
    "               l2_reg_dnn=param['l2_reg_dnn'], init_std=0.0001,\n",
    "               seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [{'lr': 0.0001,\n",
    "  'l2_reg_linear': 0.0001,\n",
    "  'l2_reg_embedding': 1e-05,\n",
    "  'l2_reg_dnn': 0.0001,\n",
    "  'emb_dim': 4,\n",
    "  'dnn_hidden_units': (256, 256, 256),\n",
    "  'dnn_dropout': 0.0,\n",
    "  'cyclic': False,\n",
    "  'bn': False}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_perf = []\n",
    "\n",
    "for param in tqdm(params):\n",
    "    \n",
    "    oof_pred_deepfm = np.zeros((len(train), ))\n",
    "    y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    sparse_feature_columns = [SparseFeat(feat, train[feat].nunique() + 1, embedding_dim=param['emb_dim']) \n",
    "                          for feat in features]\n",
    "    dense_feature_columns = [DenseFeat(feat, 1) for feat in features_enc + indicator_cols]\n",
    "\n",
    "    feature_names = get_feature_names(sparse_feature_columns + dense_feature_columns)\n",
    "\n",
    "    all_features = features + features_enc + indicator_cols\n",
    "\n",
    "    for fold, (tr_ind, val_ind) in enumerate(skf.split(train, train[target])):\n",
    "\n",
    "        # Split\n",
    "        X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "        y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "        train_model_input = {name:X_train[name] for name in feature_names}\n",
    "        val_model_input = {name:X_val[name] for name in feature_names}\n",
    "        test_model_input = {name:test[name] for name in feature_names}\n",
    "\n",
    "        # Define model\n",
    "        model = DeepFM(sparse_feature_columns, sparse_feature_columns + dense_feature_columns,\n",
    "                       dnn_hidden_units=param['dnn_hidden_units'], dnn_dropout=param['dnn_dropout'], \n",
    "                       dnn_use_bn=param['bn'], task='binary',\n",
    "                       l2_reg_linear=param['l2_reg_linear'], l2_reg_embedding=param['l2_reg_embedding'], \n",
    "                       l2_reg_dnn=param['l2_reg_dnn'], init_std=0.0001,\n",
    "                       seed=SEED)\n",
    "        opt = keras.optimizers.Adam(learning_rate=param['lr'])\n",
    "        model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "\n",
    "        # Define callbacks\n",
    "        es = callbacks.EarlyStopping(\n",
    "            monitor='val_auc', \n",
    "            min_delta=0.0, \n",
    "            patience=PATIENCE, \n",
    "            verbose=Verbose, \n",
    "            mode='max', \n",
    "            baseline=None, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        #sb = callbacks.ModelCheckpoint(\n",
    "         #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "        #)\n",
    "        if param['cyclic']:\n",
    "            reduce_lr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "                           step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "                           gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "        else:\n",
    "            reduce_lr = ReduceLROnPlateau(\n",
    "                monitor='val_auc', \n",
    "                mode='max',\n",
    "                factor=0.5,\n",
    "                patience=3, \n",
    "                min_lr=1e-7,\n",
    "                verbose=True,\n",
    "            )\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train_model_input, y_train,\n",
    "            validation_data=(val_model_input, y_val),\n",
    "            batch_size=BATCH_SIZE, \n",
    "            epochs=Epochs, \n",
    "            verbose=Verbose,\n",
    "            callbacks=[reduce_lr, es]\n",
    "        )\n",
    "\n",
    "        # Predict\n",
    "        val_pred = model.predict(val_model_input, batch_size=512)\n",
    "        print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "        oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "        y_pred_deepfm += model.predict(test_model_input, batch_size=512).ravel() / (N_Splits)\n",
    "        K.clear_session()\n",
    "    cv_perf.append(round(roc_auc_score(train.target.values, oof_pred_deepfm), 5))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_ord = [feat for feat in features if 'ord' in feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_feature_columns = [SparseFeat(feat, train[feat].nunique() + 1, embedding_dim=4) \n",
    "                          for feat in features if 'ord' not in feat]\n",
    "dense_feature_columns = [DenseFeat(feat, 1) for feat in features_enc + indicator_cols + features_ord]\n",
    "\n",
    "feature_names = get_feature_names(sparse_feature_columns + dense_feature_columns)\n",
    "\n",
    "all_features = features + features_enc + indicator_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_perf = []\n",
    "\n",
    "for param in tqdm(list_params[:10]):\n",
    "    \n",
    "    oof_pred_deepfm = np.zeros((len(train), ))\n",
    "    y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    sparse_feature_columns = [SparseFeat(feat, train[feat].nunique() + 1, embedding_dim=param['emb_dim']) \n",
    "                          for feat in features]\n",
    "    dense_feature_columns = [DenseFeat(feat, 1) for feat in features_enc + indicator_cols]\n",
    "\n",
    "    feature_names = get_feature_names(sparse_feature_columns + dense_feature_columns)\n",
    "\n",
    "    all_features = features + features_enc + indicator_cols\n",
    "\n",
    "    for fold, (tr_ind, val_ind) in enumerate(skf.split(train, train[target])):\n",
    "\n",
    "        # Split\n",
    "        X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "        y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "        train_model_input = {name:X_train[name] for name in feature_names}\n",
    "        val_model_input = {name:X_val[name] for name in feature_names}\n",
    "        test_model_input = {name:test[name] for name in feature_names}\n",
    "\n",
    "        # Define model\n",
    "        model = DeepFM(sparse_feature_columns, sparse_feature_columns + dense_feature_columns,\n",
    "                       dnn_hidden_units=param['dnn_hidden_units'], dnn_dropout=param['dnn_dropout'], \n",
    "                       dnn_use_bn=param['bn'], task='binary',\n",
    "                       l2_reg_linear=param['l2_reg_linear'], l2_reg_embedding=param['l2_reg_embedding'], \n",
    "                       l2_reg_dnn=param['l2_reg_dnn'], init_std=0.0001,\n",
    "                       seed=SEED)\n",
    "        opt = keras.optimizers.Adam(learning_rate=param['lr'])\n",
    "        model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "\n",
    "        # Define callbacks\n",
    "        es = callbacks.EarlyStopping(\n",
    "            monitor='val_auc', \n",
    "            min_delta=0.0, \n",
    "            patience=PATIENCE, \n",
    "            verbose=Verbose, \n",
    "            mode='max', \n",
    "            baseline=None, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        #sb = callbacks.ModelCheckpoint(\n",
    "         #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "        #)\n",
    "        if param['cyclic']:\n",
    "            reduce_lr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "                           step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "                           gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "        else:\n",
    "            reduce_lr = ReduceLROnPlateau(\n",
    "                monitor='val_auc', \n",
    "                mode='max',\n",
    "                factor=0.5,\n",
    "                patience=3, \n",
    "                min_lr=1e-7,\n",
    "                verbose=True,\n",
    "            )\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train_model_input, y_train,\n",
    "            validation_data=(val_model_input, y_val),\n",
    "            batch_size=BATCH_SIZE, \n",
    "            epochs=Epochs, \n",
    "            verbose=Verbose,\n",
    "            callbacks=[reduce_lr, es]\n",
    "        )\n",
    "\n",
    "        # Predict\n",
    "        val_pred = model.predict(val_model_input, batch_size=512)\n",
    "        print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "        oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "        y_pred_deepfm += model.predict(test_model_input, batch_size=512).ravel() / (N_Splits)\n",
    "        K.clear_session()\n",
    "    cv_perf.append(round(roc_auc_score(train.target.values, oof_pred_deepfm), 5))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['target']\n",
    "N_Splits = 10\n",
    "Verbose = 1\n",
    "Epochs = 50\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "grid_params = {\n",
    "    'dnn_dropout': [0.0, 0.1, 0.2],\n",
    "    'dnn_hidden_units': [(256,), (512,), (256, 256), (512, 512)],\n",
    "    'linear': [linear_feature_columns, linear_feature_columns + dnn_feature_columns],\n",
    "    'sparse': [dnn_feature_columns, linear_feature_columns + dnn_feature_columns],\n",
    "    'batch_size': [128, 256, 512],\n",
    "}\n",
    "list_params = list(ParameterSampler(grid_params,\n",
    "                                    n_iter=20,\n",
    "                                    random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_perf = []\n",
    "\n",
    "for param in list_params:\n",
    "\n",
    "    oof_pred_deepfm = np.zeros((len(train), ))\n",
    "    y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "    for fold, (tr_ind, val_ind) in enumerate(skf.split(train, train[target])):\n",
    "        X_train, X_val = train[features+features_enc+indicator_cols].iloc[tr_ind], train[features+features_enc+indicator_cols].iloc[val_ind]\n",
    "        y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "        train_model_input = {name:X_train[name] for name in feature_names}\n",
    "        val_model_input = {name:X_val[name] for name in feature_names}\n",
    "        test_model_input = {name:test[name] for name in feature_names}\n",
    "        model = DeepFM(param['linear'], param['sparse'],\n",
    "                       dnn_hidden_units=param['dnn_hidden_units'], dnn_dropout=param['dnn_dropout'], dnn_use_bn=False, task='binary')\n",
    "        model.compile(\"adam\", \"binary_crossentropy\", metrics=[auc], )\n",
    "        es = callbacks.EarlyStopping(monitor='val_auc', min_delta=0.001, patience=4, verbose=Verbose, mode='max', baseline=None, restore_best_weights=True)\n",
    "        sb = callbacks.ModelCheckpoint('./nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose)\n",
    "        clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "                           step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "                           gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "        history = model.fit(train_model_input, y_train,\n",
    "                            validation_data=(val_model_input, y_val),\n",
    "                            batch_size=param['batch_size'], epochs=Epochs, verbose=Verbose,\n",
    "                            callbacks=[es, sb, clr],)\n",
    "        model.load_weights('./nn_model.w8')\n",
    "        val_pred = model.predict(val_model_input, batch_size=param['batch_size'])\n",
    "        print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "        oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "        y_pred_deepfm += model.predict(test_model_input, batch_size=param['batch_size']).ravel() / (N_Splits)\n",
    "        K.clear_session()\n",
    "    cv_perf.append(round(roc_auc_score(train.target.values, oof_pred_deepfm), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_params[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV 50 best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['target']\n",
    "N_Splits = 50\n",
    "Verbose = 1\n",
    "Epochs = 50\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_pred_deepfm = np.zeros((len(train), ))\n",
    "y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "\n",
    "for fold, (tr_ind, val_ind) in enumerate(skf.split(train, train[target])):\n",
    "    \n",
    "    # Split\n",
    "    X_train, X_val = train[features+features_enc+indicator_cols].iloc[tr_ind], train[features+features_enc+indicator_cols].iloc[val_ind]\n",
    "    y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "    train_model_input = {name:X_train[name] for name in feature_names}\n",
    "    val_model_input = {name:X_val[name] for name in feature_names}\n",
    "    test_model_input = {name:test[name] for name in feature_names}\n",
    "    \n",
    "    # Define model\n",
    "    model = DeepFM(linear_feature_columns, linear_feature_columns + dnn_feature_columns,\n",
    "                   dnn_hidden_units=(512, 512), dnn_dropout=0.0, dnn_use_bn=False, task='binary')\n",
    "    model.compile(\"adam\", \"binary_crossentropy\", metrics=[auc])\n",
    "    \n",
    "    # Define callbacks\n",
    "    es = callbacks.EarlyStopping(monitor='val_auc', min_delta=0.001, patience=4, verbose=Verbose, mode='max', baseline=None, restore_best_weights=True)\n",
    "    sb = callbacks.ModelCheckpoint('./nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose)\n",
    "#     clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "#                        step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "#                        gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_auc', factor=0.2,\n",
    "                                  patience=5, min_lr=0.00001)\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(train_model_input, y_train,\n",
    "                        validation_data=(val_model_input, y_val),\n",
    "                        batch_size=BATCH_SIZE, epochs=Epochs, verbose=Verbose,\n",
    "                        callbacks=[es, sb, reduce_lr],)\n",
    "    model.load_weights('./nn_model.w8')\n",
    "    \n",
    "    # Predict\n",
    "    val_pred = model.predict(val_model_input, batch_size=512)\n",
    "    print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "    oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "    y_pred_deepfm += model.predict(test_model_input, batch_size=512).ravel() / (N_Splits)\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = test.id.values\n",
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test_idx,\n",
    "    'target': y_pred_deepfm\n",
    "})\n",
    "submission.to_csv(\"submission_deepfm_cv_50.csv\", index=False)\n",
    "print(\"Submission file saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('oof_pred_deepfm_cv_50.npy',oof_pred_deepfm)\n",
    "np.save('y_pred_deepfm_cv_50.npy',    y_pred_deepfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "sandbox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
