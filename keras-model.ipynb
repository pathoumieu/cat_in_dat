{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 666"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/pa/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/pa/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/pa/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/pa/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/pa/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/pa/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/pa/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/pa/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/pa/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/pa/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/pa/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import MissingIndicator, SimpleImputer, IterativeImputer, KNNImputer\n",
    "from category_encoders.cat_boost import CatBoostEncoder\n",
    "from deepctr.inputs import  SparseFeat, DenseFeat, get_feature_names\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam,RMSprop\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from deepctr.models import DeepFM\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import re\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import utils\n",
    "import math\n",
    "from sklearn.metrics import precision_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('cat-in-the-dat-ii/train.csv')\n",
    "test = pd.read_csv('cat-in-the-dat-ii/test.csv')\n",
    "\n",
    "test[\"target\"] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [feat for feat in train.columns if feat not in ['target','id']]\n",
    "#train['nan_features'] = train[features].isnull().sum(axis=1)\n",
    "#test['nan_features'] = test[features].isnull().sum(axis=1)\n",
    "\n",
    "data = pd.concat([train, test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label encode and fillna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_numeric(df):\n",
    "    \n",
    "    bin_3_mapping = {'T':1 , 'F':0}\n",
    "    bin_4_mapping = {'Y':1 , 'N':0}\n",
    "    nom_0_mapping = {'Red' : 0, 'Blue' : 1, 'Green' : 2}\n",
    "    nom_1_mapping = {'Trapezoid' : 0, 'Star' : 1, 'Circle': 2, 'Triangle' : 3, 'Polygon' : 4}\n",
    "    nom_2_mapping = {'Hamster' : 0 , 'Axolotl' : 1, 'Lion' : 2, 'Dog' : 3, 'Cat' : 4, 'Snake' : 5}\n",
    "    nom_3_mapping = {'Russia' : 0, 'Canada' : 1, 'Finland' : 2, 'Costa Rica' : 3, 'China' : 4, 'India' : 5}\n",
    "    nom_4_mapping = {'Bassoon' : 0, 'Theremin' : 1, 'Oboe' : 2, 'Piano' : 3}\n",
    "    nom_5_mapping = dict(zip((df.nom_5.dropna().unique()), range(len((df.nom_5.dropna().unique())))))\n",
    "    nom_6_mapping = dict(zip((df.nom_6.dropna().unique()), range(len((df.nom_6.dropna().unique())))))\n",
    "    nom_7_mapping = dict(zip((df.nom_7.dropna().unique()), range(len((df.nom_7.dropna().unique())))))\n",
    "    nom_8_mapping = dict(zip((df.nom_8.dropna().unique()), range(len((df.nom_8.dropna().unique())))))\n",
    "    nom_9_mapping = dict(zip((df.nom_9.dropna().unique()), range(len((df.nom_9.dropna().unique())))))\n",
    "    ord_1_mapping = {'Novice' : 0, 'Contributor' : 1, 'Expert' : 2, 'Master': 3, 'Grandmaster': 4}\n",
    "    ord_2_mapping = { 'Freezing': 0, 'Cold': 1, 'Warm' : 2, 'Hot': 3, 'Boiling Hot' : 4, 'Lava Hot' : 5}\n",
    "    ord_3_mapping = {'a':0, 'b':1, 'c':2 ,'d':3 ,'e':4, 'f':5, 'g':6, 'h':7, 'i':8, 'j':9, 'k':10, 'l':11, 'm':12, 'n':13, 'o':14}\n",
    "    ord_4_mapping = {'A':0, 'B':1, 'C':2, 'D':3, 'E':4, 'F':5, 'G':6, 'H':7, 'I':8, 'J':9, 'K':10,'L':11,'M':12,\n",
    "                 'N':13,'O':14,'P':15,'Q':16,'R':17,'S':18,'T':19,'U':20,'V':21,'W':22,'X':23,'Y':24,'Z':25}\n",
    "    sorted_ord_5 = sorted(df.ord_5.dropna().unique())\n",
    "    ord_5_mapping = dict(zip(sorted_ord_5, range(len(sorted_ord_5))))\n",
    "\n",
    "    df['bin_3'] = df.loc[df.bin_3.notnull(), 'bin_3'].map(bin_3_mapping)\n",
    "    df['bin_4'] = df.loc[df.bin_4.notnull(), 'bin_4'].map(bin_4_mapping)\n",
    "    df['nom_0'] = df.loc[df.nom_0.notnull(), 'nom_0'].map(nom_0_mapping)\n",
    "    df['nom_1'] = df.loc[df.nom_1.notnull(), 'nom_1'].map(nom_1_mapping)\n",
    "    df['nom_2'] = df.loc[df.nom_2.notnull(), 'nom_2'].map(nom_2_mapping)\n",
    "    df['nom_3'] = df.loc[df.nom_3.notnull(), 'nom_3'].map(nom_3_mapping)\n",
    "    df['nom_4'] = df.loc[df.nom_4.notnull(), 'nom_4'].map(nom_4_mapping)\n",
    "    df['nom_5'] = df.loc[df.nom_5.notnull(), 'nom_5'].map(nom_5_mapping)\n",
    "    df['nom_6'] = df.loc[df.nom_6.notnull(), 'nom_6'].map(nom_6_mapping)\n",
    "    df['nom_7'] = df.loc[df.nom_7.notnull(), 'nom_7'].map(nom_7_mapping)\n",
    "    df['nom_8'] = df.loc[df.nom_8.notnull(), 'nom_8'].map(nom_8_mapping)\n",
    "    df['nom_9'] = df.loc[df.nom_9.notnull(), 'nom_9'].map(nom_9_mapping)\n",
    "    df['ord_1'] = df.loc[df.ord_1.notnull(), 'ord_1'].map(ord_1_mapping)\n",
    "    df['ord_2'] = df.loc[df.ord_2.notnull(), 'ord_2'].map(ord_2_mapping)\n",
    "    df['ord_3'] = df.loc[df.ord_3.notnull(), 'ord_3'].map(ord_3_mapping)\n",
    "    df['ord_4'] = df.loc[df.ord_4.notnull(), 'ord_4'].map(ord_4_mapping)\n",
    "    df['ord_5'] = df.loc[df.ord_5.notnull(), 'ord_5'].map(ord_5_mapping)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define sparse features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NaN count\n",
    "# data['nan_count'] = data.isnull().sum(axis=1) Problem with nan count\n",
    "\n",
    "sparse_features = [feat for feat in train.columns if feat not in ['id', 'target']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep categories present in train AND test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 values in nom_5, {'b3ad70fcb'} Replaced with nan\n",
      "4 values in nom_6, {'ee6983c6d', 'a885aacec', 'f0732a795', '3a121fefb'} Replaced with nan\n",
      "2 values in nom_9, {'3d19cd31d', '1065f10dd'} Replaced with nan\n"
     ]
    }
   ],
   "source": [
    "for col in sparse_features:\n",
    "    train_unique_values = set(train[col].dropna().unique())\n",
    "    test_unique_values  = set(test[col].dropna().unique())\n",
    "\n",
    "    symmetric_difference_values = train_unique_values.symmetric_difference(test_unique_values)\n",
    "    if symmetric_difference_values:\n",
    "        print(f'{len(symmetric_difference_values)} values in {col}, {symmetric_difference_values} Replaced with nan')\n",
    "        data.loc[data[col].isin(symmetric_difference_values), col] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fillna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2933\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2934\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2935\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2937\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_setitem_array\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2958\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Columns must be same length as key\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2960\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2961\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2962\u001b[0m                 indexer = self.loc._get_listlike_indexer(\n",
      "\u001b[0;32m~/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2937\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2938\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2996\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2997\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2998\u001b[0;31m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3000\u001b[0m         \u001b[0;31m# check if we are modifying a copy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3623\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3624\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, item, value)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blklocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m                     \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blklocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mdelete\u001b[0;34m(self, loc)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mDelete\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mplace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \"\"\"\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdelete\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mdelete\u001b[0;34m(arr, obj, axis)\u001b[0m\n\u001b[1;32m   4415\u001b[0m         \u001b[0mkeep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4416\u001b[0m         \u001b[0mslobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4417\u001b[0;31m         \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4419\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features = [feat for feat in data.columns if feat not in ['target','id']]\n",
    "data[features] = convert_data_to_numeric(data[features])\n",
    "data[features] = data[features].astype('category')\n",
    "imp = IterativeImputer(max_iter=500, initial_strategy='most_frequent', random_state=SEED, add_indicator=True)\n",
    "indicator_cols = [feat + '_ind' for feat in features]\n",
    "for col in indicator_cols:\n",
    "    data[col] = 0\n",
    "data[features+indicator_cols] = imp.fit_transform(data[features])\n",
    "data[features] = data[features].round(0).astype(np.int16)\n",
    "data[indicator_cols] = data[indicator_cols].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[features] = convert_data_to_numeric(data[features])\n",
    "data = data.fillna(-1)\n",
    "data[features] = data[features] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[data.target != -1].reset_index(drop=True)\n",
    "test  = data[data.target == -1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dense features with catboostencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/category_encoders/cat_boost.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform() missing argument: '\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/category_encoders/cat_boost.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mX_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverride_return_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_temp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/category_encoders/cat_boost.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, y, override_return_df)\u001b[0m\n\u001b[1;32m    213\u001b[0m         X = self._transform(\n\u001b[1;32m    214\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0mmapping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         )\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/category_encoders/cat_boost.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X_in, y, mapping)\u001b[0m\n\u001b[1;32m    300\u001b[0m                 \u001b[0;31m# As a workaround, we cast the grouping column as string.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                 \u001b[0;31m# See: issue #209\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cumsum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cumcount'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cumsum'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cumcount'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   5695\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5696\u001b[0m             \u001b[0;31m# else, only a single dtype is given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5697\u001b[0;31m             \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5698\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"raise\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, filter, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0mvals1d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_nansafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals1d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0;31m# e.g. astype_nansafe can fail on object-dtype of strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mastype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_datetime64_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.astype_str\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features_enc = [col + '_enc' for col in features]\n",
    "for col in features:\n",
    "    catenc = CatBoostEncoder(return_df=False, random_state=SEED)\n",
    "    train[col + '_enc'] = catenc.fit_transform(train.loc[:, col].astype('str'), train.target.values)\n",
    "    test[col + '_enc'] = catenc.transform(test.loc[:, col].astype('str'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick nan count study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tab = pd.crosstab(train.nan_count, train.target)\n",
    "# tab.T/tab.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(y_true, y_pred):\n",
    "    def fallback_auc(y_true, y_pred):\n",
    "        try:\n",
    "            return roc_auc_score(y_true, y_pred)\n",
    "        except:\n",
    "            return 0.5\n",
    "    return tf.py_function(fallback_auc, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicLR(keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1 / (2. ** (x - 1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma ** (x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "\n",
    "    def clr(self):\n",
    "        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n",
    "        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x)) * self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x)) * self.scale_fn(\n",
    "                self.clr_iterations)\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "\n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['target']\n",
    "N_Splits = 5\n",
    "Verbose = 1\n",
    "Epochs = 50\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "DROPOUT = 0.3\n",
    "NNLAYERS = (300, 300)\n",
    "PATIENCE = 5\n",
    "\n",
    "MAX_EMB_DIM = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_enc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-1a825e55cdc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdense_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures_enc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mall_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mindicator_cols\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfeatures_enc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features_enc' is not defined"
     ]
    }
   ],
   "source": [
    "dense_features = features_enc\n",
    "all_features = features + indicator_cols + features_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features += indicator_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(data, catcols, densecols, dnn_layers, last_dense):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    outputs_emb = []\n",
    "    \n",
    "    # Cat cols\n",
    "    for c in catcols:\n",
    "        \n",
    "        num_unique_values = int(data[c].nunique())\n",
    "        embed_dim = int(min(np.ceil(math.log(num_unique_values)), MAX_EMB_DIM))\n",
    "        inp = layers.Input(shape=(1,))\n",
    "        out = layers.Embedding(num_unique_values + 1, embed_dim, name=c)(inp)\n",
    "        out = layers.SpatialDropout1D(DROPOUT)(out)\n",
    "        out = layers.Reshape(target_shape=(embed_dim, ))(out)\n",
    "        inputs.append(inp)\n",
    "        outputs.append(out)\n",
    "        outputs_emb.append(out)\n",
    "        \n",
    "    # First dense for embeddings\n",
    "#     outputs_emb = layers.Concatenate()(outputs_emb)\n",
    "#     outputs_emb = layers.Dense(last_dense, activation=\"relu\")(outputs_emb)\n",
    "#     outputs_emb = layers.Dropout(DROPOUT)(outputs_emb)\n",
    "        \n",
    "    # Dense cols\n",
    "#     if densecols:\n",
    "#         dense_inp = layers.Input(shape=(len(densecols),))\n",
    "#         inputs.append(dense_inp)\n",
    "\n",
    "#         outputs.append(dense_inp)\n",
    "        \n",
    "    x = layers.Concatenate()(outputs)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # DNN layers\n",
    "    for size in dnn_layers:\n",
    "#         x = layers.Concatenate()([x, outputs_emb])\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(size, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(DROPOUT)(x)\n",
    "        \n",
    "#     x = layers.Concatenate()([x, outputs_emb])\n",
    "    \n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    y = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=y)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_cv():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    oof_pred_deepfm = np.zeros((len(train), ))\n",
    "    y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "\n",
    "    for fold, (tr_ind, val_ind) in enumerate(skf.split(train, train[target])):\n",
    "\n",
    "        # Split\n",
    "        X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "        y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "        train_model_input = [X_train.loc[:, sparse_features].values[:, k] \\\n",
    "                             for k in range(X_train.loc[:, sparse_features].values.shape[1])]\n",
    "    #     train_model_input += [X_train.loc[:, dense_features].values]\n",
    "        val_model_input = [X_val.loc[:, sparse_features].values[:, k] \\\n",
    "                           for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "    #     val_model_input += [X_val.loc[:, dense_features].values]\n",
    "        test_model_input = [test.loc[:, sparse_features].values[:, k] \\\n",
    "                            for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "    #     test_model_input += [test.loc[:, dense_features].values]\n",
    "\n",
    "\n",
    "        # Define model\n",
    "        model = create_model(data, sparse_features, ['id'], (300, 300), 256)\n",
    "        opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "\n",
    "        # Define callbacks\n",
    "        es = callbacks.EarlyStopping(\n",
    "            monitor='val_auc', \n",
    "            min_delta=0.001, \n",
    "            patience=PATIENCE, \n",
    "            verbose=Verbose, \n",
    "            mode='max', \n",
    "            baseline=None, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        #sb = callbacks.ModelCheckpoint(\n",
    "         #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "        #)\n",
    "    #     clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "    #                        step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "    #                        gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_auc', \n",
    "            mode='max',\n",
    "            factor=0.5,\n",
    "            patience=3, \n",
    "            min_lr=1e-6,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "    #     cb = TQDMNotebookCallback()\n",
    "    #     setattr(cb,'on_train_batch_begin',lambda x,y:None)\n",
    "    #     setattr(cb,'on_train_batch_end',lambda x,y:None)\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train_model_input, utils.to_categorical(y_train),\n",
    "            validation_data=(val_model_input, utils.to_categorical(y_val)),\n",
    "            batch_size=1024, \n",
    "            epochs=Epochs, \n",
    "            verbose=1,\n",
    "            callbacks=[reduce_lr, es]\n",
    "        )\n",
    "\n",
    "        # Predict\n",
    "        val_pred = model.predict(val_model_input, batch_size=512)[:, 1]\n",
    "        print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "        oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "        y_pred_deepfm += model.predict(test_model_input, batch_size=512)[:, 1].ravel() / (N_Splits)\n",
    "        K.clear_session()\n",
    "        \n",
    "    return oof_pred_deepfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 383639 samples, validate on 95910 samples\n",
      "Epoch 1/50\n",
      "383639/383639 [==============================] - 12s 33us/sample - loss: 0.4368 - auc: 0.7161 - val_loss: 0.4016 - val_auc: 0.7841\n",
      "Epoch 2/50\n",
      "383639/383639 [==============================] - 10s 26us/sample - loss: 0.4107 - auc: 0.7660 - val_loss: 0.4001 - val_auc: 0.7851\n",
      "Epoch 3/50\n",
      "383639/383639 [==============================] - 9s 24us/sample - loss: 0.4069 - auc: 0.7721 - val_loss: 0.4034 - val_auc: 0.7850\n",
      "Epoch 4/50\n",
      "383639/383639 [==============================] - 10s 26us/sample - loss: 0.4054 - auc: 0.7741 - val_loss: 0.4042 - val_auc: 0.7833\n",
      "Epoch 5/50\n",
      "381952/383639 [============================>.] - ETA: 0s - loss: 0.4038 - auc: 0.7769\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "383639/383639 [==============================] - 9s 24us/sample - loss: 0.4037 - auc: 0.7769 - val_loss: 0.4037 - val_auc: 0.7820\n",
      "Epoch 6/50\n",
      "383639/383639 [==============================] - 9s 23us/sample - loss: 0.3995 - auc: 0.7830 - val_loss: 0.4036 - val_auc: 0.7817\n",
      "Epoch 7/50\n",
      "380928/383639 [============================>.] - ETA: 0s - loss: 0.3983 - auc: 0.7851Restoring model weights from the end of the best epoch.\n",
      "383639/383639 [==============================] - 8s 22us/sample - loss: 0.3982 - auc: 0.7850 - val_loss: 0.4047 - val_auc: 0.7801\n",
      "Epoch 00007: early stopping\n",
      "validation AUC fold 1 : 0.78518\n",
      "Train on 383639 samples, validate on 95910 samples\n",
      "Epoch 1/50\n",
      "383639/383639 [==============================] - 13s 34us/sample - loss: 0.4352 - auc: 0.7185 - val_loss: 0.4013 - val_auc: 0.7831\n",
      "Epoch 2/50\n",
      "383639/383639 [==============================] - 9s 24us/sample - loss: 0.4104 - auc: 0.7667 - val_loss: 0.4001 - val_auc: 0.7855\n",
      "Epoch 3/50\n",
      "383639/383639 [==============================] - 11s 29us/sample - loss: 0.4067 - auc: 0.7723 - val_loss: 0.3992 - val_auc: 0.7857\n",
      "Epoch 4/50\n",
      "383639/383639 [==============================] - 11s 27us/sample - loss: 0.4040 - auc: 0.7766 - val_loss: 0.4008 - val_auc: 0.7840\n",
      "Epoch 5/50\n",
      "383639/383639 [==============================] - 11s 27us/sample - loss: 0.4019 - auc: 0.7796 - val_loss: 0.4019 - val_auc: 0.7835\n",
      "Epoch 6/50\n",
      "381952/383639 [============================>.] - ETA: 0s - loss: 0.4009 - auc: 0.7816\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "383639/383639 [==============================] - 11s 28us/sample - loss: 0.4009 - auc: 0.7816 - val_loss: 0.4025 - val_auc: 0.7817\n",
      "Epoch 7/50\n",
      "380928/383639 [============================>.] - ETA: 0s - loss: 0.3964 - auc: 0.7871Restoring model weights from the end of the best epoch.\n",
      "383639/383639 [==============================] - 9s 25us/sample - loss: 0.3964 - auc: 0.7872 - val_loss: 0.4052 - val_auc: 0.7796\n",
      "Epoch 00007: early stopping\n",
      "validation AUC fold 2 : 0.78533\n",
      "Train on 383639 samples, validate on 95910 samples\n",
      "Epoch 1/50\n",
      "383639/383639 [==============================] - 11s 29us/sample - loss: 0.4340 - auc: 0.7236 - val_loss: 0.4008 - val_auc: 0.7831\n",
      "Epoch 2/50\n",
      "383639/383639 [==============================] - 8s 21us/sample - loss: 0.4101 - auc: 0.7672 - val_loss: 0.4010 - val_auc: 0.7858\n",
      "Epoch 3/50\n",
      "383639/383639 [==============================] - 8s 21us/sample - loss: 0.4067 - auc: 0.7727 - val_loss: 0.3994 - val_auc: 0.7853\n",
      "Epoch 4/50\n",
      "383639/383639 [==============================] - 8s 21us/sample - loss: 0.4044 - auc: 0.7762 - val_loss: 0.3989 - val_auc: 0.7844\n",
      "Epoch 5/50\n",
      "382976/383639 [============================>.] - ETA: 0s - loss: 0.4028 - auc: 0.7785\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "383639/383639 [==============================] - 9s 24us/sample - loss: 0.4028 - auc: 0.7785 - val_loss: 0.4044 - val_auc: 0.7840\n",
      "Epoch 6/50\n",
      "383639/383639 [==============================] - 11s 28us/sample - loss: 0.3987 - auc: 0.7845 - val_loss: 0.4016 - val_auc: 0.7821\n",
      "Epoch 7/50\n",
      "380928/383639 [============================>.] - ETA: 0s - loss: 0.3967 - auc: 0.7873Restoring model weights from the end of the best epoch.\n",
      "383639/383639 [==============================] - 10s 27us/sample - loss: 0.3968 - auc: 0.7872 - val_loss: 0.4056 - val_auc: 0.7815\n",
      "Epoch 00007: early stopping\n",
      "validation AUC fold 3 : 0.78591\n",
      "Train on 383639 samples, validate on 95910 samples\n",
      "Epoch 1/50\n",
      "383639/383639 [==============================] - 14s 35us/sample - loss: 0.4340 - auc: 0.7219 - val_loss: 0.4034 - val_auc: 0.7825\n",
      "Epoch 2/50\n",
      "383639/383639 [==============================] - 10s 26us/sample - loss: 0.4098 - auc: 0.7676 - val_loss: 0.4019 - val_auc: 0.7833\n",
      "Epoch 3/50\n",
      "383639/383639 [==============================] - 10s 26us/sample - loss: 0.4066 - auc: 0.7729 - val_loss: 0.4024 - val_auc: 0.7829\n",
      "Epoch 4/50\n",
      "383639/383639 [==============================] - 10s 27us/sample - loss: 0.4038 - auc: 0.7773 - val_loss: 0.4052 - val_auc: 0.7809\n",
      "Epoch 5/50\n",
      "381952/383639 [============================>.] - ETA: 0s - loss: 0.4027 - auc: 0.7790\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "383639/383639 [==============================] - 11s 30us/sample - loss: 0.4026 - auc: 0.7789 - val_loss: 0.4015 - val_auc: 0.7810\n",
      "Epoch 6/50\n",
      "381952/383639 [============================>.] - ETA: 0s - loss: 0.3982 - auc: 0.7847Restoring model weights from the end of the best epoch.\n",
      "383639/383639 [==============================] - 11s 28us/sample - loss: 0.3982 - auc: 0.7848 - val_loss: 0.4066 - val_auc: 0.7800\n",
      "Epoch 00006: early stopping\n",
      "validation AUC fold 4 : 0.78226\n",
      "Train on 383640 samples, validate on 95909 samples\n",
      "Epoch 1/50\n",
      "383640/383640 [==============================] - 15s 39us/sample - loss: 0.4349 - auc: 0.7198 - val_loss: 0.4020 - val_auc: 0.7835\n",
      "Epoch 2/50\n",
      "383640/383640 [==============================] - 10s 26us/sample - loss: 0.4100 - auc: 0.7675 - val_loss: 0.3997 - val_auc: 0.7856\n",
      "Epoch 3/50\n",
      "383640/383640 [==============================] - 10s 26us/sample - loss: 0.4063 - auc: 0.7731 - val_loss: 0.4005 - val_auc: 0.7844\n",
      "Epoch 4/50\n",
      "383640/383640 [==============================] - 11s 27us/sample - loss: 0.4044 - auc: 0.7757 - val_loss: 0.4000 - val_auc: 0.7842\n",
      "Epoch 5/50\n",
      "382976/383640 [============================>.] - ETA: 0s - loss: 0.4025 - auc: 0.7787\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "383640/383640 [==============================] - 10s 27us/sample - loss: 0.4025 - auc: 0.7787 - val_loss: 0.4002 - val_auc: 0.7838\n",
      "Epoch 6/50\n",
      "383640/383640 [==============================] - 10s 25us/sample - loss: 0.3989 - auc: 0.7839 - val_loss: 0.4004 - val_auc: 0.7826\n",
      "Epoch 7/50\n",
      "381952/383640 [============================>.] - ETA: 0s - loss: 0.3975 - auc: 0.7858Restoring model weights from the end of the best epoch.\n",
      "383640/383640 [==============================] - 11s 30us/sample - loss: 0.3976 - auc: 0.7858 - val_loss: 0.4018 - val_auc: 0.7820\n",
      "Epoch 00007: early stopping\n",
      "validation AUC fold 5 : 0.78572\n"
     ]
    }
   ],
   "source": [
    "oof_pred_deepfm = run_cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF AUC : 0.78466\n"
     ]
    }
   ],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "#ln dim maxembdim 50 no BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF AUC : 0.78431\n"
     ]
    }
   ],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "#ln dim maxembdim 20 no BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF AUC : 0.78399\n"
     ]
    }
   ],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "#ln dim maxembdim 200 no BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF AUC : 0.78487\n"
     ]
    }
   ],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "#ln dim maxembdim 100 no BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF AUC : 0.78306\n"
     ]
    }
   ],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "# No BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF AUC : 0.78296\n"
     ]
    }
   ],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF AUC : 0.78375\n"
     ]
    }
   ],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.7838"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "del oof_pred_deepfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7595"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* test with nan_count\n",
    "* add dense features OK\n",
    "* test with ln instead of *0.5 OK\n",
    "* increase MAX_EMB_DIM OK\n",
    "* archi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['target']\n",
    "N_Splits = 5\n",
    "Verbose = 1\n",
    "Epochs = 50\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "DROPOUT = 0.3\n",
    "NNLAYERS = (300, 300)\n",
    "PATIENCE = 5\n",
    "\n",
    "MAX_EMB_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dense_features = features_enc\n",
    "all_features = features + features_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(data, catcols, densecols, dnn_layers, last_dense):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    dense_out = []\n",
    "    \n",
    "    # Cat cols\n",
    "    for c in catcols:\n",
    "        \n",
    "        num_unique_values = int(data[c].nunique())\n",
    "        embed_dim = int(min(np.ceil(math.log(num_unique_values)), MAX_EMB_DIM))\n",
    "        inp = layers.Input(shape=(1,))\n",
    "        out = layers.Embedding(num_unique_values + 1, embed_dim, name=c)(inp)\n",
    "        out = layers.SpatialDropout1D(DROPOUT)(out)\n",
    "        out = layers.Reshape(target_shape=(embed_dim, ))(out)\n",
    "        inputs.append(inp)\n",
    "        outputs.append(out)\n",
    "        \n",
    "#     First dense for embeddings\n",
    "#     outputs_emb = layers.Concatenate()(outputs_emb)\n",
    "#     outputs_emb = layers.Dense(last_dense, activation=\"relu\")(outputs_emb)\n",
    "#     outputs_emb = layers.Dropout(DROPOUT)(outputs_emb)\n",
    "\n",
    "#     outputs_emb = layers.Concatenate()(outputs_emb)\n",
    "#     outputs_emb = layers.Dense(last_dense, activation=\"relu\")(outputs_emb)\n",
    "#     outputs_emb = layers.Dropout(DROPOUT)(outputs_emb)\n",
    "        \n",
    "    # Dense cols\n",
    "    if densecols:\n",
    "        dense_inp = layers.Input(shape=(len(densecols),))\n",
    "        inputs.append(dense_inp)\n",
    "\n",
    "    dense_inp = layers.Dense(\n",
    "        last_dense, \n",
    "        activation=\"relu\",\n",
    "        kernel_regularizer=keras.regularizers.l2(0.01)\n",
    "    )(dense_inp)\n",
    "    dense_inp = layers.Dropout(0.5)(dense_inp)\n",
    "        \n",
    "    x = layers.Concatenate()(outputs)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # DNN layers\n",
    "    for size in dnn_layers:\n",
    "#         x = layers.Concatenate()([x, outputs_emb])\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(size, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(DROPOUT)(x)\n",
    "        \n",
    "    x = layers.Concatenate()([x, dense_inp])\n",
    "    \n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    y = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=y)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_cv():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    oof_pred_deepfm = np.zeros((len(train), ))\n",
    "    y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "\n",
    "    for fold, (tr_ind, val_ind) in enumerate(skf.split(train, train[target])):\n",
    "\n",
    "        # Split\n",
    "        X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "        y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "        train_model_input = [X_train.loc[:, sparse_features].values[:, k] \\\n",
    "                             for k in range(X_train.loc[:, sparse_features].values.shape[1])]\n",
    "        train_model_input += [X_train.loc[:, dense_features].values]\n",
    "        val_model_input = [X_val.loc[:, sparse_features].values[:, k] \\\n",
    "                           for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "        val_model_input += [X_val.loc[:, dense_features].values]\n",
    "        test_model_input = [test.loc[:, sparse_features].values[:, k] \\\n",
    "                            for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "        test_model_input += [test.loc[:, dense_features].values]\n",
    "\n",
    "\n",
    "        # Define model\n",
    "        model = create_model(data, sparse_features, dense_features, (300, 300), 32)\n",
    "        opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "\n",
    "        # Define callbacks\n",
    "        es = callbacks.EarlyStopping(\n",
    "            monitor='val_auc', \n",
    "            min_delta=0.001, \n",
    "            patience=PATIENCE, \n",
    "            verbose=Verbose, \n",
    "            mode='max', \n",
    "            baseline=None, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        #sb = callbacks.ModelCheckpoint(\n",
    "         #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "        #)\n",
    "    #     clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "    #                        step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "    #                        gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_auc', \n",
    "            mode='max',\n",
    "            factor=0.5,\n",
    "            patience=3, \n",
    "            min_lr=1e-6,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "    #     cb = TQDMNotebookCallback()\n",
    "    #     setattr(cb,'on_train_batch_begin',lambda x,y:None)\n",
    "    #     setattr(cb,'on_train_batch_end',lambda x,y:None)\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train_model_input, utils.to_categorical(y_train),\n",
    "            validation_data=(val_model_input, utils.to_categorical(y_val)),\n",
    "            batch_size=1024, \n",
    "            epochs=Epochs, \n",
    "            verbose=1,\n",
    "            callbacks=[reduce_lr, es]\n",
    "        )\n",
    "\n",
    "        # Predict\n",
    "        val_pred = model.predict(val_model_input, batch_size=512)[:, 1]\n",
    "        print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "        oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "        y_pred_deepfm += model.predict(test_model_input, batch_size=512)[:, 1].ravel() / (N_Splits)\n",
    "        K.clear_session()\n",
    "        \n",
    "    return oof_pred_deepfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 383639 samples, validate on 95910 samples\n",
      "Epoch 1/50\n",
      "383639/383639 [==============================] - 17s 45us/sample - loss: 0.5121 - auc: 0.7171 - val_loss: 0.4072 - val_auc: 0.7839\n",
      "Epoch 2/50\n",
      "383639/383639 [==============================] - 11s 28us/sample - loss: 0.4113 - auc: 0.7665 - val_loss: 0.3991 - val_auc: 0.7861\n",
      "Epoch 3/50\n",
      "383639/383639 [==============================] - 11s 28us/sample - loss: 0.4064 - auc: 0.7730 - val_loss: 0.4012 - val_auc: 0.7849\n",
      "Epoch 4/50\n",
      "383639/383639 [==============================] - 11s 28us/sample - loss: 0.4047 - auc: 0.7754 - val_loss: 0.4010 - val_auc: 0.7839\n",
      "Epoch 5/50\n",
      "382976/383639 [============================>.] - ETA: 0s - loss: 0.4025 - auc: 0.7788\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "383639/383639 [==============================] - 11s 28us/sample - loss: 0.4025 - auc: 0.7789 - val_loss: 0.4016 - val_auc: 0.7829\n",
      "Epoch 6/50\n",
      "383639/383639 [==============================] - 11s 28us/sample - loss: 0.3984 - auc: 0.7847 - val_loss: 0.4041 - val_auc: 0.7808\n",
      "Epoch 7/50\n",
      "381952/383639 [============================>.] - ETA: 0s - loss: 0.3974 - auc: 0.7863Restoring model weights from the end of the best epoch.\n",
      "383639/383639 [==============================] - 11s 28us/sample - loss: 0.3974 - auc: 0.7863 - val_loss: 0.4071 - val_auc: 0.7800\n",
      "Epoch 00007: early stopping\n",
      "validation AUC fold 1 : 0.78585\n",
      "Train on 383639 samples, validate on 95910 samples\n",
      "Epoch 1/50\n",
      "383639/383639 [==============================] - 18s 46us/sample - loss: 0.5094 - auc: 0.7204 - val_loss: 0.4088 - val_auc: 0.7840\n",
      "Epoch 2/50\n",
      "383639/383639 [==============================] - 11s 28us/sample - loss: 0.4108 - auc: 0.7677 - val_loss: 0.4014 - val_auc: 0.7856\n",
      "Epoch 3/50\n",
      "383639/383639 [==============================] - 11s 28us/sample - loss: 0.4061 - auc: 0.7736 - val_loss: 0.4004 - val_auc: 0.7852\n",
      "Epoch 4/50\n",
      "383639/383639 [==============================] - 11s 28us/sample - loss: 0.4041 - auc: 0.7765 - val_loss: 0.4027 - val_auc: 0.7845\n",
      "Epoch 5/50\n",
      "380928/383639 [============================>.] - ETA: 0s - loss: 0.4027 - auc: 0.7787\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "383639/383639 [==============================] - 11s 27us/sample - loss: 0.4027 - auc: 0.7786 - val_loss: 0.4013 - val_auc: 0.7831\n",
      "Epoch 6/50\n",
      "383639/383639 [==============================] - 9s 23us/sample - loss: 0.3986 - auc: 0.7846 - val_loss: 0.4019 - val_auc: 0.7820\n",
      "Epoch 7/50\n",
      "381952/383639 [============================>.] - ETA: 0s - loss: 0.3973 - auc: 0.7862Restoring model weights from the end of the best epoch.\n",
      "383639/383639 [==============================] - 9s 22us/sample - loss: 0.3974 - auc: 0.7860 - val_loss: 0.4041 - val_auc: 0.7807\n",
      "Epoch 00007: early stopping\n",
      "validation AUC fold 2 : 0.78582\n",
      "Train on 383639 samples, validate on 95910 samples\n",
      "Epoch 1/50\n",
      "383639/383639 [==============================] - 13s 34us/sample - loss: 0.5056 - auc: 0.7162 - val_loss: 0.4064 - val_auc: 0.7833\n",
      "Epoch 2/50\n",
      "383639/383639 [==============================] - 9s 24us/sample - loss: 0.4109 - auc: 0.7670 - val_loss: 0.3995 - val_auc: 0.7857\n",
      "Epoch 3/50\n",
      "383639/383639 [==============================] - 9s 22us/sample - loss: 0.4067 - auc: 0.7723 - val_loss: 0.4011 - val_auc: 0.7852\n",
      "Epoch 4/50\n",
      "383639/383639 [==============================] - 9s 23us/sample - loss: 0.4045 - auc: 0.7759 - val_loss: 0.4026 - val_auc: 0.7834\n",
      "Epoch 5/50\n",
      "381952/383639 [============================>.] - ETA: 0s - loss: 0.4027 - auc: 0.7787\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "383639/383639 [==============================] - 9s 23us/sample - loss: 0.4026 - auc: 0.7789 - val_loss: 0.4005 - val_auc: 0.7843\n",
      "Epoch 6/50\n",
      "383639/383639 [==============================] - 9s 23us/sample - loss: 0.3985 - auc: 0.7847 - val_loss: 0.4013 - val_auc: 0.7828\n",
      "Epoch 7/50\n",
      "382976/383639 [============================>.] - ETA: 0s - loss: 0.3973 - auc: 0.7864Restoring model weights from the end of the best epoch.\n",
      "383639/383639 [==============================] - 9s 23us/sample - loss: 0.3973 - auc: 0.7865 - val_loss: 0.4041 - val_auc: 0.7808\n",
      "Epoch 00007: early stopping\n",
      "validation AUC fold 3 : 0.78591\n",
      "Train on 383639 samples, validate on 95910 samples\n",
      "Epoch 1/50\n",
      "383639/383639 [==============================] - 12s 31us/sample - loss: 0.5065 - auc: 0.7217 - val_loss: 0.4057 - val_auc: 0.7820\n",
      "Epoch 2/50\n",
      "383639/383639 [==============================] - 9s 23us/sample - loss: 0.4102 - auc: 0.7682 - val_loss: 0.4010 - val_auc: 0.7827\n",
      "Epoch 3/50\n",
      "383639/383639 [==============================] - 9s 22us/sample - loss: 0.4056 - auc: 0.7745 - val_loss: 0.4005 - val_auc: 0.7822\n",
      "Epoch 4/50\n",
      "383639/383639 [==============================] - 9s 23us/sample - loss: 0.4037 - auc: 0.7770 - val_loss: 0.4013 - val_auc: 0.7813\n",
      "Epoch 5/50\n",
      "381952/383639 [============================>.] - ETA: 0s - loss: 0.4024 - auc: 0.7793\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "383639/383639 [==============================] - 9s 23us/sample - loss: 0.4024 - auc: 0.7794 - val_loss: 0.4033 - val_auc: 0.7806\n",
      "Epoch 6/50\n",
      "381952/383639 [============================>.] - ETA: 0s - loss: 0.3987 - auc: 0.7843Restoring model weights from the end of the best epoch.\n",
      "383639/383639 [==============================] - 9s 23us/sample - loss: 0.3986 - auc: 0.7843 - val_loss: 0.4033 - val_auc: 0.7793\n",
      "Epoch 00006: early stopping\n",
      "validation AUC fold 4 : 0.78179\n",
      "Train on 383640 samples, validate on 95909 samples\n",
      "Epoch 1/50\n",
      "383640/383640 [==============================] - 12s 32us/sample - loss: 0.5097 - auc: 0.7188 - val_loss: 0.4055 - val_auc: 0.7833\n",
      "Epoch 2/50\n",
      "383640/383640 [==============================] - 10s 27us/sample - loss: 0.4109 - auc: 0.7673 - val_loss: 0.4010 - val_auc: 0.7852\n",
      "Epoch 3/50\n",
      "383640/383640 [==============================] - 12s 32us/sample - loss: 0.4062 - auc: 0.7732 - val_loss: 0.3998 - val_auc: 0.7847\n",
      "Epoch 4/50\n",
      "383640/383640 [==============================] - 12s 31us/sample - loss: 0.4039 - auc: 0.7770 - val_loss: 0.4013 - val_auc: 0.7832\n",
      "Epoch 5/50\n",
      "382976/383640 [============================>.] - ETA: 0s - loss: 0.4019 - auc: 0.7795\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "383640/383640 [==============================] - 12s 32us/sample - loss: 0.4019 - auc: 0.7795 - val_loss: 0.4004 - val_auc: 0.7820\n",
      "Epoch 6/50\n",
      "383640/383640 [==============================] - 11s 29us/sample - loss: 0.3980 - auc: 0.7852 - val_loss: 0.4017 - val_auc: 0.7813\n",
      "Epoch 7/50\n",
      "381952/383640 [============================>.] - ETA: 0s - loss: 0.3970 - auc: 0.7866Restoring model weights from the end of the best epoch.\n",
      "383640/383640 [==============================] - 12s 32us/sample - loss: 0.3969 - auc: 0.7866 - val_loss: 0.4036 - val_auc: 0.7805\n",
      "Epoch 00007: early stopping\n",
      "validation AUC fold 5 : 0.78496\n"
     ]
    }
   ],
   "source": [
    "oof_pred_deepfm = run_cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF AUC : 0.78444\n"
     ]
    }
   ],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "#ln dim maxembdim 100 no BN (300, 300), 32 reg 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF AUC : 0.78476\n"
     ]
    }
   ],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "#ln dim maxembdim 100 no BN (300, 300), 128 reg 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF AUC : 0.78412\n"
     ]
    }
   ],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "#ln dim maxembdim 100 no BN (300, 300), 128 no reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* add nan_features OK\n",
    "* archi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['target']\n",
    "N_Splits = 5\n",
    "Verbose = 1\n",
    "Epochs = 50\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "DROPOUT = 0.3\n",
    "NNLAYERS = (300, 300)\n",
    "PATIENCE = 5\n",
    "\n",
    "MAX_EMB_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_enc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-1a825e55cdc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdense_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures_enc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mall_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mindicator_cols\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfeatures_enc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features_enc' is not defined"
     ]
    }
   ],
   "source": [
    "dense_features = features_enc\n",
    "all_features = features + indicator_cols + features_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features += indicator_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = features + ['nan_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(data, catcols, densecols, dnn_layers, last_dense):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    outputs_emb = []\n",
    "    \n",
    "    # Cat cols\n",
    "    for c in catcols:\n",
    "        \n",
    "        num_unique_values = int(data[c].nunique())\n",
    "        embed_dim = int(min(np.ceil(math.log(num_unique_values)), MAX_EMB_DIM))\n",
    "        inp = layers.Input(shape=(1,))\n",
    "        out = layers.Embedding(num_unique_values + 1, embed_dim, name=c)(inp)\n",
    "        out = layers.SpatialDropout1D(DROPOUT)(out)\n",
    "        out = layers.Reshape(target_shape=(embed_dim, ))(out)\n",
    "        inputs.append(inp)\n",
    "        outputs.append(out)\n",
    "        outputs_emb.append(out)\n",
    "        \n",
    "    # First dense for embeddings\n",
    "#     outputs_emb = layers.Concatenate()(outputs_emb)\n",
    "#     outputs_emb = layers.Dense(last_dense, activation=\"relu\")(outputs_emb)\n",
    "#     outputs_emb = layers.Dropout(DROPOUT)(outputs_emb)\n",
    "        \n",
    "    # Dense cols\n",
    "#     if densecols:\n",
    "#         dense_inp = layers.Input(shape=(len(densecols),))\n",
    "#         inputs.append(dense_inp)\n",
    "\n",
    "#         outputs.append(dense_inp)\n",
    "        \n",
    "    x = layers.Concatenate()(outputs)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # DNN layers\n",
    "    for size in dnn_layers:\n",
    "#         x = layers.Concatenate()([x, outputs_emb])\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(size, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(DROPOUT)(x)\n",
    "        \n",
    "#     x = layers.Concatenate()([x, outputs_emb])\n",
    "    \n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    y = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=y)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.nan_features = train.nan_features.replace({-1: 0})\n",
    "test.nan_features = test.nan_features.replace({-1: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features = sparse_features[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_cv():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    oof_pred_deepfm = np.zeros((len(train), ))\n",
    "    y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "\n",
    "    for fold, (tr_ind, val_ind) in enumerate(skf.split(train, train[target])):\n",
    "\n",
    "        # Split\n",
    "        X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "        y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "        train_model_input = [X_train.loc[:, sparse_features].values[:, k] \\\n",
    "                             for k in range(X_train.loc[:, sparse_features].values.shape[1])]\n",
    "    #     train_model_input += [X_train.loc[:, dense_features].values]\n",
    "        val_model_input = [X_val.loc[:, sparse_features].values[:, k] \\\n",
    "                           for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "    #     val_model_input += [X_val.loc[:, dense_features].values]\n",
    "        test_model_input = [test.loc[:, sparse_features].values[:, k] \\\n",
    "                            for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "    #     test_model_input += [test.loc[:, dense_features].values]\n",
    "\n",
    "\n",
    "        # Define model\n",
    "        model = create_model(data, sparse_features, ['id'], (300, 300), 256)\n",
    "        opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "\n",
    "        # Define callbacks\n",
    "        es = callbacks.EarlyStopping(\n",
    "            monitor='val_auc', \n",
    "            min_delta=0.001, \n",
    "            patience=PATIENCE, \n",
    "            verbose=Verbose, \n",
    "            mode='max', \n",
    "            baseline=None, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        #sb = callbacks.ModelCheckpoint(\n",
    "         #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "        #)\n",
    "    #     clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "    #                        step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "    #                        gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_auc', \n",
    "            mode='max',\n",
    "            factor=0.5,\n",
    "            patience=3, \n",
    "            min_lr=1e-6,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "    #     cb = TQDMNotebookCallback()\n",
    "    #     setattr(cb,'on_train_batch_begin',lambda x,y:None)\n",
    "    #     setattr(cb,'on_train_batch_end',lambda x,y:None)\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train_model_input, utils.to_categorical(y_train),\n",
    "            validation_data=(val_model_input, utils.to_categorical(y_val)),\n",
    "            batch_size=1024, \n",
    "            epochs=Epochs, \n",
    "            verbose=1,\n",
    "            callbacks=[reduce_lr, es]\n",
    "        )\n",
    "\n",
    "        # Predict\n",
    "        val_pred = model.predict(val_model_input, batch_size=512)[:, 1]\n",
    "        print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "        oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "        y_pred_deepfm += model.predict(test_model_input, batch_size=512)[:, 1].ravel() / (N_Splits)\n",
    "        K.clear_session()\n",
    "        \n",
    "    return oof_pred_deepfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 383639 samples, validate on 95910 samples\n",
      "Epoch 1/50\n",
      "383639/383639 [==============================] - 13s 33us/sample - loss: 0.4357 - auc: 0.7173 - val_loss: 0.4011 - val_auc: 0.7847\n",
      "Epoch 2/50\n",
      "383639/383639 [==============================] - 8s 22us/sample - loss: 0.4106 - auc: 0.7664 - val_loss: 0.3985 - val_auc: 0.7852\n",
      "Epoch 3/50\n",
      "383639/383639 [==============================] - 9s 24us/sample - loss: 0.4060 - auc: 0.7739 - val_loss: 0.3993 - val_auc: 0.7855\n",
      "Epoch 4/50\n",
      "383639/383639 [==============================] - 9s 23us/sample - loss: 0.4048 - auc: 0.7753 - val_loss: 0.4030 - val_auc: 0.7842\n",
      "Epoch 5/50\n",
      "383639/383639 [==============================] - 9s 24us/sample - loss: 0.4027 - auc: 0.7789 - val_loss: 0.4013 - val_auc: 0.7831\n",
      "Epoch 6/50\n",
      "382976/383639 [============================>.] - ETA: 0s - loss: 0.4009 - auc: 0.7810\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "383639/383639 [==============================] - 9s 23us/sample - loss: 0.4009 - auc: 0.7811 - val_loss: 0.4028 - val_auc: 0.7820\n",
      "Epoch 00006: early stopping\n",
      "validation AUC fold 1 : 0.78483\n",
      "Train on 383639 samples, validate on 95910 samples\n",
      "Epoch 1/50\n",
      "383639/383639 [==============================] - 13s 33us/sample - loss: 0.4338 - auc: 0.7240 - val_loss: 0.4014 - val_auc: 0.7837\n",
      "Epoch 2/50\n",
      "383639/383639 [==============================] - 9s 22us/sample - loss: 0.4103 - auc: 0.7664 - val_loss: 0.4025 - val_auc: 0.7855\n",
      "Epoch 3/50\n",
      "383639/383639 [==============================] - 9s 22us/sample - loss: 0.4062 - auc: 0.7730 - val_loss: 0.4030 - val_auc: 0.7848\n",
      "Epoch 4/50\n",
      "383639/383639 [==============================] - 9s 24us/sample - loss: 0.4044 - auc: 0.7764 - val_loss: 0.4022 - val_auc: 0.7837\n",
      "Epoch 5/50\n",
      "381952/383639 [============================>.] - ETA: 0s - loss: 0.4024 - auc: 0.7792\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "383639/383639 [==============================] - 12s 31us/sample - loss: 0.4023 - auc: 0.7791 - val_loss: 0.4029 - val_auc: 0.7823\n",
      "Epoch 6/50\n",
      "383639/383639 [==============================] - 12s 30us/sample - loss: 0.3985 - auc: 0.7843 - val_loss: 0.4046 - val_auc: 0.7818\n",
      "Epoch 7/50\n",
      "382976/383639 [============================>.] - ETA: 0s - loss: 0.3978 - auc: 0.7856Restoring model weights from the end of the best epoch.\n",
      "383639/383639 [==============================] - 12s 30us/sample - loss: 0.3978 - auc: 0.7855 - val_loss: 0.4038 - val_auc: 0.7806\n",
      "Epoch 00007: early stopping\n",
      "validation AUC fold 2 : 0.78542\n",
      "Train on 383639 samples, validate on 95910 samples\n",
      "Epoch 1/50\n",
      "383639/383639 [==============================] - 17s 43us/sample - loss: 0.4329 - auc: 0.7249 - val_loss: 0.4022 - val_auc: 0.7821\n",
      "Epoch 2/50\n",
      "383639/383639 [==============================] - 12s 32us/sample - loss: 0.4095 - auc: 0.7681 - val_loss: 0.4024 - val_auc: 0.7860\n",
      "Epoch 3/50\n",
      "383639/383639 [==============================] - 12s 32us/sample - loss: 0.4066 - auc: 0.7727 - val_loss: 0.4002 - val_auc: 0.7857\n",
      "Epoch 4/50\n",
      "383639/383639 [==============================] - 12s 30us/sample - loss: 0.4041 - auc: 0.7763 - val_loss: 0.3986 - val_auc: 0.7849\n",
      "Epoch 5/50\n",
      "381952/383639 [============================>.] - ETA: 0s - loss: 0.4026 - auc: 0.7788\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "383639/383639 [==============================] - 15s 40us/sample - loss: 0.4025 - auc: 0.7789 - val_loss: 0.4030 - val_auc: 0.7832\n",
      "Epoch 6/50\n",
      "383639/383639 [==============================] - 13s 35us/sample - loss: 0.3983 - auc: 0.7848 - val_loss: 0.4039 - val_auc: 0.7814\n",
      "Epoch 7/50\n",
      "380928/383639 [============================>.] - ETA: 0s - loss: 0.3967 - auc: 0.7871Restoring model weights from the end of the best epoch.\n",
      "383639/383639 [==============================] - 8s 20us/sample - loss: 0.3966 - auc: 0.7871 - val_loss: 0.4040 - val_auc: 0.7810\n",
      "Epoch 00007: early stopping\n",
      "validation AUC fold 3 : 0.78611\n",
      "Train on 383639 samples, validate on 95910 samples\n",
      "Epoch 1/50\n",
      "383639/383639 [==============================] - 11s 29us/sample - loss: 0.4333 - auc: 0.7219 - val_loss: 0.4025 - val_auc: 0.7819\n",
      "Epoch 2/50\n",
      "383639/383639 [==============================] - 8s 20us/sample - loss: 0.4101 - auc: 0.7673 - val_loss: 0.4008 - val_auc: 0.7839\n",
      "Epoch 3/50\n",
      "383639/383639 [==============================] - 8s 20us/sample - loss: 0.4063 - auc: 0.7738 - val_loss: 0.4029 - val_auc: 0.7826\n",
      "Epoch 4/50\n",
      "383639/383639 [==============================] - 9s 23us/sample - loss: 0.4036 - auc: 0.7774 - val_loss: 0.4010 - val_auc: 0.7830\n",
      "Epoch 5/50\n",
      "382976/383639 [============================>.] - ETA: 0s - loss: 0.4020 - auc: 0.7797\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "383639/383639 [==============================] - 9s 24us/sample - loss: 0.4020 - auc: 0.7797 - val_loss: 0.4022 - val_auc: 0.7798\n",
      "Epoch 6/50\n",
      "383639/383639 [==============================] - 9s 22us/sample - loss: 0.3982 - auc: 0.7855 - val_loss: 0.4056 - val_auc: 0.7793\n",
      "Epoch 7/50\n",
      "381952/383639 [============================>.] - ETA: 0s - loss: 0.3972 - auc: 0.7868Restoring model weights from the end of the best epoch.\n",
      "383639/383639 [==============================] - 8s 20us/sample - loss: 0.3973 - auc: 0.7866 - val_loss: 0.4071 - val_auc: 0.7779\n",
      "Epoch 00007: early stopping\n",
      "validation AUC fold 4 : 0.78375\n",
      "Train on 383640 samples, validate on 95909 samples\n",
      "Epoch 1/50\n",
      "383640/383640 [==============================] - 11s 28us/sample - loss: 0.4343 - auc: 0.7222 - val_loss: 0.4021 - val_auc: 0.7835\n",
      "Epoch 2/50\n",
      "383640/383640 [==============================] - 8s 20us/sample - loss: 0.4097 - auc: 0.7675 - val_loss: 0.3986 - val_auc: 0.7857\n",
      "Epoch 3/50\n",
      "383640/383640 [==============================] - 8s 21us/sample - loss: 0.4066 - auc: 0.7725 - val_loss: 0.3998 - val_auc: 0.7846\n",
      "Epoch 4/50\n",
      "383640/383640 [==============================] - 8s 20us/sample - loss: 0.4041 - auc: 0.7766 - val_loss: 0.4008 - val_auc: 0.7842\n",
      "Epoch 5/50\n",
      "381952/383640 [============================>.] - ETA: 0s - loss: 0.4030 - auc: 0.7779\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "383640/383640 [==============================] - 8s 20us/sample - loss: 0.4030 - auc: 0.7778 - val_loss: 0.3997 - val_auc: 0.7835\n",
      "Epoch 6/50\n",
      "383640/383640 [==============================] - 8s 21us/sample - loss: 0.3990 - auc: 0.7836 - val_loss: 0.4038 - val_auc: 0.7819\n",
      "Epoch 7/50\n",
      "381952/383640 [============================>.] - ETA: 0s - loss: 0.3976 - auc: 0.7856Restoring model weights from the end of the best epoch.\n",
      "383640/383640 [==============================] - 8s 20us/sample - loss: 0.3975 - auc: 0.7857 - val_loss: 0.4036 - val_auc: 0.7811\n",
      "Epoch 00007: early stopping\n",
      "validation AUC fold 5 : 0.7856\n"
     ]
    }
   ],
   "source": [
    "oof_pred_deepfm = run_cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF AUC : 0.78435\n"
     ]
    }
   ],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")\n",
    "#ln dim maxembdim 100 no BN (300, 300) nan features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['target']\n",
    "N_Splits = 50\n",
    "Verbose = 0\n",
    "Epochs = 50\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "DROPOUT = 0.3\n",
    "NNLAYERS = (300, 300)\n",
    "PATIENCE = 5\n",
    "\n",
    "MAX_EMB_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(data, catcols, densecols, dnn_layers, last_dense):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    outputs_emb = []\n",
    "    \n",
    "    # Cat cols\n",
    "    for c in catcols:\n",
    "        \n",
    "        num_unique_values = int(data[c].nunique())\n",
    "        embed_dim = int(min(np.ceil(math.log(num_unique_values)), MAX_EMB_DIM))\n",
    "        inp = layers.Input(shape=(1,))\n",
    "        out = layers.Embedding(num_unique_values + 1, embed_dim, name=c)(inp)\n",
    "        out = layers.SpatialDropout1D(DROPOUT)(out)\n",
    "        out = layers.Reshape(target_shape=(embed_dim, ))(out)\n",
    "        inputs.append(inp)\n",
    "        outputs.append(out)\n",
    "        outputs_emb.append(out)\n",
    "        \n",
    "    # First dense for embeddings\n",
    "#     outputs_emb = layers.Concatenate()(outputs_emb)\n",
    "#     outputs_emb = layers.Dense(last_dense, activation=\"relu\")(outputs_emb)\n",
    "#     outputs_emb = layers.Dropout(DROPOUT)(outputs_emb)\n",
    "        \n",
    "    # Dense cols\n",
    "#     if densecols:\n",
    "#         dense_inp = layers.Input(shape=(len(densecols),))\n",
    "#         inputs.append(dense_inp)\n",
    "\n",
    "#         outputs.append(dense_inp)\n",
    "        \n",
    "    x = layers.Concatenate()(outputs)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # DNN layers\n",
    "    for size in dnn_layers:\n",
    "#         x = layers.Concatenate()([x, outputs_emb])\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(size, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(DROPOUT)(x)\n",
    "        \n",
    "#     x = layers.Concatenate()([x, outputs_emb])\n",
    "    \n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    y = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=y)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da7353fc8ec64ec09991059ff4634454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation AUC fold 1 : 0.78683\n",
      "validation AUC fold 2 : 0.79335\n",
      "validation AUC fold 3 : 0.78759\n",
      "validation AUC fold 4 : 0.79131\n",
      "validation AUC fold 5 : 0.79805\n",
      "validation AUC fold 6 : 0.79275\n",
      "validation AUC fold 7 : 0.78331\n",
      "validation AUC fold 8 : 0.79068\n",
      "validation AUC fold 9 : 0.78166\n",
      "validation AUC fold 10 : 0.78099\n",
      "validation AUC fold 11 : 0.7857\n",
      "validation AUC fold 12 : 0.78747\n",
      "validation AUC fold 13 : 0.78753\n",
      "validation AUC fold 14 : 0.79007\n",
      "validation AUC fold 15 : 0.78197\n",
      "validation AUC fold 16 : 0.78627\n",
      "validation AUC fold 17 : 0.78638\n",
      "validation AUC fold 18 : 0.79578\n",
      "validation AUC fold 19 : 0.79776\n",
      "validation AUC fold 20 : 0.78076\n",
      "validation AUC fold 21 : 0.78291\n",
      "validation AUC fold 22 : 0.78526\n",
      "validation AUC fold 23 : 0.78769\n",
      "validation AUC fold 24 : 0.7911\n",
      "validation AUC fold 25 : 0.78344\n",
      "validation AUC fold 26 : 0.79862\n",
      "validation AUC fold 27 : 0.7887\n",
      "validation AUC fold 28 : 0.77895\n",
      "validation AUC fold 29 : 0.78704\n",
      "validation AUC fold 30 : 0.78083\n",
      "validation AUC fold 31 : 0.79191\n",
      "validation AUC fold 32 : 0.79089\n",
      "validation AUC fold 33 : 0.7964\n",
      "validation AUC fold 34 : 0.78032\n",
      "validation AUC fold 35 : 0.77326\n",
      "validation AUC fold 36 : 0.78571\n",
      "validation AUC fold 37 : 0.78377\n",
      "validation AUC fold 38 : 0.78565\n",
      "validation AUC fold 39 : 0.78424\n",
      "validation AUC fold 40 : 0.78648\n",
      "validation AUC fold 41 : 0.77879\n",
      "validation AUC fold 42 : 0.7867\n",
      "validation AUC fold 43 : 0.78284\n",
      "validation AUC fold 44 : 0.78025\n",
      "validation AUC fold 45 : 0.79433\n",
      "validation AUC fold 46 : 0.78498\n",
      "validation AUC fold 47 : 0.78236\n",
      "validation AUC fold 48 : 0.78828\n",
      "validation AUC fold 49 : 0.79491\n",
      "validation AUC fold 50 : 0.78602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oof_pred_deepfm = np.zeros((len(train), ))\n",
    "y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "\n",
    "for fold, (tr_ind, val_ind) in tqdm(enumerate(skf.split(train, train[target]))):\n",
    "\n",
    "    # Split\n",
    "    X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "    y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "    train_model_input = [X_train.loc[:, sparse_features].values[:, k] \\\n",
    "                         for k in range(X_train.loc[:, sparse_features].values.shape[1])]\n",
    "#     train_model_input += [X_train.loc[:, dense_features].values]\n",
    "    val_model_input = [X_val.loc[:, sparse_features].values[:, k] \\\n",
    "                       for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "#     val_model_input += [X_val.loc[:, dense_features].values]\n",
    "    test_model_input = [test.loc[:, sparse_features].values[:, k] \\\n",
    "                        for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "#     test_model_input += [test.loc[:, dense_features].values]\n",
    "\n",
    "\n",
    "    # Define model\n",
    "    model = create_model(data, sparse_features, ['id'], (300, 300), 256)\n",
    "    opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "\n",
    "    # Define callbacks\n",
    "    es = callbacks.EarlyStopping(\n",
    "        monitor='val_auc', \n",
    "        min_delta=0.001, \n",
    "        patience=PATIENCE, \n",
    "        verbose=Verbose, \n",
    "        mode='max', \n",
    "        baseline=None, \n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    #sb = callbacks.ModelCheckpoint(\n",
    "     #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "    #)\n",
    "#     clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "#                        step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "#                        gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_auc', \n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=3, \n",
    "        min_lr=1e-6,\n",
    "        verbose=Verbose,\n",
    "    )\n",
    "\n",
    "#     cb = TQDMNotebookCallback()\n",
    "#     setattr(cb,'on_train_batch_begin',lambda x,y:None)\n",
    "#     setattr(cb,'on_train_batch_end',lambda x,y:None)\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_model_input, utils.to_categorical(y_train),\n",
    "        validation_data=(val_model_input, utils.to_categorical(y_val)),\n",
    "        batch_size=1024, \n",
    "        epochs=Epochs, \n",
    "        verbose=Verbose,\n",
    "        callbacks=[reduce_lr, es]\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    val_pred = model.predict(val_model_input, batch_size=512)[:, 1]\n",
    "    print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "    oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "    y_pred_deepfm += model.predict(test_model_input, batch_size=512)[:, 1].ravel() / (N_Splits)\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF AUC : 0.78612\n"
     ]
    }
   ],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved!\n"
     ]
    }
   ],
   "source": [
    "test_idx = test.id.values\n",
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test_idx,\n",
    "    'target': y_pred_deepfm\n",
    "})\n",
    "submission.to_csv(\"submission_kerasemb_cv_50.csv\", index=False)\n",
    "print(\"Submission file saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('oof_pred_kerasemb_cv_50.npy',oof_pred_deepfm)\n",
    "np.save('y_pred_kerasemb_cv_50.npy',    y_pred_deepfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_pred_deepfm = np.load('oof_pred_kerasemb_cv_50.npy')\n",
    "y_pred_deepfm = np.load('y_pred_kerasemb_cv_50.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "percs, bins = np.histogram(oof_pred_deepfm, bins=500)\n",
    "precs = [precision_score(train.target.values, (oof_pred_deepfm > thresh).astype(int)) for thresh in bins]\n",
    "pops = (percs[::-1].cumsum() / 600000)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines",
         "name": "lines",
         "type": "scatter",
         "x": [
          1,
          0.999555,
          0.9981533333333333,
          0.9958366666666667,
          0.99279,
          0.989215,
          0.9852483333333333,
          0.9808066666666667,
          0.9761833333333333,
          0.971295,
          0.9661883333333333,
          0.9608266666666667,
          0.9553133333333333,
          0.9497833333333333,
          0.9440966666666667,
          0.93854,
          0.9328533333333333,
          0.927265,
          0.921455,
          0.9155166666666666,
          0.9096366666666666,
          0.9037883333333333,
          0.897825,
          0.8916683333333333,
          0.8855516666666666,
          0.8795666666666667,
          0.8735433333333333,
          0.867505,
          0.861685,
          0.85602,
          0.850185,
          0.84433,
          0.8385416666666666,
          0.8329116666666667,
          0.827275,
          0.8214766666666666,
          0.8157566666666667,
          0.8101966666666667,
          0.8045716666666667,
          0.79888,
          0.7932866666666667,
          0.7877833333333333,
          0.7823633333333333,
          0.77668,
          0.7712266666666666,
          0.7658866666666667,
          0.7605216666666667,
          0.7550766666666666,
          0.7496533333333333,
          0.744295,
          0.7389766666666666,
          0.7337033333333334,
          0.7283316666666667,
          0.7231933333333334,
          0.71795,
          0.712805,
          0.7077133333333333,
          0.7026166666666667,
          0.6976816666666666,
          0.6925216666666667,
          0.6874833333333333,
          0.6825,
          0.6775533333333333,
          0.6727916666666667,
          0.6678,
          0.6628583333333333,
          0.6579566666666666,
          0.6529183333333334,
          0.6479466666666667,
          0.6433783333333334,
          0.6385683333333333,
          0.63395,
          0.629495,
          0.62482,
          0.62007,
          0.6155816666666667,
          0.610895,
          0.6063066666666667,
          0.6017466666666667,
          0.59707,
          0.5924866666666667,
          0.5880133333333334,
          0.583665,
          0.5792533333333333,
          0.5748066666666667,
          0.570425,
          0.566035,
          0.5617966666666667,
          0.55747,
          0.5532316666666667,
          0.5488,
          0.544665,
          0.5403483333333333,
          0.5362316666666667,
          0.531975,
          0.5276983333333334,
          0.5234766666666667,
          0.51933,
          0.5151483333333333,
          0.5110183333333334,
          0.5069633333333333,
          0.5028616666666667,
          0.49884,
          0.49472333333333335,
          0.490765,
          0.48679333333333336,
          0.4827533333333333,
          0.47873,
          0.4747766666666667,
          0.4708216666666667,
          0.46698833333333334,
          0.46309333333333336,
          0.45898833333333333,
          0.45536166666666666,
          0.45177666666666666,
          0.44789833333333334,
          0.44404333333333335,
          0.44015666666666664,
          0.436505,
          0.43280833333333335,
          0.429095,
          0.42544,
          0.4218983333333333,
          0.4181666666666667,
          0.4145216666666667,
          0.41083,
          0.407415,
          0.40369666666666665,
          0.4000783333333333,
          0.39646333333333333,
          0.393,
          0.3895033333333333,
          0.38608333333333333,
          0.382565,
          0.37914333333333333,
          0.3757333333333333,
          0.37235,
          0.36884666666666666,
          0.36551666666666666,
          0.3620933333333333,
          0.3588233333333333,
          0.3554866666666667,
          0.35212333333333334,
          0.3488,
          0.345555,
          0.3422983333333333,
          0.339,
          0.33586666666666665,
          0.33250833333333335,
          0.329235,
          0.32602166666666665,
          0.3228933333333333,
          0.31961333333333336,
          0.3164166666666667,
          0.31326166666666666,
          0.31006,
          0.30689833333333333,
          0.303755,
          0.3006316666666667,
          0.297655,
          0.29455,
          0.2915466666666667,
          0.288595,
          0.28550166666666665,
          0.2824933333333333,
          0.27958666666666665,
          0.2766133333333333,
          0.27374,
          0.27084166666666665,
          0.26783833333333334,
          0.26499333333333336,
          0.262155,
          0.25935166666666665,
          0.2565483333333333,
          0.253865,
          0.25114,
          0.24831,
          0.24555666666666667,
          0.24288,
          0.24012,
          0.23736,
          0.23472166666666666,
          0.23217333333333334,
          0.22949166666666668,
          0.226825,
          0.22417666666666666,
          0.22167666666666666,
          0.21905166666666667,
          0.21661333333333332,
          0.21419166666666667,
          0.211795,
          0.20933666666666667,
          0.20698666666666668,
          0.20456166666666667,
          0.2022,
          0.19986,
          0.197565,
          0.19519666666666666,
          0.19287166666666666,
          0.19060166666666667,
          0.18835166666666667,
          0.18615666666666666,
          0.18396166666666666,
          0.18172166666666667,
          0.17949333333333334,
          0.177345,
          0.17523833333333333,
          0.17309833333333333,
          0.17103,
          0.168975,
          0.16682833333333333,
          0.16491833333333333,
          0.162955,
          0.16096333333333335,
          0.15905,
          0.15719333333333332,
          0.15528,
          0.15351166666666666,
          0.15168,
          0.14989333333333332,
          0.14820166666666668,
          0.14639666666666667,
          0.144675,
          0.142955,
          0.14127333333333333,
          0.13966,
          0.13800666666666667,
          0.13646166666666668,
          0.13486,
          0.13329,
          0.13163666666666668,
          0.13003166666666666,
          0.12840666666666667,
          0.12683166666666668,
          0.12524666666666667,
          0.12380833333333334,
          0.12238,
          0.12093166666666667,
          0.11948333333333333,
          0.11813833333333333,
          0.11675333333333333,
          0.11551666666666667,
          0.11420166666666667,
          0.11291666666666667,
          0.11154166666666666,
          0.11033166666666666,
          0.10903,
          0.10777166666666667,
          0.10657666666666667,
          0.10524,
          0.10399666666666667,
          0.10274333333333334,
          0.10162,
          0.10041,
          0.09923166666666666,
          0.09804666666666667,
          0.096995,
          0.095865,
          0.09475666666666667,
          0.09366666666666666,
          0.09264,
          0.09149666666666667,
          0.09048666666666667,
          0.08944,
          0.08840333333333333,
          0.08733666666666667,
          0.08631166666666666,
          0.08526,
          0.08423833333333333,
          0.08326166666666666,
          0.08226333333333333,
          0.08125666666666667,
          0.08028666666666667,
          0.07934,
          0.07836333333333333,
          0.077465,
          0.07647833333333333,
          0.07562333333333333,
          0.07472666666666666,
          0.07384666666666667,
          0.07292333333333334,
          0.07201166666666667,
          0.07115333333333333,
          0.07027666666666667,
          0.06945,
          0.06862666666666667,
          0.06781833333333333,
          0.06695833333333333,
          0.06608666666666667,
          0.06538166666666667,
          0.06455,
          0.063785,
          0.062955,
          0.062235,
          0.061443333333333336,
          0.06068,
          0.05989,
          0.059136666666666664,
          0.058348333333333335,
          0.057585,
          0.056881666666666664,
          0.056125,
          0.05534666666666667,
          0.05461166666666667,
          0.053875,
          0.053085,
          0.05241833333333333,
          0.05175333333333333,
          0.051041666666666666,
          0.050385,
          0.049681666666666666,
          0.048995,
          0.048305,
          0.04763333333333333,
          0.04695,
          0.04625833333333333,
          0.04558,
          0.04493166666666667,
          0.04427333333333333,
          0.04367666666666667,
          0.04307166666666667,
          0.042445,
          0.041805,
          0.041225,
          0.04064333333333333,
          0.03998,
          0.039338333333333336,
          0.03874,
          0.03815166666666667,
          0.037588333333333335,
          0.037,
          0.03638333333333333,
          0.035855,
          0.035271666666666666,
          0.03472833333333333,
          0.03416166666666667,
          0.03360333333333333,
          0.03298333333333334,
          0.03247333333333333,
          0.03194333333333333,
          0.03136666666666667,
          0.03083,
          0.030288333333333334,
          0.02969,
          0.029138333333333332,
          0.028643333333333333,
          0.028153333333333332,
          0.02763,
          0.027061666666666668,
          0.026525,
          0.026003333333333333,
          0.025538333333333333,
          0.025045,
          0.024498333333333334,
          0.023981666666666665,
          0.023476666666666667,
          0.02296,
          0.0225,
          0.021985,
          0.021481666666666666,
          0.020995,
          0.020563333333333333,
          0.02014,
          0.019728333333333334,
          0.019298333333333334,
          0.018901666666666667,
          0.01846,
          0.01804,
          0.017605,
          0.017213333333333334,
          0.016721666666666666,
          0.016286666666666668,
          0.01588,
          0.01548,
          0.015101666666666666,
          0.014685,
          0.014251666666666666,
          0.013836666666666667,
          0.013446666666666668,
          0.013106666666666668,
          0.012726666666666667,
          0.012328333333333334,
          0.011963333333333333,
          0.011655,
          0.011331666666666667,
          0.011008333333333334,
          0.010711666666666666,
          0.01039,
          0.010066666666666666,
          0.009763333333333334,
          0.009448333333333333,
          0.009136666666666666,
          0.008851666666666667,
          0.008573333333333334,
          0.008271666666666667,
          0.007973333333333334,
          0.007751666666666666,
          0.007486666666666667,
          0.007241666666666667,
          0.00699,
          0.006741666666666667,
          0.006476666666666667,
          0.00625,
          0.005995,
          0.0057733333333333334,
          0.00558,
          0.005343333333333334,
          0.005096666666666667,
          0.00489,
          0.00469,
          0.004488333333333334,
          0.004275,
          0.004091666666666667,
          0.003916666666666666,
          0.0037383333333333335,
          0.003575,
          0.0034083333333333335,
          0.003276666666666667,
          0.0031383333333333333,
          0.003023333333333333,
          0.0028783333333333334,
          0.002746666666666667,
          0.00263,
          0.0025,
          0.002375,
          0.00226,
          0.0021483333333333333,
          0.0020583333333333335,
          0.0019533333333333334,
          0.0018416666666666666,
          0.001745,
          0.0016766666666666666,
          0.0015983333333333333,
          0.0015266666666666666,
          0.0014616666666666667,
          0.001375,
          0.001305,
          0.001235,
          0.00118,
          0.001125,
          0.001055,
          0.000985,
          0.0009366666666666667,
          0.0008916666666666667,
          0.0008416666666666667,
          0.000785,
          0.0007433333333333334,
          0.0006933333333333333,
          0.0006466666666666667,
          0.000615,
          0.0005816666666666666,
          0.0005383333333333334,
          0.00051,
          0.0004966666666666666,
          0.0004516666666666667,
          0.0004166666666666667,
          0.00038666666666666667,
          0.000365,
          0.00034333333333333335,
          0.00032333333333333335,
          0.00029833333333333334,
          0.00027666666666666665,
          0.00025666666666666665,
          0.00024166666666666667,
          0.00022833333333333334,
          0.00020666666666666666,
          0.00019666666666666666,
          0.00018333333333333334,
          0.00016666666666666666,
          0.00014833333333333332,
          0.00014333333333333334,
          0.00013166666666666668,
          0.00012833333333333333,
          0.00012,
          0.00011666666666666667,
          0.0001,
          8.833333333333333e-05,
          7.666666666666667e-05,
          7.333333333333333e-05,
          6.5e-05,
          5.5e-05,
          4.8333333333333334e-05,
          4.8333333333333334e-05,
          4.6666666666666665e-05,
          4.1666666666666665e-05,
          3.8333333333333334e-05,
          3.3333333333333335e-05,
          3e-05,
          2.1666666666666667e-05,
          2.1666666666666667e-05,
          1.6666666666666667e-05,
          1.6666666666666667e-05,
          1.1666666666666666e-05,
          1.1666666666666666e-05,
          1e-05,
          6.666666666666667e-06,
          3.3333333333333333e-06,
          3.3333333333333333e-06,
          3.3333333333333333e-06,
          3.3333333333333333e-06
         ],
         "y": [
          0.18720531200885335,
          0.1872883433127742,
          0.1875513448167616,
          0.18797928709862058,
          0.18855447778482862,
          0.18922580025575836,
          0.1899656431796383,
          0.19080552742300555,
          0.19168530501442693,
          0.19261398442285815,
          0.1935940715491976,
          0.1946119313924121,
          0.19567401969336412,
          0.19672732377559796,
          0.1978487372408899,
          0.19893842208820792,
          0.2000796838374021,
          0.20119023867682556,
          0.20233398990364876,
          0.20351349875298103,
          0.20470444243305935,
          0.20584096940102126,
          0.20706150975969703,
          0.20831362932031902,
          0.20957745736650035,
          0.21080077310796982,
          0.21206160350755351,
          0.2133378674090255,
          0.21456216598873137,
          0.21574067584090714,
          0.216997869091237,
          0.21823023383432227,
          0.21948422360248449,
          0.22068966897250009,
          0.2219536026915677,
          0.22320171398660135,
          0.22450526505044396,
          0.2257702862268009,
          0.2270338461665938,
          0.228315474998331,
          0.22959123645928753,
          0.23082277275900734,
          0.2320533085650742,
          0.2333650924447649,
          0.23464351163514402,
          0.23589652080812654,
          0.23718149603452016,
          0.23849233852633067,
          0.23975971115537847,
          0.24109839960409962,
          0.24236669628720797,
          0.243645251713908,
          0.2449822539639679,
          0.24625042634980043,
          0.247496343756529,
          0.24876602530378808,
          0.25003768003994087,
          0.251303460872453,
          0.25259132030128356,
          0.25395835990691024,
          0.2552692186477248,
          0.25652747252747254,
          0.25786653941141163,
          0.259131727255837,
          0.2605121293800539,
          0.2619061388179978,
          0.2632518858891416,
          0.2645889863714451,
          0.2659426701786155,
          0.26720410748470963,
          0.2685878044897309,
          0.26981886058311644,
          0.27107972793006035,
          0.27244646458179955,
          0.2739045591626752,
          0.27513273353928125,
          0.2765096020319913,
          0.27789017658830517,
          0.2791013937205025,
          0.28053662049675915,
          0.28191914304681964,
          0.28322487018434955,
          0.28451823106290997,
          0.28579895958014917,
          0.287215411558669,
          0.28851879446611445,
          0.28993790136652325,
          0.2912797631408754,
          0.29258973577053476,
          0.29384314587921273,
          0.2952927599611273,
          0.29671449423039853,
          0.298097831954079,
          0.2994166078715978,
          0.3008600028196814,
          0.3022654989119415,
          0.30375438574148483,
          0.3051046540735178,
          0.3064004218849587,
          0.30769607091722084,
          0.3090295813635437,
          0.3104134006370208,
          0.31172987998823937,
          0.31313124507300377,
          0.3144444557646396,
          0.31589380846081155,
          0.3172531175341444,
          0.31866953536788306,
          0.320048724663526,
          0.32151239145748745,
          0.322905996937825,
          0.32432267073592075,
          0.32589789863939894,
          0.3272307360083743,
          0.3285694258962762,
          0.33008606863908846,
          0.3315554788196347,
          0.3329458450400236,
          0.33432988549195697,
          0.33573752815911584,
          0.33728350753718095,
          0.33868857339852076,
          0.33999107209872836,
          0.34144280589876447,
          0.3428128002959234,
          0.3442867690610066,
          0.34572037521118926,
          0.3471955015729632,
          0.3486900481988944,
          0.3501500769301911,
          0.351696352841391,
          0.3531291987231603,
          0.3546125620548241,
          0.35610070619807527,
          0.3575824446339555,
          0.35910220014194466,
          0.3606552974352088,
          0.36209716774811573,
          0.3635401942455884,
          0.365149869278639,
          0.366726429905153,
          0.3681713331958067,
          0.36970947679316907,
          0.37115825688073395,
          0.3725166760718265,
          0.37402558197284047,
          0.3755014749262537,
          0.37717844382691545,
          0.37878749906017395,
          0.38041722984089377,
          0.3819378057695552,
          0.38347028946607753,
          0.38500166868299196,
          0.3864787990518831,
          0.3879451151061147,
          0.3895428841729558,
          0.3910741342138276,
          0.3927397628571272,
          0.39439735224166894,
          0.3958889766116253,
          0.39758954337124425,
          0.39907276136467573,
          0.40055556518073193,
          0.4020758781326437,
          0.40363784396091945,
          0.4052112642472221,
          0.40678926058035286,
          0.4083315067825918,
          0.409950463062675,
          0.4117098000659602,
          0.4130669953961106,
          0.4147737025805344,
          0.41630733045864365,
          0.41805637664117873,
          0.4194946132787111,
          0.42119933105041013,
          0.4226840105781751,
          0.42459988868828646,
          0.4263353645147123,
          0.427938808373591,
          0.4296006066734075,
          0.4313619677206337,
          0.4328734278986964,
          0.4344892697628817,
          0.43621000036739044,
          0.4376087312090167,
          0.43953656226034915,
          0.44102989401282805,
          0.4425473962821618,
          0.44413492588413805,
          0.44572975440087503,
          0.4475565675705801,
          0.44934456325689254,
          0.45097240440942826,
          0.4523986152324431,
          0.4541095433470096,
          0.4558921536371996,
          0.4575641660547482,
          0.4593296060420141,
          0.4611099937915898,
          0.46283105184451073,
          0.46430425985281215,
          0.46577638457287296,
          0.46762906642942964,
          0.46944176199673154,
          0.47108742845865403,
          0.47263251001017664,
          0.474267997958771,
          0.47600810773938296,
          0.4776347585934803,
          0.4795448415037414,
          0.48110681044153164,
          0.4828531394147669,
          0.48469630764770444,
          0.4863564916692864,
          0.48795538402816063,
          0.4899214322514168,
          0.491547873668668,
          0.49329729254571025,
          0.49487413271659847,
          0.49629446362501545,
          0.4977686195040871,
          0.49914175450722886,
          0.500600422044233,
          0.5020645557076118,
          0.5034369182299872,
          0.504673687261485,
          0.5062593890836254,
          0.50782292748035,
          0.5092655112911696,
          0.5108505735484035,
          0.5122341993616939,
          0.5133949431493692,
          0.5151184641059672,
          0.5166205354766594,
          0.5183280608467389,
          0.5198561856512502,
          0.5213688171005665,
          0.522862323894546,
          0.524032560698616,
          0.5260092502712271,
          0.5273120761794835,
          0.5288743596853519,
          0.5301402214022141,
          0.5313858797161001,
          0.532802610311334,
          0.5340273319269926,
          0.5355767595069824,
          0.5369843305288837,
          0.5388002027112632,
          0.5402577005673259,
          0.5414463225513415,
          0.5424949156990094,
          0.5441523088669787,
          0.545675943499219,
          0.547341402053444,
          0.5484646974929979,
          0.5501312609746345,
          0.5517817567805255,
          0.5531138790035587,
          0.5542422279792746,
          0.555575795110933,
          0.5568223679363442,
          0.5580463625521765,
          0.5597262546661137,
          0.5614289530933934,
          0.5628632668430301,
          0.5642153413089374,
          0.5652612626872168,
          0.5668675060552075,
          0.567972770371571,
          0.5694096894613775,
          0.5710371169974259,
          0.5721367952272918,
          0.573461227614956,
          0.5747972202500053,
          0.5759583324253057,
          0.5776876625380174,
          0.5792666607190651,
          0.5803015256838494,
          0.581638250217123,
          0.5831693938482191,
          0.5846997095474562,
          0.5861594649717782,
          0.5873770098392128,
          0.5888381581503789,
          0.5905237030301541,
          0.5919103920348475,
          0.5930848380913951,
          0.5944581814473986,
          0.5951975213013168,
          0.5964568472211335,
          0.5974902708283695,
          0.5989127232800407,
          0.6005262301307438,
          0.601928147659855,
          0.6032170089608727,
          0.604137309058114,
          0.6059013396555172,
          0.607305143121762,
          0.6087198570130974,
          0.6102449888641426,
          0.6112382558419658,
          0.6125980407116917,
          0.6137973704563032,
          0.6148943518256884,
          0.6161648278274141,
          0.617576967667139,
          0.6191673469387755,
          0.6206873738877311,
          0.6225636552718977,
          0.6240432697214001,
          0.6250215643653176,
          0.6262771168649405,
          0.6283280085197018,
          0.6299045217078004,
          0.6319657744624836,
          0.633183723431878,
          0.6355594037042614,
          0.6368388918568266,
          0.6380064234028557,
          0.6395335139592414,
          0.6412311127058167,
          0.6422478269658379,
          0.6440990732387435,
          0.6455727863931966,
          0.646697453713511,
          0.6478661159869213,
          0.649556594294701,
          0.6505564669888707,
          0.6528828828828829,
          0.6548786074209803,
          0.6563473248733324,
          0.6576099796815197,
          0.6588280462638576,
          0.6597550861101624,
          0.662037496280131,
          0.6637190500252653,
          0.6658796961609525,
          0.6674840864030053,
          0.6689160467587673,
          0.6705049194507514,
          0.6718758597919992,
          0.6732345346356798,
          0.6750557684607905,
          0.6770627254742232,
          0.6781908595784987,
          0.6793943780914465,
          0.6808523742070579,
          0.6824379516179705,
          0.684143058582233,
          0.6851791424655747,
          0.6867638251147934,
          0.6880059868018232,
          0.6898325109458614,
          0.6920346443277012,
          0.693815331010453,
          0.6958518518518518,
          0.6968387536957016,
          0.700054309876639,
          0.7015956180042867,
          0.7023018317393419,
          0.7037404832836809,
          0.7042324913407113,
          0.7043786164608342,
          0.7064632748434883,
          0.7071144817623691,
          0.7091648189209164,
          0.7101202309949824,
          0.7112703330751355,
          0.7121499053124688,
          0.7144903806794924,
          0.716414777497901,
          0.718776916451335,
          0.7200088290475665,
          0.7229599364430825,
          0.7249444509414104,
          0.7254878342568056,
          0.7288051561725335,
          0.7302899287894201,
          0.7328444211629125,
          0.7346221441124781,
          0.73627751462803,
          0.7397397397397397,
          0.7405500808942491,
          0.7427706283118849,
          0.7423370157149526,
          0.745107475136349,
          0.7468543046357616,
          0.7485489928303175,
          0.7511024872111484,
          0.7539219263042685,
          0.7548484277913764,
          0.7525272161741835,
          0.7537779568809188,
          0.754180602006689,
          0.7555364437755322,
          0.7557880676758683,
          0.7592635212888378,
          0.7594182164997616,
          0.7594561186650185,
          0.762995367987648,
          0.7621333333333333,
          0.7631359466221852,
          0.7678983833718245,
          0.7718040621266428,
          0.7732376793512165,
          0.7776324395029431,
          0.7791411042944786,
          0.7810945273631841,
          0.7820274786483475,
          0.7824561403508772,
          0.7845213849287169,
          0.7872340425531915,
          0.7904592064199732,
          0.7976689976689977,
          0.8014669926650366,
          0.8031536113936928,
          0.8029739776951673,
          0.8004410143329658,
          0.802547770700637,
          0.8027912621359223,
          0.8022813688212928,
          0.8026666666666666,
          0.8028070175438596,
          0.8067846607669616,
          0.8068269976726145,
          0.8080971659919028,
          0.8088737201365188,
          0.8108597285067873,
          0.8108882521489972,
          0.8111332007952287,
          0.8112617309697602,
          0.8133187772925764,
          0.814139110604333,
          0.8157575757575758,
          0.8148148148148148,
          0.8110661268556005,
          0.8135593220338984,
          0.8207407407407408,
          0.8199052132701422,
          0.8291032148900169,
          0.8291814946619217,
          0.8336448598130841,
          0.8376237623762376,
          0.8428874734607219,
          0.8497757847533632,
          0.84375,
          0.8427835051546392,
          0.8401084010840109,
          0.8424068767908309,
          0.848297213622291,
          0.8562091503267973,
          0.8523489932885906,
          0.8671586715867159,
          0.876,
          0.875,
          0.8904109589041096,
          0.8883495145631068,
          0.8917525773195877,
          0.88268156424581,
          0.8795180722891566,
          0.8831168831168831,
          0.8827586206896552,
          0.8905109489051095,
          0.8951612903225806,
          0.8898305084745762,
          0.9,
          0.89,
          0.9101123595505618,
          0.9069767441860465,
          0.9240506329113924,
          0.922077922077922,
          0.9305555555555556,
          0.9428571428571428,
          0.95,
          0.9433962264150944,
          0.9347826086956522,
          0.9318181818181818,
          0.9230769230769231,
          0.9393939393939394,
          0.9310344827586207,
          0.9310344827586207,
          0.9285714285714286,
          0.92,
          0.9130434782608695,
          0.9,
          0.9444444444444444,
          0.9230769230769231,
          0.9230769230769231,
          0.9,
          0.9,
          0.8571428571428571,
          0.8571428571428571,
          0.8333333333333334,
          0.75,
          0.5,
          0.5,
          0.5,
          0.5
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"4586e0b8-04f3-45a8-89dd-99fa98589ce4\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"4586e0b8-04f3-45a8-89dd-99fa98589ce4\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '4586e0b8-04f3-45a8-89dd-99fa98589ce4',\n",
       "                        [{\"mode\": \"lines\", \"name\": \"lines\", \"type\": \"scatter\", \"x\": [1.0, 0.999555, 0.9981533333333333, 0.9958366666666667, 0.99279, 0.989215, 0.9852483333333333, 0.9808066666666667, 0.9761833333333333, 0.971295, 0.9661883333333333, 0.9608266666666667, 0.9553133333333333, 0.9497833333333333, 0.9440966666666667, 0.93854, 0.9328533333333333, 0.927265, 0.921455, 0.9155166666666666, 0.9096366666666666, 0.9037883333333333, 0.897825, 0.8916683333333333, 0.8855516666666666, 0.8795666666666667, 0.8735433333333333, 0.867505, 0.861685, 0.85602, 0.850185, 0.84433, 0.8385416666666666, 0.8329116666666667, 0.827275, 0.8214766666666666, 0.8157566666666667, 0.8101966666666667, 0.8045716666666667, 0.79888, 0.7932866666666667, 0.7877833333333333, 0.7823633333333333, 0.77668, 0.7712266666666666, 0.7658866666666667, 0.7605216666666667, 0.7550766666666666, 0.7496533333333333, 0.744295, 0.7389766666666666, 0.7337033333333334, 0.7283316666666667, 0.7231933333333334, 0.71795, 0.712805, 0.7077133333333333, 0.7026166666666667, 0.6976816666666666, 0.6925216666666667, 0.6874833333333333, 0.6825, 0.6775533333333333, 0.6727916666666667, 0.6678, 0.6628583333333333, 0.6579566666666666, 0.6529183333333334, 0.6479466666666667, 0.6433783333333334, 0.6385683333333333, 0.63395, 0.629495, 0.62482, 0.62007, 0.6155816666666667, 0.610895, 0.6063066666666667, 0.6017466666666667, 0.59707, 0.5924866666666667, 0.5880133333333334, 0.583665, 0.5792533333333333, 0.5748066666666667, 0.570425, 0.566035, 0.5617966666666667, 0.55747, 0.5532316666666667, 0.5488, 0.544665, 0.5403483333333333, 0.5362316666666667, 0.531975, 0.5276983333333334, 0.5234766666666667, 0.51933, 0.5151483333333333, 0.5110183333333334, 0.5069633333333333, 0.5028616666666667, 0.49884, 0.49472333333333335, 0.490765, 0.48679333333333336, 0.4827533333333333, 0.47873, 0.4747766666666667, 0.4708216666666667, 0.46698833333333334, 0.46309333333333336, 0.45898833333333333, 0.45536166666666666, 0.45177666666666666, 0.44789833333333334, 0.44404333333333335, 0.44015666666666664, 0.436505, 0.43280833333333335, 0.429095, 0.42544, 0.4218983333333333, 0.4181666666666667, 0.4145216666666667, 0.41083, 0.407415, 0.40369666666666665, 0.4000783333333333, 0.39646333333333333, 0.393, 0.3895033333333333, 0.38608333333333333, 0.382565, 0.37914333333333333, 0.3757333333333333, 0.37235, 0.36884666666666666, 0.36551666666666666, 0.3620933333333333, 0.3588233333333333, 0.3554866666666667, 0.35212333333333334, 0.3488, 0.345555, 0.3422983333333333, 0.339, 0.33586666666666665, 0.33250833333333335, 0.329235, 0.32602166666666665, 0.3228933333333333, 0.31961333333333336, 0.3164166666666667, 0.31326166666666666, 0.31006, 0.30689833333333333, 0.303755, 0.3006316666666667, 0.297655, 0.29455, 0.2915466666666667, 0.288595, 0.28550166666666665, 0.2824933333333333, 0.27958666666666665, 0.2766133333333333, 0.27374, 0.27084166666666665, 0.26783833333333334, 0.26499333333333336, 0.262155, 0.25935166666666665, 0.2565483333333333, 0.253865, 0.25114, 0.24831, 0.24555666666666667, 0.24288, 0.24012, 0.23736, 0.23472166666666666, 0.23217333333333334, 0.22949166666666668, 0.226825, 0.22417666666666666, 0.22167666666666666, 0.21905166666666667, 0.21661333333333332, 0.21419166666666667, 0.211795, 0.20933666666666667, 0.20698666666666668, 0.20456166666666667, 0.2022, 0.19986, 0.197565, 0.19519666666666666, 0.19287166666666666, 0.19060166666666667, 0.18835166666666667, 0.18615666666666666, 0.18396166666666666, 0.18172166666666667, 0.17949333333333334, 0.177345, 0.17523833333333333, 0.17309833333333333, 0.17103, 0.168975, 0.16682833333333333, 0.16491833333333333, 0.162955, 0.16096333333333335, 0.15905, 0.15719333333333332, 0.15528, 0.15351166666666666, 0.15168, 0.14989333333333332, 0.14820166666666668, 0.14639666666666667, 0.144675, 0.142955, 0.14127333333333333, 0.13966, 0.13800666666666667, 0.13646166666666668, 0.13486, 0.13329, 0.13163666666666668, 0.13003166666666666, 0.12840666666666667, 0.12683166666666668, 0.12524666666666667, 0.12380833333333334, 0.12238, 0.12093166666666667, 0.11948333333333333, 0.11813833333333333, 0.11675333333333333, 0.11551666666666667, 0.11420166666666667, 0.11291666666666667, 0.11154166666666666, 0.11033166666666666, 0.10903, 0.10777166666666667, 0.10657666666666667, 0.10524, 0.10399666666666667, 0.10274333333333334, 0.10162, 0.10041, 0.09923166666666666, 0.09804666666666667, 0.096995, 0.095865, 0.09475666666666667, 0.09366666666666666, 0.09264, 0.09149666666666667, 0.09048666666666667, 0.08944, 0.08840333333333333, 0.08733666666666667, 0.08631166666666666, 0.08526, 0.08423833333333333, 0.08326166666666666, 0.08226333333333333, 0.08125666666666667, 0.08028666666666667, 0.07934, 0.07836333333333333, 0.077465, 0.07647833333333333, 0.07562333333333333, 0.07472666666666666, 0.07384666666666667, 0.07292333333333334, 0.07201166666666667, 0.07115333333333333, 0.07027666666666667, 0.06945, 0.06862666666666667, 0.06781833333333333, 0.06695833333333333, 0.06608666666666667, 0.06538166666666667, 0.06455, 0.063785, 0.062955, 0.062235, 0.061443333333333336, 0.06068, 0.05989, 0.059136666666666664, 0.058348333333333335, 0.057585, 0.056881666666666664, 0.056125, 0.05534666666666667, 0.05461166666666667, 0.053875, 0.053085, 0.05241833333333333, 0.05175333333333333, 0.051041666666666666, 0.050385, 0.049681666666666666, 0.048995, 0.048305, 0.04763333333333333, 0.04695, 0.04625833333333333, 0.04558, 0.04493166666666667, 0.04427333333333333, 0.04367666666666667, 0.04307166666666667, 0.042445, 0.041805, 0.041225, 0.04064333333333333, 0.03998, 0.039338333333333336, 0.03874, 0.03815166666666667, 0.037588333333333335, 0.037, 0.03638333333333333, 0.035855, 0.035271666666666666, 0.03472833333333333, 0.03416166666666667, 0.03360333333333333, 0.03298333333333334, 0.03247333333333333, 0.03194333333333333, 0.03136666666666667, 0.03083, 0.030288333333333334, 0.02969, 0.029138333333333332, 0.028643333333333333, 0.028153333333333332, 0.02763, 0.027061666666666668, 0.026525, 0.026003333333333333, 0.025538333333333333, 0.025045, 0.024498333333333334, 0.023981666666666665, 0.023476666666666667, 0.02296, 0.0225, 0.021985, 0.021481666666666666, 0.020995, 0.020563333333333333, 0.02014, 0.019728333333333334, 0.019298333333333334, 0.018901666666666667, 0.01846, 0.01804, 0.017605, 0.017213333333333334, 0.016721666666666666, 0.016286666666666668, 0.01588, 0.01548, 0.015101666666666666, 0.014685, 0.014251666666666666, 0.013836666666666667, 0.013446666666666668, 0.013106666666666668, 0.012726666666666667, 0.012328333333333334, 0.011963333333333333, 0.011655, 0.011331666666666667, 0.011008333333333334, 0.010711666666666666, 0.01039, 0.010066666666666666, 0.009763333333333334, 0.009448333333333333, 0.009136666666666666, 0.008851666666666667, 0.008573333333333334, 0.008271666666666667, 0.007973333333333334, 0.007751666666666666, 0.007486666666666667, 0.007241666666666667, 0.00699, 0.006741666666666667, 0.006476666666666667, 0.00625, 0.005995, 0.0057733333333333334, 0.00558, 0.005343333333333334, 0.005096666666666667, 0.00489, 0.00469, 0.004488333333333334, 0.004275, 0.004091666666666667, 0.003916666666666666, 0.0037383333333333335, 0.003575, 0.0034083333333333335, 0.003276666666666667, 0.0031383333333333333, 0.003023333333333333, 0.0028783333333333334, 0.002746666666666667, 0.00263, 0.0025, 0.002375, 0.00226, 0.0021483333333333333, 0.0020583333333333335, 0.0019533333333333334, 0.0018416666666666666, 0.001745, 0.0016766666666666666, 0.0015983333333333333, 0.0015266666666666666, 0.0014616666666666667, 0.001375, 0.001305, 0.001235, 0.00118, 0.001125, 0.001055, 0.000985, 0.0009366666666666667, 0.0008916666666666667, 0.0008416666666666667, 0.000785, 0.0007433333333333334, 0.0006933333333333333, 0.0006466666666666667, 0.000615, 0.0005816666666666666, 0.0005383333333333334, 0.00051, 0.0004966666666666666, 0.0004516666666666667, 0.0004166666666666667, 0.00038666666666666667, 0.000365, 0.00034333333333333335, 0.00032333333333333335, 0.00029833333333333334, 0.00027666666666666665, 0.00025666666666666665, 0.00024166666666666667, 0.00022833333333333334, 0.00020666666666666666, 0.00019666666666666666, 0.00018333333333333334, 0.00016666666666666666, 0.00014833333333333332, 0.00014333333333333334, 0.00013166666666666668, 0.00012833333333333333, 0.00012, 0.00011666666666666667, 0.0001, 8.833333333333333e-05, 7.666666666666667e-05, 7.333333333333333e-05, 6.5e-05, 5.5e-05, 4.8333333333333334e-05, 4.8333333333333334e-05, 4.6666666666666665e-05, 4.1666666666666665e-05, 3.8333333333333334e-05, 3.3333333333333335e-05, 3e-05, 2.1666666666666667e-05, 2.1666666666666667e-05, 1.6666666666666667e-05, 1.6666666666666667e-05, 1.1666666666666666e-05, 1.1666666666666666e-05, 1e-05, 6.666666666666667e-06, 3.3333333333333333e-06, 3.3333333333333333e-06, 3.3333333333333333e-06, 3.3333333333333333e-06], \"y\": [0.18720531200885335, 0.1872883433127742, 0.1875513448167616, 0.18797928709862058, 0.18855447778482862, 0.18922580025575836, 0.1899656431796383, 0.19080552742300555, 0.19168530501442693, 0.19261398442285815, 0.1935940715491976, 0.1946119313924121, 0.19567401969336412, 0.19672732377559796, 0.1978487372408899, 0.19893842208820792, 0.2000796838374021, 0.20119023867682556, 0.20233398990364876, 0.20351349875298103, 0.20470444243305935, 0.20584096940102126, 0.20706150975969703, 0.20831362932031902, 0.20957745736650035, 0.21080077310796982, 0.21206160350755351, 0.2133378674090255, 0.21456216598873137, 0.21574067584090714, 0.216997869091237, 0.21823023383432227, 0.21948422360248449, 0.22068966897250009, 0.2219536026915677, 0.22320171398660135, 0.22450526505044396, 0.2257702862268009, 0.2270338461665938, 0.228315474998331, 0.22959123645928753, 0.23082277275900734, 0.2320533085650742, 0.2333650924447649, 0.23464351163514402, 0.23589652080812654, 0.23718149603452016, 0.23849233852633067, 0.23975971115537847, 0.24109839960409962, 0.24236669628720797, 0.243645251713908, 0.2449822539639679, 0.24625042634980043, 0.247496343756529, 0.24876602530378808, 0.25003768003994087, 0.251303460872453, 0.25259132030128356, 0.25395835990691024, 0.2552692186477248, 0.25652747252747254, 0.25786653941141163, 0.259131727255837, 0.2605121293800539, 0.2619061388179978, 0.2632518858891416, 0.2645889863714451, 0.2659426701786155, 0.26720410748470963, 0.2685878044897309, 0.26981886058311644, 0.27107972793006035, 0.27244646458179955, 0.2739045591626752, 0.27513273353928125, 0.2765096020319913, 0.27789017658830517, 0.2791013937205025, 0.28053662049675915, 0.28191914304681964, 0.28322487018434955, 0.28451823106290997, 0.28579895958014917, 0.287215411558669, 0.28851879446611445, 0.28993790136652325, 0.2912797631408754, 0.29258973577053476, 0.29384314587921273, 0.2952927599611273, 0.29671449423039853, 0.298097831954079, 0.2994166078715978, 0.3008600028196814, 0.3022654989119415, 0.30375438574148483, 0.3051046540735178, 0.3064004218849587, 0.30769607091722084, 0.3090295813635437, 0.3104134006370208, 0.31172987998823937, 0.31313124507300377, 0.3144444557646396, 0.31589380846081155, 0.3172531175341444, 0.31866953536788306, 0.320048724663526, 0.32151239145748745, 0.322905996937825, 0.32432267073592075, 0.32589789863939894, 0.3272307360083743, 0.3285694258962762, 0.33008606863908846, 0.3315554788196347, 0.3329458450400236, 0.33432988549195697, 0.33573752815911584, 0.33728350753718095, 0.33868857339852076, 0.33999107209872836, 0.34144280589876447, 0.3428128002959234, 0.3442867690610066, 0.34572037521118926, 0.3471955015729632, 0.3486900481988944, 0.3501500769301911, 0.351696352841391, 0.3531291987231603, 0.3546125620548241, 0.35610070619807527, 0.3575824446339555, 0.35910220014194466, 0.3606552974352088, 0.36209716774811573, 0.3635401942455884, 0.365149869278639, 0.366726429905153, 0.3681713331958067, 0.36970947679316907, 0.37115825688073395, 0.3725166760718265, 0.37402558197284047, 0.3755014749262537, 0.37717844382691545, 0.37878749906017395, 0.38041722984089377, 0.3819378057695552, 0.38347028946607753, 0.38500166868299196, 0.3864787990518831, 0.3879451151061147, 0.3895428841729558, 0.3910741342138276, 0.3927397628571272, 0.39439735224166894, 0.3958889766116253, 0.39758954337124425, 0.39907276136467573, 0.40055556518073193, 0.4020758781326437, 0.40363784396091945, 0.4052112642472221, 0.40678926058035286, 0.4083315067825918, 0.409950463062675, 0.4117098000659602, 0.4130669953961106, 0.4147737025805344, 0.41630733045864365, 0.41805637664117873, 0.4194946132787111, 0.42119933105041013, 0.4226840105781751, 0.42459988868828646, 0.4263353645147123, 0.427938808373591, 0.4296006066734075, 0.4313619677206337, 0.4328734278986964, 0.4344892697628817, 0.43621000036739044, 0.4376087312090167, 0.43953656226034915, 0.44102989401282805, 0.4425473962821618, 0.44413492588413805, 0.44572975440087503, 0.4475565675705801, 0.44934456325689254, 0.45097240440942826, 0.4523986152324431, 0.4541095433470096, 0.4558921536371996, 0.4575641660547482, 0.4593296060420141, 0.4611099937915898, 0.46283105184451073, 0.46430425985281215, 0.46577638457287296, 0.46762906642942964, 0.46944176199673154, 0.47108742845865403, 0.47263251001017664, 0.474267997958771, 0.47600810773938296, 0.4776347585934803, 0.4795448415037414, 0.48110681044153164, 0.4828531394147669, 0.48469630764770444, 0.4863564916692864, 0.48795538402816063, 0.4899214322514168, 0.491547873668668, 0.49329729254571025, 0.49487413271659847, 0.49629446362501545, 0.4977686195040871, 0.49914175450722886, 0.500600422044233, 0.5020645557076118, 0.5034369182299872, 0.504673687261485, 0.5062593890836254, 0.50782292748035, 0.5092655112911696, 0.5108505735484035, 0.5122341993616939, 0.5133949431493692, 0.5151184641059672, 0.5166205354766594, 0.5183280608467389, 0.5198561856512502, 0.5213688171005665, 0.522862323894546, 0.524032560698616, 0.5260092502712271, 0.5273120761794835, 0.5288743596853519, 0.5301402214022141, 0.5313858797161001, 0.532802610311334, 0.5340273319269926, 0.5355767595069824, 0.5369843305288837, 0.5388002027112632, 0.5402577005673259, 0.5414463225513415, 0.5424949156990094, 0.5441523088669787, 0.545675943499219, 0.547341402053444, 0.5484646974929979, 0.5501312609746345, 0.5517817567805255, 0.5531138790035587, 0.5542422279792746, 0.555575795110933, 0.5568223679363442, 0.5580463625521765, 0.5597262546661137, 0.5614289530933934, 0.5628632668430301, 0.5642153413089374, 0.5652612626872168, 0.5668675060552075, 0.567972770371571, 0.5694096894613775, 0.5710371169974259, 0.5721367952272918, 0.573461227614956, 0.5747972202500053, 0.5759583324253057, 0.5776876625380174, 0.5792666607190651, 0.5803015256838494, 0.581638250217123, 0.5831693938482191, 0.5846997095474562, 0.5861594649717782, 0.5873770098392128, 0.5888381581503789, 0.5905237030301541, 0.5919103920348475, 0.5930848380913951, 0.5944581814473986, 0.5951975213013168, 0.5964568472211335, 0.5974902708283695, 0.5989127232800407, 0.6005262301307438, 0.601928147659855, 0.6032170089608727, 0.604137309058114, 0.6059013396555172, 0.607305143121762, 0.6087198570130974, 0.6102449888641426, 0.6112382558419658, 0.6125980407116917, 0.6137973704563032, 0.6148943518256884, 0.6161648278274141, 0.617576967667139, 0.6191673469387755, 0.6206873738877311, 0.6225636552718977, 0.6240432697214001, 0.6250215643653176, 0.6262771168649405, 0.6283280085197018, 0.6299045217078004, 0.6319657744624836, 0.633183723431878, 0.6355594037042614, 0.6368388918568266, 0.6380064234028557, 0.6395335139592414, 0.6412311127058167, 0.6422478269658379, 0.6440990732387435, 0.6455727863931966, 0.646697453713511, 0.6478661159869213, 0.649556594294701, 0.6505564669888707, 0.6528828828828829, 0.6548786074209803, 0.6563473248733324, 0.6576099796815197, 0.6588280462638576, 0.6597550861101624, 0.662037496280131, 0.6637190500252653, 0.6658796961609525, 0.6674840864030053, 0.6689160467587673, 0.6705049194507514, 0.6718758597919992, 0.6732345346356798, 0.6750557684607905, 0.6770627254742232, 0.6781908595784987, 0.6793943780914465, 0.6808523742070579, 0.6824379516179705, 0.684143058582233, 0.6851791424655747, 0.6867638251147934, 0.6880059868018232, 0.6898325109458614, 0.6920346443277012, 0.693815331010453, 0.6958518518518518, 0.6968387536957016, 0.700054309876639, 0.7015956180042867, 0.7023018317393419, 0.7037404832836809, 0.7042324913407113, 0.7043786164608342, 0.7064632748434883, 0.7071144817623691, 0.7091648189209164, 0.7101202309949824, 0.7112703330751355, 0.7121499053124688, 0.7144903806794924, 0.716414777497901, 0.718776916451335, 0.7200088290475665, 0.7229599364430825, 0.7249444509414104, 0.7254878342568056, 0.7288051561725335, 0.7302899287894201, 0.7328444211629125, 0.7346221441124781, 0.73627751462803, 0.7397397397397397, 0.7405500808942491, 0.7427706283118849, 0.7423370157149526, 0.745107475136349, 0.7468543046357616, 0.7485489928303175, 0.7511024872111484, 0.7539219263042685, 0.7548484277913764, 0.7525272161741835, 0.7537779568809188, 0.754180602006689, 0.7555364437755322, 0.7557880676758683, 0.7592635212888378, 0.7594182164997616, 0.7594561186650185, 0.762995367987648, 0.7621333333333333, 0.7631359466221852, 0.7678983833718245, 0.7718040621266428, 0.7732376793512165, 0.7776324395029431, 0.7791411042944786, 0.7810945273631841, 0.7820274786483475, 0.7824561403508772, 0.7845213849287169, 0.7872340425531915, 0.7904592064199732, 0.7976689976689977, 0.8014669926650366, 0.8031536113936928, 0.8029739776951673, 0.8004410143329658, 0.802547770700637, 0.8027912621359223, 0.8022813688212928, 0.8026666666666666, 0.8028070175438596, 0.8067846607669616, 0.8068269976726145, 0.8080971659919028, 0.8088737201365188, 0.8108597285067873, 0.8108882521489972, 0.8111332007952287, 0.8112617309697602, 0.8133187772925764, 0.814139110604333, 0.8157575757575758, 0.8148148148148148, 0.8110661268556005, 0.8135593220338984, 0.8207407407407408, 0.8199052132701422, 0.8291032148900169, 0.8291814946619217, 0.8336448598130841, 0.8376237623762376, 0.8428874734607219, 0.8497757847533632, 0.84375, 0.8427835051546392, 0.8401084010840109, 0.8424068767908309, 0.848297213622291, 0.8562091503267973, 0.8523489932885906, 0.8671586715867159, 0.876, 0.875, 0.8904109589041096, 0.8883495145631068, 0.8917525773195877, 0.88268156424581, 0.8795180722891566, 0.8831168831168831, 0.8827586206896552, 0.8905109489051095, 0.8951612903225806, 0.8898305084745762, 0.9, 0.89, 0.9101123595505618, 0.9069767441860465, 0.9240506329113924, 0.922077922077922, 0.9305555555555556, 0.9428571428571428, 0.95, 0.9433962264150944, 0.9347826086956522, 0.9318181818181818, 0.9230769230769231, 0.9393939393939394, 0.9310344827586207, 0.9310344827586207, 0.9285714285714286, 0.92, 0.9130434782608695, 0.9, 0.9444444444444444, 0.9230769230769231, 0.9230769230769231, 0.9, 0.9, 0.8571428571428571, 0.8571428571428571, 0.8333333333333334, 0.75, 0.5, 0.5, 0.5, 0.5]}],\n",
       "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('4586e0b8-04f3-45a8-89dd-99fa98589ce4');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create traces\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=pops, y=precs[:-1],\n",
    "                    mode='lines',\n",
    "                    name='lines'))\n",
    "#fig.add_trace(go.Scatter(y=bins[:-1], x=precs[:-1],\n",
    " #                   mode='lines',\n",
    "  #                  name='lines'))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "percs, bins = np.histogram(1 - oof_pred_deepfm, bins=500)\n",
    "precs = [precision_score(1 - train.target.values, ((1 - oof_pred_deepfm) > thresh).astype(int)) for thresh in bins]\n",
    "\n",
    "pops = (percs[::-1].cumsum() / 600000)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines",
         "name": "lines",
         "type": "scatter",
         "x": [
          1,
          0.9999966666666666,
          0.9999966666666666,
          0.9999966666666666,
          0.9999966666666666,
          0.9999933333333333,
          0.99999,
          0.9999883333333334,
          0.9999883333333334,
          0.9999833333333333,
          0.9999833333333333,
          0.9999783333333333,
          0.9999783333333333,
          0.99997,
          0.9999666666666667,
          0.9999616666666666,
          0.9999583333333333,
          0.9999533333333334,
          0.9999516666666667,
          0.9999516666666667,
          0.999945,
          0.999935,
          0.9999266666666666,
          0.9999233333333334,
          0.9999116666666666,
          0.9999,
          0.9998833333333333,
          0.99988,
          0.9998716666666667,
          0.9998683333333334,
          0.9998566666666666,
          0.9998516666666667,
          0.9998333333333334,
          0.9998166666666667,
          0.9998033333333334,
          0.9997933333333333,
          0.9997716666666666,
          0.9997583333333333,
          0.9997433333333333,
          0.9997233333333333,
          0.9997016666666667,
          0.9996766666666667,
          0.9996566666666666,
          0.999635,
          0.9996133333333334,
          0.9995833333333334,
          0.9995483333333334,
          0.9995033333333333,
          0.99949,
          0.9994616666666667,
          0.9994183333333333,
          0.999385,
          0.9993533333333333,
          0.9993066666666667,
          0.9992566666666667,
          0.999215,
          0.9991583333333334,
          0.9991083333333334,
          0.9990633333333333,
          0.999015,
          0.998945,
          0.998875,
          0.99882,
          0.998765,
          0.998695,
          0.998625,
          0.9985383333333333,
          0.9984733333333333,
          0.9984016666666666,
          0.9983233333333333,
          0.998255,
          0.9981583333333334,
          0.9980466666666666,
          0.9979416666666666,
          0.9978516666666667,
          0.99774,
          0.997625,
          0.9975,
          0.99737,
          0.9972533333333333,
          0.9971216666666667,
          0.9969766666666666,
          0.9968616666666666,
          0.9967233333333333,
          0.9965916666666667,
          0.996425,
          0.9962616666666667,
          0.9960833333333333,
          0.9959083333333333,
          0.995725,
          0.9955116666666667,
          0.99531,
          0.99511,
          0.9949033333333334,
          0.9946566666666666,
          0.99442,
          0.9942266666666667,
          0.994005,
          0.99375,
          0.9935233333333333,
          0.9932583333333334,
          0.99301,
          0.9927583333333333,
          0.9925133333333334,
          0.9922483333333333,
          0.9920266666666666,
          0.9917283333333333,
          0.9914266666666667,
          0.9911483333333333,
          0.9908633333333333,
          0.9905516666666667,
          0.9902366666666667,
          0.9899333333333333,
          0.98961,
          0.9892883333333333,
          0.9889916666666667,
          0.9886683333333334,
          0.988345,
          0.9880366666666667,
          0.9876716666666666,
          0.9872733333333333,
          0.9868933333333333,
          0.9865533333333333,
          0.9861633333333333,
          0.9857483333333333,
          0.985315,
          0.9848983333333333,
          0.98452,
          0.98412,
          0.9837133333333333,
          0.9832783333333334,
          0.9827866666666667,
          0.982395,
          0.98196,
          0.98154,
          0.9810983333333333,
          0.9807016666666667,
          0.9802716666666667,
          0.97986,
          0.9794366666666666,
          0.979005,
          0.9785183333333334,
          0.978015,
          0.9775,
          0.97704,
          0.9765233333333333,
          0.9760183333333333,
          0.9755016666666667,
          0.974955,
          0.9744616666666667,
          0.9739966666666666,
          0.973475,
          0.9729383333333333,
          0.97237,
          0.9718466666666666,
          0.9713566666666666,
          0.9708616666666666,
          0.97031,
          0.9697116666666666,
          0.96917,
          0.9686333333333333,
          0.9680566666666667,
          0.9675266666666666,
          0.9670166666666666,
          0.9663966666666667,
          0.9658383333333334,
          0.9652716666666666,
          0.9647283333333333,
          0.964145,
          0.9636166666666667,
          0.963,
          0.9624116666666667,
          0.9618483333333333,
          0.96126,
          0.9606616666666666,
          0.96002,
          0.9593566666666666,
          0.958775,
          0.958195,
          0.957555,
          0.9569283333333334,
          0.9563233333333333,
          0.9557266666666666,
          0.9550683333333333,
          0.95442,
          0.9537416666666667,
          0.95305,
          0.9523666666666667,
          0.951695,
          0.951005,
          0.9503183333333334,
          0.949615,
          0.9489583333333333,
          0.9482466666666667,
          0.9475816666666667,
          0.946915,
          0.946125,
          0.9453883333333334,
          0.9446533333333333,
          0.943875,
          0.9431183333333333,
          0.942415,
          0.9416516666666667,
          0.9408633333333334,
          0.94011,
          0.93932,
          0.9385566666666667,
          0.937765,
          0.937045,
          0.936215,
          0.93545,
          0.9346183333333333,
          0.9339133333333334,
          0.9330416666666667,
          0.9321816666666667,
          0.9313733333333334,
          0.93055,
          0.9297233333333333,
          0.9288466666666667,
          0.9279883333333333,
          0.9270766666666667,
          0.9261533333333334,
          0.9252733333333333,
          0.9243766666666666,
          0.9235216666666667,
          0.922535,
          0.9216366666666667,
          0.92066,
          0.9197133333333334,
          0.9187433333333334,
          0.9177366666666666,
          0.9167383333333333,
          0.9157616666666667,
          0.91474,
          0.9136883333333333,
          0.9126633333333334,
          0.9115966666666667,
          0.91056,
          0.9095133333333333,
          0.9085033333333333,
          0.90736,
          0.9063333333333333,
          0.9052433333333333,
          0.904135,
          0.903005,
          0.9019533333333334,
          0.9007683333333333,
          0.89959,
          0.89838,
          0.8972566666666667,
          0.8960033333333334,
          0.89476,
          0.8934233333333333,
          0.8922283333333333,
          0.89097,
          0.8896683333333333,
          0.8884583333333333,
          0.8870833333333333,
          0.8857983333333334,
          0.8844833333333333,
          0.8832466666666666,
          0.8818616666666667,
          0.8805166666666666,
          0.8790683333333333,
          0.87762,
          0.8761916666666667,
          0.8747533333333334,
          0.8731683333333333,
          0.8715933333333333,
          0.8699683333333333,
          0.8683633333333334,
          0.86671,
          0.86514,
          0.8635383333333333,
          0.8619933333333333,
          0.86034,
          0.8587266666666666,
          0.857045,
          0.855325,
          0.8536033333333334,
          0.8517983333333333,
          0.8501066666666667,
          0.84832,
          0.8464883333333333,
          0.84472,
          0.8428066666666667,
          0.84095,
          0.8390366666666667,
          0.837045,
          0.8350816666666667,
          0.8331716666666666,
          0.831025,
          0.82897,
          0.8269016666666666,
          0.8247616666666666,
          0.822655,
          0.8205066666666667,
          0.8182783333333333,
          0.8160383333333333,
          0.8138433333333334,
          0.8116483333333333,
          0.8093983333333333,
          0.8071283333333333,
          0.8048033333333333,
          0.802435,
          0.80014,
          0.7978,
          0.7954383333333334,
          0.7930133333333333,
          0.7906633333333334,
          0.788205,
          0.7858083333333333,
          0.7833866666666667,
          0.7809483333333334,
          0.7783233333333334,
          0.7758233333333333,
          0.773175,
          0.7705083333333334,
          0.7678266666666667,
          0.7652783333333333,
          0.76264,
          0.75988,
          0.75712,
          0.7544433333333334,
          0.75169,
          0.74886,
          0.746135,
          0.7434516666666666,
          0.7406483333333334,
          0.737845,
          0.7350066666666667,
          0.7321616666666667,
          0.7291583333333334,
          0.72626,
          0.7233866666666666,
          0.7204133333333333,
          0.7175066666666666,
          0.7144983333333333,
          0.711405,
          0.7084533333333334,
          0.70545,
          0.702345,
          0.6993683333333334,
          0.696245,
          0.6931016666666666,
          0.68994,
          0.6867383333333333,
          0.6835833333333333,
          0.6803866666666667,
          0.6771066666666666,
          0.6739783333333333,
          0.670765,
          0.6674916666666667,
          0.6641333333333334,
          0.661,
          0.6577016666666666,
          0.654445,
          0.6512,
          0.6478766666666667,
          0.6445133333333334,
          0.6411766666666666,
          0.6379066666666666,
          0.6344833333333333,
          0.6311533333333333,
          0.62765,
          0.6242666666666666,
          0.6208566666666666,
          0.617435,
          0.6139166666666667,
          0.6104966666666667,
          0.607,
          0.6035366666666667,
          0.5999216666666667,
          0.5963033333333333,
          0.592585,
          0.58917,
          0.5854783333333333,
          0.5818333333333333,
          0.5781016666666666,
          0.57456,
          0.570905,
          0.5671916666666666,
          0.563495,
          0.5598433333333334,
          0.5559566666666667,
          0.5521016666666667,
          0.5482233333333333,
          0.5446383333333333,
          0.5410116666666667,
          0.5369066666666666,
          0.5330116666666667,
          0.5291783333333333,
          0.5252233333333334,
          0.52127,
          0.5172466666666666,
          0.5132066666666667,
          0.509235,
          0.5052766666666667,
          0.50116,
          0.49713833333333335,
          0.4930366666666667,
          0.48898166666666665,
          0.4848516666666667,
          0.48067,
          0.47652333333333335,
          0.4723016666666667,
          0.468025,
          0.46376833333333334,
          0.4596516666666667,
          0.455335,
          0.4512,
          0.4467683333333333,
          0.44253,
          0.43820333333333333,
          0.433965,
          0.429575,
          0.4251933333333333,
          0.42074666666666666,
          0.416335,
          0.41198666666666667,
          0.40751333333333334,
          0.40293,
          0.39825333333333335,
          0.39369333333333334,
          0.389105,
          0.3844183333333333,
          0.37993,
          0.37518,
          0.370505,
          0.36605,
          0.36143166666666665,
          0.35662166666666667,
          0.35205333333333333,
          0.3470816666666667,
          0.3420433333333333,
          0.3371416666666667,
          0.3322,
          0.3272083333333333,
          0.32244666666666666,
          0.3175,
          0.31251666666666666,
          0.30747833333333335,
          0.30231833333333336,
          0.29738333333333333,
          0.29228666666666664,
          0.287195,
          0.28205,
          0.27680666666666665,
          0.27166833333333334,
          0.2662966666666667,
          0.26102333333333333,
          0.255705,
          0.25034666666666666,
          0.24492333333333333,
          0.23947833333333332,
          0.23411333333333334,
          0.22877333333333333,
          0.22332,
          0.21763666666666667,
          0.21221666666666666,
          0.20671333333333333,
          0.20112,
          0.19542833333333334,
          0.18980333333333332,
          0.18424333333333334,
          0.17852333333333334,
          0.172725,
          0.16708833333333334,
          0.16145833333333334,
          0.15567,
          0.149815,
          0.14398,
          0.138315,
          0.132495,
          0.12645666666666666,
          0.12043333333333334,
          0.11444833333333333,
          0.10833166666666667,
          0.102175,
          0.09621166666666667,
          0.09036333333333334,
          0.08448333333333333,
          0.078545,
          0.072735,
          0.06714666666666666,
          0.06146,
          0.05590333333333333,
          0.050216666666666666,
          0.044686666666666666,
          0.03917333333333333,
          0.033811666666666663,
          0.028705,
          0.023816666666666667,
          0.019193333333333333,
          0.014751666666666666,
          0.010785,
          0.00721,
          0.004163333333333333,
          0.0018466666666666666,
          0.000445
         ],
         "y": [
          0.8127946879911466,
          0.8127960426534755,
          0.8127960426534755,
          0.8127960426534755,
          0.8127960426534755,
          0.81279875199168,
          0.8128014613479468,
          0.8128028160328538,
          0.8128028160328538,
          0.8128068801146686,
          0.8128068801146686,
          0.8128109442371252,
          0.8128109442371252,
          0.8128177178648692,
          0.8128187606253542,
          0.8128228248749535,
          0.8128255343972666,
          0.8128295987146067,
          0.8128309534960857,
          0.8128309534960857,
          0.8128363726671634,
          0.812842834784261,
          0.8128496089713245,
          0.8128523186777653,
          0.81286180279258,
          0.8128712871287128,
          0.812883169703132,
          0.8128842127721994,
          0.8128893207961688,
          0.8128920307840533,
          0.8128981820727638,
          0.812902247166663,
          0.8129121520253375,
          0.8129257030455583,
          0.8129332101980056,
          0.8129413412105169,
          0.8129556248676781,
          0.812963132757083,
          0.8129736632402317,
          0.8129865929573848,
          0.8130025457594849,
          0.8130228773970251,
          0.8130358089610766,
          0.8130517638938213,
          0.8130610502727721,
          0.8130821175489787,
          0.8131022511834511,
          0.8131255190077739,
          0.8131363662134355,
          0.8131510796645528,
          0.8131763308991397,
          0.8131967826880198,
          0.8132192150924265,
          0.8132505203607835,
          0.8132878773221428,
          0.8133101151070257,
          0.8133428969382563,
          0.8133719232982743,
          0.813396881745902,
          0.813427893141411,
          0.8134632036798822,
          0.8135085304300672,
          0.813534971266094,
          0.8135664211968447,
          0.8136151010401907,
          0.8136604497851212,
          0.8137127100778971,
          0.8137523285549272,
          0.8137940474858983,
          0.8138428765563595,
          0.8138852297258716,
          0.8139456833000777,
          0.8140117028595857,
          0.814075639023657,
          0.8141290205124008,
          0.8141984217665257,
          0.8142605354383327,
          0.8143375104427736,
          0.8144169164903696,
          0.8144904671497714,
          0.8145712743179118,
          0.8146546391924919,
          0.8147335721940691,
          0.8148198931833307,
          0.8148957697485597,
          0.8149852388957188,
          0.8150586275024801,
          0.8151543545553418,
          0.8152490607401953,
          0.8153506239172462,
          0.8154767983633207,
          0.8155934666921194,
          0.8157037915406337,
          0.8158196273674829,
          0.8159431897773771,
          0.8160753672157305,
          0.8161670041707457,
          0.816268529836369,
          0.8164109014675053,
          0.8165485125328377,
          0.8166791116778952,
          0.8168229255831596,
          0.8169678756998598,
          0.8170839014757149,
          0.8172349327873231,
          0.8173520201069864,
          0.8175205911565163,
          0.8176836074613015,
          0.8178644636103913,
          0.8180206486599229,
          0.818173711282097,
          0.8183296249044841,
          0.8184860933396189,
          0.8186524657861851,
          0.8188057745214152,
          0.8189789263475425,
          0.8191371895866663,
          0.8193107332628451,
          0.819443273022931,
          0.8196279802835963,
          0.8198286864157849,
          0.8200075658294717,
          0.8201769797882189,
          0.8203475421583307,
          0.8205694827449197,
          0.8207798182983783,
          0.8209646004071486,
          0.8211531169165346,
          0.8213344578574429,
          0.8215249022411678,
          0.8217222319214464,
          0.8219739109199691,
          0.8221658972884295,
          0.8223841432780697,
          0.8225730314947259,
          0.8227989379250126,
          0.8229719877434694,
          0.823200371325636,
          0.8234118479510678,
          0.8236094898087676,
          0.8238262317352822,
          0.824053713863307,
          0.8242511617919971,
          0.8245029838022165,
          0.8247001146319496,
          0.8249316452585191,
          0.8251450194753855,
          0.8253719026620491,
          0.8256278494904893,
          0.8258457233652085,
          0.826062033066506,
          0.8262889819118793,
          0.8265254906528848,
          0.8267806150607965,
          0.8270183225062938,
          0.8272399084442035,
          0.8274367958360014,
          0.8276667594205288,
          0.8279333891345709,
          0.8281691206565067,
          0.8283939571217179,
          0.8286429513423801,
          0.8288608754969716,
          0.8290481032729529,
          0.829305771612071,
          0.8295090793317722,
          0.8297629510172438,
          0.8299935905963164,
          0.83024164760833,
          0.8304529809571579,
          0.830687088958117,
          0.8308918394242242,
          0.8311341531669061,
          0.831360228588866,
          0.8316108515484987,
          0.831883710756026,
          0.8321514070192872,
          0.8323607380946173,
          0.8326036627895853,
          0.8328451107247103,
          0.8330857239396193,
          0.8333304286904359,
          0.8335646872536778,
          0.8337762917487579,
          0.8340353303577042,
          0.8342667913218988,
          0.8345259954881695,
          0.8347555213328199,
          0.8350171711174974,
          0.835300550470292,
          0.83555510346533,
          0.835794857214064,
          0.836028979143798,
          0.8362838080105739,
          0.8365242045980206,
          0.836771691932926,
          0.8370863610340423,
          0.8373684182690359,
          0.8376388516422251,
          0.8379499404052443,
          0.8382175442812938,
          0.8384646537530352,
          0.8387390241614469,
          0.8390007050262348,
          0.8392971744441253,
          0.839586083549802,
          0.8398533919102735,
          0.84011808217766,
          0.8403598546494565,
          0.8406776221273959,
          0.8409482067454166,
          0.8412845885397069,
          0.8415163434151635,
          0.8418380743982494,
          0.8421373516249515,
          0.8423886590411293,
          0.8426611502158222,
          0.842951487360039,
          0.8432446690160558,
          0.8435217396770434,
          0.8438208993862428,
          0.8441384795894128,
          0.8444585744032393,
          0.8447404196701922,
          0.8449882966109801,
          0.8453410078389076,
          0.8456369285076187,
          0.8459673857160442,
          0.8463017461962785,
          0.846598433367317,
          0.8469259518888861,
          0.8472773946764162,
          0.8475713295125915,
          0.8479349323304983,
          0.848281598575736,
          0.8486060942516226,
          0.8489207580838016,
          0.8492209922099221,
          0.8495679007821032,
          0.849894148251153,
          0.8502689120084641,
          0.8506105185730047,
          0.8509572011945222,
          0.8512758972203635,
          0.8515991974942922,
          0.8519435595337527,
          0.8522853637912818,
          0.8526365714751535,
          0.8529835184814147,
          0.8533585707545592,
          0.853772865428328,
          0.8541489710462395,
          0.8545202535565447,
          0.8548745930133729,
          0.8552364277136155,
          0.8556540733344449,
          0.8560052525442011,
          0.8564471582902772,
          0.8568447671497839,
          0.8572141928432795,
          0.8575803663755689,
          0.8579180030125663,
          0.8583426397380326,
          0.8587652457810447,
          0.8591816503725986,
          0.8595836146961757,
          0.8599605222043547,
          0.8604259201643059,
          0.8608506260564942,
          0.8613761803590552,
          0.861856979989175,
          0.8623241914827335,
          0.8627736551309615,
          0.8632139472673477,
          0.8636223017966109,
          0.8641292977195062,
          0.8645940889223579,
          0.8650693176359857,
          0.8655579263243017,
          0.8660580050843288,
          0.866572486836673,
          0.8670441356378807,
          0.8675244404627185,
          0.8679879423421862,
          0.8684416137891846,
          0.8688884758070258,
          0.8693739223497235,
          0.8698666327653539,
          0.8703514546211175,
          0.8708369840075522,
          0.871331038221415,
          0.8718490217903593,
          0.8723797805308596,
          0.8728869414138319,
          0.873440205958489,
          0.8739933507971143,
          0.8745368715265365,
          0.8750710332873696,
          0.8755940795264928,
          0.8761780522868857,
          0.876756969869956,
          0.8772956867137935,
          0.8778219903072002,
          0.8783678000008284,
          0.878947619848752,
          0.879462760683547,
          0.8800075206818752,
          0.8806276623161653,
          0.8812167933284014,
          0.8817258926049435,
          0.8822620172839976,
          0.8828276615374826,
          0.8833994281240426,
          0.883991471907019,
          0.8846623297087328,
          0.8851499697095988,
          0.8858451622638256,
          0.8864472588442694,
          0.8870795492038134,
          0.8876813882530756,
          0.8882369138781077,
          0.8888662244915864,
          0.8895067272471119,
          0.8900624301361268,
          0.8905821105331542,
          0.8912680607857276,
          0.8918292266144866,
          0.8924565820239378,
          0.8930194041706703,
          0.8936497502863068,
          0.8942254492022748,
          0.8949230247418034,
          0.8955325203716614,
          0.8961414185920561,
          0.8967610683083275,
          0.8974014917362255,
          0.8980079163027521,
          0.8986538340420668,
          0.8993447239383099,
          0.8999840027101291,
          0.9006378907080587,
          0.9012356225691552,
          0.901859058531389,
          0.9024648890357082,
          0.9030661687438831,
          0.9037259278584612,
          0.9043643503226605,
          0.9050347433865659,
          0.9057104783554449,
          0.9063884567670283,
          0.9069925591079832,
          0.9076303424696677,
          0.908231064058228,
          0.9088687010640434,
          0.9093645990922844,
          0.9100250620215345,
          0.9106418415604062,
          0.9113252457002456,
          0.9119868493488987,
          0.9126084797831956,
          0.9132610356999891,
          0.9138013920531739,
          0.914379153642071,
          0.9150021653480929,
          0.9156934597307417,
          0.9162564075181546,
          0.9168407093424676,
          0.9174434015996286,
          0.9180752002171847,
          0.9186563945203086,
          0.9192943437671609,
          0.9198336472238637,
          0.920486841337619,
          0.9211083844352147,
          0.9217777477773934,
          0.9223285865426504,
          0.9229661217181393,
          0.9236465196218848,
          0.9242982059095718,
          0.9249628701383087,
          0.9255947428498028,
          0.9261360798084128,
          0.9267636210909886,
          0.9273784927926265,
          0.9280879205213836,
          0.9287087825008076,
          0.9292897663361039,
          0.929867832781999,
          0.9304605261131152,
          0.9310618853680341,
          0.9316869236758419,
          0.9322912565707213,
          0.932879346563684,
          0.9335309276702413,
          0.9341706729220102,
          0.9348605500058456,
          0.9354194690728904,
          0.9360911183971817,
          0.9367434219277941,
          0.9374218443558634,
          0.938060725706675,
          0.9387161754530674,
          0.9394385224346965,
          0.9401772525849336,
          0.940828360975678,
          0.9413510432950691,
          0.9419795951070989,
          0.9425395581845821,
          0.9431562306239145,
          0.9437886391338245,
          0.9442634456264776,
          0.9448446435699337,
          0.9455517140080898,
          0.946223594830406,
          0.9467929441314392,
          0.9473277852140681,
          0.947996205647627,
          0.948531974901762,
          0.9492195787847126,
          0.9498406097284702,
          0.9505005971174767,
          0.9510957238229966,
          0.9516471927416384,
          0.9524545331391607,
          0.9530032596171556,
          0.9535965593038833,
          0.9542942120917011,
          0.9547550509088971,
          0.9552997485414052,
          0.9558712379911669,
          0.9565800819887577,
          0.9571207582265052,
          0.9577100060596879,
          0.9583671470211141,
          0.9590792589632893,
          0.9596658180290185,
          0.9601595424443107,
          0.960687635298612,
          0.9612752496536895,
          0.9618110236220473,
          0.9625246653511813,
          0.963141034327622,
          0.9636916936341935,
          0.9642380765566329,
          0.9649316880687909,
          0.9655866803623554,
          0.9662648466583939,
          0.9670588376965873,
          0.9676934497334372,
          0.9682997659252213,
          0.9689619063429834,
          0.969665565136909,
          0.9701680336599915,
          0.9709092641234672,
          0.971507512857809,
          0.9720861690918928,
          0.9727168084858375,
          0.9733342289091886,
          0.9740163269056991,
          0.9747113798790544,
          0.9754571548360048,
          0.9760922169185893,
          0.9767689775450507,
          0.9774152192620432,
          0.9779458325041159,
          0.97843419160894,
          0.9792251652434023,
          0.9797113303342543,
          0.9804387096774193,
          0.9810710691419884,
          0.9818665242688205,
          0.9824512663795898,
          0.9832266926942125,
          0.9838987634753513,
          0.9845006194480322,
          0.9851231663437586,
          0.9859033916323232,
          0.9865382544346836,
          0.9872767310986054,
          0.98785663554316,
          0.9889520085580434,
          0.9895245610574078,
          0.9902815795616101,
          0.9910863637405193,
          0.9916600476568705,
          0.9919730990346024,
          0.992546657921412,
          0.9928974444075672,
          0.993846039086976,
          0.9944690265486725,
          0.9953664908562133,
          0.9958195436335133,
          0.9964310706787963,
          0.9967870788468218,
          0.9971754603999549,
          0.9981455725544738,
          0.9986130374479889,
          0.9979983987189752,
          1,
          1
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"ec86a72d-10cf-4295-8d51-fe55336c42f9\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"ec86a72d-10cf-4295-8d51-fe55336c42f9\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        'ec86a72d-10cf-4295-8d51-fe55336c42f9',\n",
       "                        [{\"mode\": \"lines\", \"name\": \"lines\", \"type\": \"scatter\", \"x\": [1.0, 0.9999966666666666, 0.9999966666666666, 0.9999966666666666, 0.9999966666666666, 0.9999933333333333, 0.99999, 0.9999883333333334, 0.9999883333333334, 0.9999833333333333, 0.9999833333333333, 0.9999783333333333, 0.9999783333333333, 0.99997, 0.9999666666666667, 0.9999616666666666, 0.9999583333333333, 0.9999533333333334, 0.9999516666666667, 0.9999516666666667, 0.999945, 0.999935, 0.9999266666666666, 0.9999233333333334, 0.9999116666666666, 0.9999, 0.9998833333333333, 0.99988, 0.9998716666666667, 0.9998683333333334, 0.9998566666666666, 0.9998516666666667, 0.9998333333333334, 0.9998166666666667, 0.9998033333333334, 0.9997933333333333, 0.9997716666666666, 0.9997583333333333, 0.9997433333333333, 0.9997233333333333, 0.9997016666666667, 0.9996766666666667, 0.9996566666666666, 0.999635, 0.9996133333333334, 0.9995833333333334, 0.9995483333333334, 0.9995033333333333, 0.99949, 0.9994616666666667, 0.9994183333333333, 0.999385, 0.9993533333333333, 0.9993066666666667, 0.9992566666666667, 0.999215, 0.9991583333333334, 0.9991083333333334, 0.9990633333333333, 0.999015, 0.998945, 0.998875, 0.99882, 0.998765, 0.998695, 0.998625, 0.9985383333333333, 0.9984733333333333, 0.9984016666666666, 0.9983233333333333, 0.998255, 0.9981583333333334, 0.9980466666666666, 0.9979416666666666, 0.9978516666666667, 0.99774, 0.997625, 0.9975, 0.99737, 0.9972533333333333, 0.9971216666666667, 0.9969766666666666, 0.9968616666666666, 0.9967233333333333, 0.9965916666666667, 0.996425, 0.9962616666666667, 0.9960833333333333, 0.9959083333333333, 0.995725, 0.9955116666666667, 0.99531, 0.99511, 0.9949033333333334, 0.9946566666666666, 0.99442, 0.9942266666666667, 0.994005, 0.99375, 0.9935233333333333, 0.9932583333333334, 0.99301, 0.9927583333333333, 0.9925133333333334, 0.9922483333333333, 0.9920266666666666, 0.9917283333333333, 0.9914266666666667, 0.9911483333333333, 0.9908633333333333, 0.9905516666666667, 0.9902366666666667, 0.9899333333333333, 0.98961, 0.9892883333333333, 0.9889916666666667, 0.9886683333333334, 0.988345, 0.9880366666666667, 0.9876716666666666, 0.9872733333333333, 0.9868933333333333, 0.9865533333333333, 0.9861633333333333, 0.9857483333333333, 0.985315, 0.9848983333333333, 0.98452, 0.98412, 0.9837133333333333, 0.9832783333333334, 0.9827866666666667, 0.982395, 0.98196, 0.98154, 0.9810983333333333, 0.9807016666666667, 0.9802716666666667, 0.97986, 0.9794366666666666, 0.979005, 0.9785183333333334, 0.978015, 0.9775, 0.97704, 0.9765233333333333, 0.9760183333333333, 0.9755016666666667, 0.974955, 0.9744616666666667, 0.9739966666666666, 0.973475, 0.9729383333333333, 0.97237, 0.9718466666666666, 0.9713566666666666, 0.9708616666666666, 0.97031, 0.9697116666666666, 0.96917, 0.9686333333333333, 0.9680566666666667, 0.9675266666666666, 0.9670166666666666, 0.9663966666666667, 0.9658383333333334, 0.9652716666666666, 0.9647283333333333, 0.964145, 0.9636166666666667, 0.963, 0.9624116666666667, 0.9618483333333333, 0.96126, 0.9606616666666666, 0.96002, 0.9593566666666666, 0.958775, 0.958195, 0.957555, 0.9569283333333334, 0.9563233333333333, 0.9557266666666666, 0.9550683333333333, 0.95442, 0.9537416666666667, 0.95305, 0.9523666666666667, 0.951695, 0.951005, 0.9503183333333334, 0.949615, 0.9489583333333333, 0.9482466666666667, 0.9475816666666667, 0.946915, 0.946125, 0.9453883333333334, 0.9446533333333333, 0.943875, 0.9431183333333333, 0.942415, 0.9416516666666667, 0.9408633333333334, 0.94011, 0.93932, 0.9385566666666667, 0.937765, 0.937045, 0.936215, 0.93545, 0.9346183333333333, 0.9339133333333334, 0.9330416666666667, 0.9321816666666667, 0.9313733333333334, 0.93055, 0.9297233333333333, 0.9288466666666667, 0.9279883333333333, 0.9270766666666667, 0.9261533333333334, 0.9252733333333333, 0.9243766666666666, 0.9235216666666667, 0.922535, 0.9216366666666667, 0.92066, 0.9197133333333334, 0.9187433333333334, 0.9177366666666666, 0.9167383333333333, 0.9157616666666667, 0.91474, 0.9136883333333333, 0.9126633333333334, 0.9115966666666667, 0.91056, 0.9095133333333333, 0.9085033333333333, 0.90736, 0.9063333333333333, 0.9052433333333333, 0.904135, 0.903005, 0.9019533333333334, 0.9007683333333333, 0.89959, 0.89838, 0.8972566666666667, 0.8960033333333334, 0.89476, 0.8934233333333333, 0.8922283333333333, 0.89097, 0.8896683333333333, 0.8884583333333333, 0.8870833333333333, 0.8857983333333334, 0.8844833333333333, 0.8832466666666666, 0.8818616666666667, 0.8805166666666666, 0.8790683333333333, 0.87762, 0.8761916666666667, 0.8747533333333334, 0.8731683333333333, 0.8715933333333333, 0.8699683333333333, 0.8683633333333334, 0.86671, 0.86514, 0.8635383333333333, 0.8619933333333333, 0.86034, 0.8587266666666666, 0.857045, 0.855325, 0.8536033333333334, 0.8517983333333333, 0.8501066666666667, 0.84832, 0.8464883333333333, 0.84472, 0.8428066666666667, 0.84095, 0.8390366666666667, 0.837045, 0.8350816666666667, 0.8331716666666666, 0.831025, 0.82897, 0.8269016666666666, 0.8247616666666666, 0.822655, 0.8205066666666667, 0.8182783333333333, 0.8160383333333333, 0.8138433333333334, 0.8116483333333333, 0.8093983333333333, 0.8071283333333333, 0.8048033333333333, 0.802435, 0.80014, 0.7978, 0.7954383333333334, 0.7930133333333333, 0.7906633333333334, 0.788205, 0.7858083333333333, 0.7833866666666667, 0.7809483333333334, 0.7783233333333334, 0.7758233333333333, 0.773175, 0.7705083333333334, 0.7678266666666667, 0.7652783333333333, 0.76264, 0.75988, 0.75712, 0.7544433333333334, 0.75169, 0.74886, 0.746135, 0.7434516666666666, 0.7406483333333334, 0.737845, 0.7350066666666667, 0.7321616666666667, 0.7291583333333334, 0.72626, 0.7233866666666666, 0.7204133333333333, 0.7175066666666666, 0.7144983333333333, 0.711405, 0.7084533333333334, 0.70545, 0.702345, 0.6993683333333334, 0.696245, 0.6931016666666666, 0.68994, 0.6867383333333333, 0.6835833333333333, 0.6803866666666667, 0.6771066666666666, 0.6739783333333333, 0.670765, 0.6674916666666667, 0.6641333333333334, 0.661, 0.6577016666666666, 0.654445, 0.6512, 0.6478766666666667, 0.6445133333333334, 0.6411766666666666, 0.6379066666666666, 0.6344833333333333, 0.6311533333333333, 0.62765, 0.6242666666666666, 0.6208566666666666, 0.617435, 0.6139166666666667, 0.6104966666666667, 0.607, 0.6035366666666667, 0.5999216666666667, 0.5963033333333333, 0.592585, 0.58917, 0.5854783333333333, 0.5818333333333333, 0.5781016666666666, 0.57456, 0.570905, 0.5671916666666666, 0.563495, 0.5598433333333334, 0.5559566666666667, 0.5521016666666667, 0.5482233333333333, 0.5446383333333333, 0.5410116666666667, 0.5369066666666666, 0.5330116666666667, 0.5291783333333333, 0.5252233333333334, 0.52127, 0.5172466666666666, 0.5132066666666667, 0.509235, 0.5052766666666667, 0.50116, 0.49713833333333335, 0.4930366666666667, 0.48898166666666665, 0.4848516666666667, 0.48067, 0.47652333333333335, 0.4723016666666667, 0.468025, 0.46376833333333334, 0.4596516666666667, 0.455335, 0.4512, 0.4467683333333333, 0.44253, 0.43820333333333333, 0.433965, 0.429575, 0.4251933333333333, 0.42074666666666666, 0.416335, 0.41198666666666667, 0.40751333333333334, 0.40293, 0.39825333333333335, 0.39369333333333334, 0.389105, 0.3844183333333333, 0.37993, 0.37518, 0.370505, 0.36605, 0.36143166666666665, 0.35662166666666667, 0.35205333333333333, 0.3470816666666667, 0.3420433333333333, 0.3371416666666667, 0.3322, 0.3272083333333333, 0.32244666666666666, 0.3175, 0.31251666666666666, 0.30747833333333335, 0.30231833333333336, 0.29738333333333333, 0.29228666666666664, 0.287195, 0.28205, 0.27680666666666665, 0.27166833333333334, 0.2662966666666667, 0.26102333333333333, 0.255705, 0.25034666666666666, 0.24492333333333333, 0.23947833333333332, 0.23411333333333334, 0.22877333333333333, 0.22332, 0.21763666666666667, 0.21221666666666666, 0.20671333333333333, 0.20112, 0.19542833333333334, 0.18980333333333332, 0.18424333333333334, 0.17852333333333334, 0.172725, 0.16708833333333334, 0.16145833333333334, 0.15567, 0.149815, 0.14398, 0.138315, 0.132495, 0.12645666666666666, 0.12043333333333334, 0.11444833333333333, 0.10833166666666667, 0.102175, 0.09621166666666667, 0.09036333333333334, 0.08448333333333333, 0.078545, 0.072735, 0.06714666666666666, 0.06146, 0.05590333333333333, 0.050216666666666666, 0.044686666666666666, 0.03917333333333333, 0.033811666666666663, 0.028705, 0.023816666666666667, 0.019193333333333333, 0.014751666666666666, 0.010785, 0.00721, 0.004163333333333333, 0.0018466666666666666, 0.000445], \"y\": [0.8127946879911466, 0.8127960426534755, 0.8127960426534755, 0.8127960426534755, 0.8127960426534755, 0.81279875199168, 0.8128014613479468, 0.8128028160328538, 0.8128028160328538, 0.8128068801146686, 0.8128068801146686, 0.8128109442371252, 0.8128109442371252, 0.8128177178648692, 0.8128187606253542, 0.8128228248749535, 0.8128255343972666, 0.8128295987146067, 0.8128309534960857, 0.8128309534960857, 0.8128363726671634, 0.812842834784261, 0.8128496089713245, 0.8128523186777653, 0.81286180279258, 0.8128712871287128, 0.812883169703132, 0.8128842127721994, 0.8128893207961688, 0.8128920307840533, 0.8128981820727638, 0.812902247166663, 0.8129121520253375, 0.8129257030455583, 0.8129332101980056, 0.8129413412105169, 0.8129556248676781, 0.812963132757083, 0.8129736632402317, 0.8129865929573848, 0.8130025457594849, 0.8130228773970251, 0.8130358089610766, 0.8130517638938213, 0.8130610502727721, 0.8130821175489787, 0.8131022511834511, 0.8131255190077739, 0.8131363662134355, 0.8131510796645528, 0.8131763308991397, 0.8131967826880198, 0.8132192150924265, 0.8132505203607835, 0.8132878773221428, 0.8133101151070257, 0.8133428969382563, 0.8133719232982743, 0.813396881745902, 0.813427893141411, 0.8134632036798822, 0.8135085304300672, 0.813534971266094, 0.8135664211968447, 0.8136151010401907, 0.8136604497851212, 0.8137127100778971, 0.8137523285549272, 0.8137940474858983, 0.8138428765563595, 0.8138852297258716, 0.8139456833000777, 0.8140117028595857, 0.814075639023657, 0.8141290205124008, 0.8141984217665257, 0.8142605354383327, 0.8143375104427736, 0.8144169164903696, 0.8144904671497714, 0.8145712743179118, 0.8146546391924919, 0.8147335721940691, 0.8148198931833307, 0.8148957697485597, 0.8149852388957188, 0.8150586275024801, 0.8151543545553418, 0.8152490607401953, 0.8153506239172462, 0.8154767983633207, 0.8155934666921194, 0.8157037915406337, 0.8158196273674829, 0.8159431897773771, 0.8160753672157305, 0.8161670041707457, 0.816268529836369, 0.8164109014675053, 0.8165485125328377, 0.8166791116778952, 0.8168229255831596, 0.8169678756998598, 0.8170839014757149, 0.8172349327873231, 0.8173520201069864, 0.8175205911565163, 0.8176836074613015, 0.8178644636103913, 0.8180206486599229, 0.818173711282097, 0.8183296249044841, 0.8184860933396189, 0.8186524657861851, 0.8188057745214152, 0.8189789263475425, 0.8191371895866663, 0.8193107332628451, 0.819443273022931, 0.8196279802835963, 0.8198286864157849, 0.8200075658294717, 0.8201769797882189, 0.8203475421583307, 0.8205694827449197, 0.8207798182983783, 0.8209646004071486, 0.8211531169165346, 0.8213344578574429, 0.8215249022411678, 0.8217222319214464, 0.8219739109199691, 0.8221658972884295, 0.8223841432780697, 0.8225730314947259, 0.8227989379250126, 0.8229719877434694, 0.823200371325636, 0.8234118479510678, 0.8236094898087676, 0.8238262317352822, 0.824053713863307, 0.8242511617919971, 0.8245029838022165, 0.8247001146319496, 0.8249316452585191, 0.8251450194753855, 0.8253719026620491, 0.8256278494904893, 0.8258457233652085, 0.826062033066506, 0.8262889819118793, 0.8265254906528848, 0.8267806150607965, 0.8270183225062938, 0.8272399084442035, 0.8274367958360014, 0.8276667594205288, 0.8279333891345709, 0.8281691206565067, 0.8283939571217179, 0.8286429513423801, 0.8288608754969716, 0.8290481032729529, 0.829305771612071, 0.8295090793317722, 0.8297629510172438, 0.8299935905963164, 0.83024164760833, 0.8304529809571579, 0.830687088958117, 0.8308918394242242, 0.8311341531669061, 0.831360228588866, 0.8316108515484987, 0.831883710756026, 0.8321514070192872, 0.8323607380946173, 0.8326036627895853, 0.8328451107247103, 0.8330857239396193, 0.8333304286904359, 0.8335646872536778, 0.8337762917487579, 0.8340353303577042, 0.8342667913218988, 0.8345259954881695, 0.8347555213328199, 0.8350171711174974, 0.835300550470292, 0.83555510346533, 0.835794857214064, 0.836028979143798, 0.8362838080105739, 0.8365242045980206, 0.836771691932926, 0.8370863610340423, 0.8373684182690359, 0.8376388516422251, 0.8379499404052443, 0.8382175442812938, 0.8384646537530352, 0.8387390241614469, 0.8390007050262348, 0.8392971744441253, 0.839586083549802, 0.8398533919102735, 0.84011808217766, 0.8403598546494565, 0.8406776221273959, 0.8409482067454166, 0.8412845885397069, 0.8415163434151635, 0.8418380743982494, 0.8421373516249515, 0.8423886590411293, 0.8426611502158222, 0.842951487360039, 0.8432446690160558, 0.8435217396770434, 0.8438208993862428, 0.8441384795894128, 0.8444585744032393, 0.8447404196701922, 0.8449882966109801, 0.8453410078389076, 0.8456369285076187, 0.8459673857160442, 0.8463017461962785, 0.846598433367317, 0.8469259518888861, 0.8472773946764162, 0.8475713295125915, 0.8479349323304983, 0.848281598575736, 0.8486060942516226, 0.8489207580838016, 0.8492209922099221, 0.8495679007821032, 0.849894148251153, 0.8502689120084641, 0.8506105185730047, 0.8509572011945222, 0.8512758972203635, 0.8515991974942922, 0.8519435595337527, 0.8522853637912818, 0.8526365714751535, 0.8529835184814147, 0.8533585707545592, 0.853772865428328, 0.8541489710462395, 0.8545202535565447, 0.8548745930133729, 0.8552364277136155, 0.8556540733344449, 0.8560052525442011, 0.8564471582902772, 0.8568447671497839, 0.8572141928432795, 0.8575803663755689, 0.8579180030125663, 0.8583426397380326, 0.8587652457810447, 0.8591816503725986, 0.8595836146961757, 0.8599605222043547, 0.8604259201643059, 0.8608506260564942, 0.8613761803590552, 0.861856979989175, 0.8623241914827335, 0.8627736551309615, 0.8632139472673477, 0.8636223017966109, 0.8641292977195062, 0.8645940889223579, 0.8650693176359857, 0.8655579263243017, 0.8660580050843288, 0.866572486836673, 0.8670441356378807, 0.8675244404627185, 0.8679879423421862, 0.8684416137891846, 0.8688884758070258, 0.8693739223497235, 0.8698666327653539, 0.8703514546211175, 0.8708369840075522, 0.871331038221415, 0.8718490217903593, 0.8723797805308596, 0.8728869414138319, 0.873440205958489, 0.8739933507971143, 0.8745368715265365, 0.8750710332873696, 0.8755940795264928, 0.8761780522868857, 0.876756969869956, 0.8772956867137935, 0.8778219903072002, 0.8783678000008284, 0.878947619848752, 0.879462760683547, 0.8800075206818752, 0.8806276623161653, 0.8812167933284014, 0.8817258926049435, 0.8822620172839976, 0.8828276615374826, 0.8833994281240426, 0.883991471907019, 0.8846623297087328, 0.8851499697095988, 0.8858451622638256, 0.8864472588442694, 0.8870795492038134, 0.8876813882530756, 0.8882369138781077, 0.8888662244915864, 0.8895067272471119, 0.8900624301361268, 0.8905821105331542, 0.8912680607857276, 0.8918292266144866, 0.8924565820239378, 0.8930194041706703, 0.8936497502863068, 0.8942254492022748, 0.8949230247418034, 0.8955325203716614, 0.8961414185920561, 0.8967610683083275, 0.8974014917362255, 0.8980079163027521, 0.8986538340420668, 0.8993447239383099, 0.8999840027101291, 0.9006378907080587, 0.9012356225691552, 0.901859058531389, 0.9024648890357082, 0.9030661687438831, 0.9037259278584612, 0.9043643503226605, 0.9050347433865659, 0.9057104783554449, 0.9063884567670283, 0.9069925591079832, 0.9076303424696677, 0.908231064058228, 0.9088687010640434, 0.9093645990922844, 0.9100250620215345, 0.9106418415604062, 0.9113252457002456, 0.9119868493488987, 0.9126084797831956, 0.9132610356999891, 0.9138013920531739, 0.914379153642071, 0.9150021653480929, 0.9156934597307417, 0.9162564075181546, 0.9168407093424676, 0.9174434015996286, 0.9180752002171847, 0.9186563945203086, 0.9192943437671609, 0.9198336472238637, 0.920486841337619, 0.9211083844352147, 0.9217777477773934, 0.9223285865426504, 0.9229661217181393, 0.9236465196218848, 0.9242982059095718, 0.9249628701383087, 0.9255947428498028, 0.9261360798084128, 0.9267636210909886, 0.9273784927926265, 0.9280879205213836, 0.9287087825008076, 0.9292897663361039, 0.929867832781999, 0.9304605261131152, 0.9310618853680341, 0.9316869236758419, 0.9322912565707213, 0.932879346563684, 0.9335309276702413, 0.9341706729220102, 0.9348605500058456, 0.9354194690728904, 0.9360911183971817, 0.9367434219277941, 0.9374218443558634, 0.938060725706675, 0.9387161754530674, 0.9394385224346965, 0.9401772525849336, 0.940828360975678, 0.9413510432950691, 0.9419795951070989, 0.9425395581845821, 0.9431562306239145, 0.9437886391338245, 0.9442634456264776, 0.9448446435699337, 0.9455517140080898, 0.946223594830406, 0.9467929441314392, 0.9473277852140681, 0.947996205647627, 0.948531974901762, 0.9492195787847126, 0.9498406097284702, 0.9505005971174767, 0.9510957238229966, 0.9516471927416384, 0.9524545331391607, 0.9530032596171556, 0.9535965593038833, 0.9542942120917011, 0.9547550509088971, 0.9552997485414052, 0.9558712379911669, 0.9565800819887577, 0.9571207582265052, 0.9577100060596879, 0.9583671470211141, 0.9590792589632893, 0.9596658180290185, 0.9601595424443107, 0.960687635298612, 0.9612752496536895, 0.9618110236220473, 0.9625246653511813, 0.963141034327622, 0.9636916936341935, 0.9642380765566329, 0.9649316880687909, 0.9655866803623554, 0.9662648466583939, 0.9670588376965873, 0.9676934497334372, 0.9682997659252213, 0.9689619063429834, 0.969665565136909, 0.9701680336599915, 0.9709092641234672, 0.971507512857809, 0.9720861690918928, 0.9727168084858375, 0.9733342289091886, 0.9740163269056991, 0.9747113798790544, 0.9754571548360048, 0.9760922169185893, 0.9767689775450507, 0.9774152192620432, 0.9779458325041159, 0.97843419160894, 0.9792251652434023, 0.9797113303342543, 0.9804387096774193, 0.9810710691419884, 0.9818665242688205, 0.9824512663795898, 0.9832266926942125, 0.9838987634753513, 0.9845006194480322, 0.9851231663437586, 0.9859033916323232, 0.9865382544346836, 0.9872767310986054, 0.98785663554316, 0.9889520085580434, 0.9895245610574078, 0.9902815795616101, 0.9910863637405193, 0.9916600476568705, 0.9919730990346024, 0.992546657921412, 0.9928974444075672, 0.993846039086976, 0.9944690265486725, 0.9953664908562133, 0.9958195436335133, 0.9964310706787963, 0.9967870788468218, 0.9971754603999549, 0.9981455725544738, 0.9986130374479889, 0.9979983987189752, 1.0, 1.0]}],\n",
       "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('ec86a72d-10cf-4295-8d51-fe55336c42f9');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create traces\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=pops, y=precs[:-1],\n",
    "                    mode='lines',\n",
    "                    name='lines'))\n",
    "#fig.add_trace(go.Scatter(y=bins[:-1], x=precs[:-1],\n",
    " #                   mode='lines',\n",
    "  #                  name='lines'))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.30373441e-04, 3.74876690e-04, 5.01987815e-04, ...,\n",
       "       8.29082370e-01, 8.35074425e-01, 8.36214662e-01])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(oof_pred_deepfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pseudo_index(oof_pred, y_pred, pos_frac=0.01, neg_frac=0.08):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    idx_neg = int(neg_frac * len(oof_pred))\n",
    "    neg_thresh = np.sort(oof_pred)[idx_neg]\n",
    "    \n",
    "    idx_pos = int(pos_frac * len(oof_pred))\n",
    "    pos_thresh = np.sort(oof_pred)[::-1][idx_pos]\n",
    "    \n",
    "    neg_indices = np.argwhere(y_pred < neg_thresh)\n",
    "    pos_indices = np.argwhere(y_pred > pos_thresh)\n",
    "    \n",
    "    print(f'Selected {len(pos_indices) * 100. / len(y_pred)}% ({len(pos_indices)}) of test as 1, for threshold {pos_thresh}')\n",
    "    print(f'Selected {len(neg_indices) * 100. / len(y_pred)}% ({len(neg_indices)}) of test as 0, for threshold {neg_thresh}')\n",
    "    \n",
    "    return pos_indices.flatten(), neg_indices.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 0.86325% (3453) of test as 1, for threshold 0.6492663025856018\n",
      "Selected 7.42125% (29685) of test as 0, for threshold 0.03081486001610756\n"
     ]
    }
   ],
   "source": [
    "pos_ind, neg_ind = get_pseudo_index(oof_pred_deepfm, y_pred_deepfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV 50 pseudo labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['target']\n",
    "N_Splits = 50\n",
    "Verbose = 0\n",
    "Epochs = 50\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "DROPOUT = 0.3\n",
    "NNLAYERS = (300, 300)\n",
    "PATIENCE = 5\n",
    "\n",
    "MAX_EMB_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(data, catcols, densecols, dnn_layers, last_dense):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    outputs_emb = []\n",
    "    \n",
    "    # Cat cols\n",
    "    for c in catcols:\n",
    "        \n",
    "        num_unique_values = int(data[c].nunique())\n",
    "        embed_dim = int(min(np.ceil(math.log(num_unique_values)), MAX_EMB_DIM))\n",
    "        inp = layers.Input(shape=(1,))\n",
    "        out = layers.Embedding(num_unique_values + 1, embed_dim, name=c)(inp)\n",
    "        out = layers.SpatialDropout1D(DROPOUT)(out)\n",
    "        out = layers.Reshape(target_shape=(embed_dim, ))(out)\n",
    "        inputs.append(inp)\n",
    "        outputs.append(out)\n",
    "        outputs_emb.append(out)\n",
    "        \n",
    "    # First dense for embeddings\n",
    "#     outputs_emb = layers.Concatenate()(outputs_emb)\n",
    "#     outputs_emb = layers.Dense(last_dense, activation=\"relu\")(outputs_emb)\n",
    "#     outputs_emb = layers.Dropout(DROPOUT)(outputs_emb)\n",
    "        \n",
    "    # Dense cols\n",
    "#     if densecols:\n",
    "#         dense_inp = layers.Input(shape=(len(densecols),))\n",
    "#         inputs.append(dense_inp)\n",
    "\n",
    "#         outputs.append(dense_inp)\n",
    "        \n",
    "    x = layers.Concatenate()(outputs)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # DNN layers\n",
    "    for size in dnn_layers:\n",
    "#         x = layers.Concatenate()([x, outputs_emb])\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(size, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(DROPOUT)(x)\n",
    "        \n",
    "#     x = layers.Concatenate()([x, outputs_emb])\n",
    "    \n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    y = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=y)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bak = train.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = test.iloc[pos_ind]\n",
    "pos_df.target = 1\n",
    "\n",
    "neg_df = test.iloc[neg_ind]\n",
    "neg_df.target = 0\n",
    "\n",
    "selected_test = pd.concat([pos_df, neg_df], axis=0).sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train, selected_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pa/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/pa/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/pa/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "validation AUC fold 1 : 0.80263\n",
      "validation AUC fold 2 : 0.80296\n",
      "validation AUC fold 3 : 0.81196\n",
      "validation AUC fold 4 : 0.80866\n",
      "validation AUC fold 5 : 0.80426\n",
      "validation AUC fold 6 : 0.80454\n",
      "validation AUC fold 7 : 0.80193\n",
      "validation AUC fold 8 : 0.80601\n",
      "validation AUC fold 9 : 0.80501\n",
      "validation AUC fold 10 : 0.8066\n",
      "validation AUC fold 11 : 0.80054\n",
      "validation AUC fold 12 : 0.80207\n",
      "validation AUC fold 13 : 0.80694\n",
      "validation AUC fold 14 : 0.80863\n",
      "validation AUC fold 15 : 0.79973\n",
      "validation AUC fold 16 : 0.80819\n",
      "validation AUC fold 17 : 0.80221\n",
      "validation AUC fold 18 : 0.80745\n",
      "validation AUC fold 19 : 0.79427\n",
      "validation AUC fold 20 : 0.8093\n",
      "validation AUC fold 21 : 0.80733\n",
      "validation AUC fold 22 : 0.80197\n",
      "validation AUC fold 23 : 0.8056\n",
      "validation AUC fold 24 : 0.80374\n",
      "validation AUC fold 25 : 0.8052\n",
      "validation AUC fold 26 : 0.80781\n",
      "validation AUC fold 27 : 0.80696\n",
      "validation AUC fold 28 : 0.80525\n",
      "validation AUC fold 29 : 0.80915\n",
      "validation AUC fold 30 : 0.8062\n",
      "validation AUC fold 31 : 0.80223\n",
      "validation AUC fold 32 : 0.80085\n",
      "validation AUC fold 33 : 0.80563\n",
      "validation AUC fold 34 : 0.80648\n",
      "validation AUC fold 35 : 0.81085\n",
      "validation AUC fold 36 : 0.80299\n",
      "validation AUC fold 37 : 0.81227\n",
      "validation AUC fold 38 : 0.80542\n",
      "validation AUC fold 39 : 0.80513\n",
      "validation AUC fold 40 : 0.80555\n",
      "validation AUC fold 41 : 0.81154\n",
      "validation AUC fold 42 : 0.80757\n",
      "validation AUC fold 43 : 0.79878\n",
      "validation AUC fold 44 : 0.79552\n",
      "validation AUC fold 45 : 0.80811\n",
      "validation AUC fold 46 : 0.80324\n",
      "validation AUC fold 47 : 0.80648\n",
      "validation AUC fold 48 : 0.80601\n",
      "validation AUC fold 49 : 0.80008\n",
      "validation AUC fold 50 : 0.80681\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oof_pred_deepfm = np.zeros((len(train), ))\n",
    "y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "for fold, (tr_ind, val_ind) in tqdm(enumerate(skf.split(train, train[target]))):\n",
    "\n",
    "    # Split\n",
    "    X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "    y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "    train_model_input = [X_train.loc[:, sparse_features].values[:, k] \\\n",
    "                         for k in range(X_train.loc[:, sparse_features].values.shape[1])]\n",
    "#     train_model_input += [X_train.loc[:, dense_features].values]\n",
    "    val_model_input = [X_val.loc[:, sparse_features].values[:, k] \\\n",
    "                       for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "#     val_model_input += [X_val.loc[:, dense_features].values]\n",
    "    test_model_input = [test.loc[:, sparse_features].values[:, k] \\\n",
    "                        for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "#     test_model_input += [test.loc[:, dense_features].values]\n",
    "\n",
    "\n",
    "    # Define model\n",
    "    model = create_model(data, sparse_features, ['id'], (300, 300), 256)\n",
    "    opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "\n",
    "    # Define callbacks\n",
    "    es = callbacks.EarlyStopping(\n",
    "        monitor='val_auc', \n",
    "        min_delta=0.001, \n",
    "        patience=PATIENCE, \n",
    "        verbose=Verbose, \n",
    "        mode='max', \n",
    "        baseline=None, \n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    #sb = callbacks.ModelCheckpoint(\n",
    "     #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "    #)\n",
    "#     clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "#                        step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "#                        gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_auc', \n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=3, \n",
    "        min_lr=1e-6,\n",
    "        verbose=Verbose,\n",
    "    )\n",
    "\n",
    "#     cb = TQDMNotebookCallback()\n",
    "#     setattr(cb,'on_train_batch_begin',lambda x,y:None)\n",
    "#     setattr(cb,'on_train_batch_end',lambda x,y:None)\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_model_input, utils.to_categorical(y_train),\n",
    "        validation_data=(val_model_input, utils.to_categorical(y_val)),\n",
    "        batch_size=1024, \n",
    "        epochs=Epochs, \n",
    "        verbose=Verbose,\n",
    "        callbacks=[reduce_lr, es]\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    val_pred = model.predict(val_model_input, batch_size=512)[:, 1]\n",
    "    print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "    oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "    y_pred_deepfm += model.predict(test_model_input, batch_size=512)[:, 1].ravel() / (N_Splits)\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF AUC : 0.78661\n"
     ]
    }
   ],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values[:600000], oof_pred_deepfm[:600000]), 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved!\n"
     ]
    }
   ],
   "source": [
    "test_idx = test.id.values\n",
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test_idx,\n",
    "    'target': y_pred_deepfm\n",
    "})\n",
    "submission.to_csv(\"submission_kerasemb_pseudo_cv_50.csv\", index=False)\n",
    "print(\"Submission file saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('oof_pred_kerasemb_pseudo_cv_50.npy',oof_pred_deepfm)\n",
    "np.save('y_pred_kerasemb_pseudo_cv_50.npy',    y_pred_deepfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* weight loss\n",
    "* pseudo labelling with only pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 0.3895% (1558) of test as 1, for threshold 0.681551456451416\n",
      "Selected 0.0% (0) of test as 0, for threshold 0.0003303734411019832\n"
     ]
    }
   ],
   "source": [
    "pos_ind, neg_ind = get_pseudo_index(oof_pred_deepfm, y_pred_deepfm, pos_frac=0.005, neg_frac=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = test.iloc[pos_ind]\n",
    "pos_df.target = 1\n",
    "\n",
    "neg_df = test.iloc[neg_ind]\n",
    "neg_df.target = 0\n",
    "\n",
    "selected_test = pd.concat([pos_df, neg_df], axis=0).sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train_bak\n",
    "                   , selected_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.187205"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bak.target.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05105372d6b940739c7724b58ce49307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pa/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/pa/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/pa/.virtualenvs/cat_in_dat/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "validation AUC fold 1 : 0.78902\n",
      "validation AUC fold 2 : 0.79128\n",
      "validation AUC fold 3 : 0.78552\n",
      "validation AUC fold 4 : 0.79237\n",
      "validation AUC fold 5 : 0.79634\n",
      "validation AUC fold 6 : 0.7974\n",
      "validation AUC fold 7 : 0.78726\n",
      "validation AUC fold 8 : 0.78786\n",
      "validation AUC fold 9 : 0.79466\n",
      "validation AUC fold 10 : 0.78852\n",
      "validation AUC fold 11 : 0.7878\n",
      "validation AUC fold 12 : 0.79338\n",
      "validation AUC fold 13 : 0.80213\n",
      "validation AUC fold 14 : 0.78815\n",
      "validation AUC fold 15 : 0.798\n",
      "validation AUC fold 16 : 0.79408\n",
      "validation AUC fold 17 : 0.79694\n",
      "validation AUC fold 18 : 0.79434\n",
      "validation AUC fold 19 : 0.79403\n",
      "validation AUC fold 20 : 0.78927\n",
      "validation AUC fold 21 : 0.79277\n",
      "validation AUC fold 22 : 0.78543\n",
      "validation AUC fold 23 : 0.78661\n",
      "validation AUC fold 24 : 0.79274\n",
      "validation AUC fold 25 : 0.78311\n",
      "validation AUC fold 26 : 0.79835\n",
      "validation AUC fold 27 : 0.78414\n",
      "validation AUC fold 28 : 0.79234\n",
      "validation AUC fold 29 : 0.7897\n",
      "validation AUC fold 30 : 0.79278\n",
      "validation AUC fold 31 : 0.78488\n",
      "validation AUC fold 32 : 0.79325\n",
      "validation AUC fold 33 : 0.78917\n",
      "validation AUC fold 34 : 0.78632\n",
      "validation AUC fold 35 : 0.78278\n",
      "validation AUC fold 36 : 0.77924\n",
      "validation AUC fold 37 : 0.78941\n",
      "validation AUC fold 38 : 0.78455\n",
      "validation AUC fold 39 : 0.78744\n",
      "validation AUC fold 40 : 0.78638\n",
      "validation AUC fold 41 : 0.79417\n",
      "validation AUC fold 42 : 0.79499\n",
      "validation AUC fold 43 : 0.7889\n",
      "validation AUC fold 44 : 0.78337\n",
      "validation AUC fold 45 : 0.78738\n",
      "validation AUC fold 46 : 0.78893\n",
      "validation AUC fold 47 : 0.78724\n",
      "validation AUC fold 48 : 0.79184\n",
      "validation AUC fold 49 : 0.78729\n",
      "validation AUC fold 50 : 0.78101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oof_pred_deepfm = np.zeros((len(train), ))\n",
    "y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "for fold, (tr_ind, val_ind) in tqdm(enumerate(skf.split(train, train[target]))):\n",
    "\n",
    "    # Split\n",
    "    X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "    y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "    train_model_input = [X_train.loc[:, sparse_features].values[:, k] \\\n",
    "                         for k in range(X_train.loc[:, sparse_features].values.shape[1])]\n",
    "#     train_model_input += [X_train.loc[:, dense_features].values]\n",
    "    val_model_input = [X_val.loc[:, sparse_features].values[:, k] \\\n",
    "                       for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "#     val_model_input += [X_val.loc[:, dense_features].values]\n",
    "    test_model_input = [test.loc[:, sparse_features].values[:, k] \\\n",
    "                        for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "#     test_model_input += [test.loc[:, dense_features].values]\n",
    "\n",
    "\n",
    "    # Define model\n",
    "    model = create_model(data, sparse_features, ['id'], (300, 300), 256)\n",
    "    opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "\n",
    "    # Define callbacks\n",
    "    es = callbacks.EarlyStopping(\n",
    "        monitor='val_auc', \n",
    "        min_delta=0.001, \n",
    "        patience=PATIENCE, \n",
    "        verbose=Verbose, \n",
    "        mode='max', \n",
    "        baseline=None, \n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    #sb = callbacks.ModelCheckpoint(\n",
    "     #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "    #)\n",
    "#     clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "#                        step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "#                        gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_auc', \n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=3, \n",
    "        min_lr=1e-6,\n",
    "        verbose=Verbose,\n",
    "    )\n",
    "\n",
    "#     cb = TQDMNotebookCallback()\n",
    "#     setattr(cb,'on_train_batch_begin',lambda x,y:None)\n",
    "#     setattr(cb,'on_train_batch_end',lambda x,y:None)\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_model_input, utils.to_categorical(y_train),\n",
    "        validation_data=(val_model_input, utils.to_categorical(y_val)),\n",
    "        batch_size=1024, \n",
    "        epochs=Epochs, \n",
    "        verbose=Verbose,\n",
    "        callbacks=[reduce_lr, es]\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    val_pred = model.predict(val_model_input, batch_size=512)[:, 1]\n",
    "    print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "    oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "    y_pred_deepfm += model.predict(test_model_input, batch_size=512)[:, 1].ravel() / (N_Splits)\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF AUC : 0.78627\n"
     ]
    }
   ],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values[:600000], oof_pred_deepfm[:600000]), 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved!\n"
     ]
    }
   ],
   "source": [
    "test_idx = test.id.values\n",
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test_idx,\n",
    "    'target': y_pred_deepfm\n",
    "})\n",
    "submission.to_csv(\"submission_kerasemb_pseudo005_cv_50.csv\", index=False)\n",
    "print(\"Submission file saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('oof_pred_kerasemb_pseudo005_cv_50.npy',oof_pred_deepfm)\n",
    "np.save('y_pred_kerasemb_pseudo005_cv_50.npy',    y_pred_deepfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_bak.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a4ba1b6039e46a98dc17fc7802355ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation AUC fold 1 : 0.7871\n",
      "validation AUC fold 2 : 0.79396\n",
      "validation AUC fold 3 : 0.78772\n",
      "validation AUC fold 4 : 0.79179\n",
      "validation AUC fold 5 : 0.79785\n",
      "validation AUC fold 6 : 0.79219\n",
      "validation AUC fold 7 : 0.78447\n",
      "validation AUC fold 8 : 0.79023\n",
      "validation AUC fold 9 : 0.78111\n",
      "validation AUC fold 10 : 0.7823\n",
      "validation AUC fold 11 : 0.78575\n",
      "validation AUC fold 12 : 0.78637\n",
      "validation AUC fold 13 : 0.78692\n",
      "validation AUC fold 14 : 0.78853\n",
      "validation AUC fold 15 : 0.7809\n",
      "validation AUC fold 16 : 0.78487\n",
      "validation AUC fold 17 : 0.78648\n",
      "validation AUC fold 18 : 0.79542\n",
      "validation AUC fold 19 : 0.79545\n",
      "validation AUC fold 20 : 0.78024\n",
      "validation AUC fold 21 : 0.78339\n",
      "validation AUC fold 22 : 0.78542\n",
      "validation AUC fold 23 : 0.78816\n",
      "validation AUC fold 24 : 0.7938\n",
      "validation AUC fold 25 : 0.78268\n",
      "validation AUC fold 26 : 0.79593\n",
      "validation AUC fold 27 : 0.78774\n",
      "validation AUC fold 28 : 0.77663\n",
      "validation AUC fold 29 : 0.7883\n",
      "validation AUC fold 30 : 0.78317\n",
      "validation AUC fold 31 : 0.79376\n",
      "validation AUC fold 32 : 0.78884\n",
      "validation AUC fold 33 : 0.79507\n",
      "validation AUC fold 34 : 0.78007\n",
      "validation AUC fold 35 : 0.77373\n",
      "validation AUC fold 36 : 0.78753\n",
      "validation AUC fold 37 : 0.78177\n",
      "validation AUC fold 38 : 0.7873\n",
      "validation AUC fold 39 : 0.7847\n",
      "validation AUC fold 40 : 0.78706\n",
      "validation AUC fold 41 : 0.77844\n",
      "validation AUC fold 42 : 0.78531\n",
      "validation AUC fold 43 : 0.78269\n",
      "validation AUC fold 44 : 0.77906\n",
      "validation AUC fold 45 : 0.79633\n",
      "validation AUC fold 46 : 0.78441\n",
      "validation AUC fold 47 : 0.78337\n",
      "validation AUC fold 48 : 0.78804\n",
      "validation AUC fold 49 : 0.7953\n",
      "validation AUC fold 50 : 0.78615\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oof_pred_deepfm = np.zeros((len(train), ))\n",
    "y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "for fold, (tr_ind, val_ind) in tqdm(enumerate(skf.split(train, train[target]))):\n",
    "\n",
    "    # Split\n",
    "    X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "    y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "    train_model_input = [X_train.loc[:, sparse_features].values[:, k] \\\n",
    "                         for k in range(X_train.loc[:, sparse_features].values.shape[1])]\n",
    "#     train_model_input += [X_train.loc[:, dense_features].values]\n",
    "    val_model_input = [X_val.loc[:, sparse_features].values[:, k] \\\n",
    "                       for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "#     val_model_input += [X_val.loc[:, dense_features].values]\n",
    "    test_model_input = [test.loc[:, sparse_features].values[:, k] \\\n",
    "                        for k in range(X_val.loc[:, sparse_features].values.shape[1])]\n",
    "#     test_model_input += [test.loc[:, dense_features].values]\n",
    "\n",
    "\n",
    "    # Define model\n",
    "    model = create_model(data, sparse_features, ['id'], (300, 300), 256)\n",
    "    opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "\n",
    "    # Define callbacks\n",
    "    es = callbacks.EarlyStopping(\n",
    "        monitor='val_auc', \n",
    "        min_delta=0.001, \n",
    "        patience=PATIENCE, \n",
    "        verbose=Verbose, \n",
    "        mode='max', \n",
    "        baseline=None, \n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    #sb = callbacks.ModelCheckpoint(\n",
    "     #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "    #)\n",
    "#     clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "#                        step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "#                        gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_auc', \n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=3, \n",
    "        min_lr=1e-6,\n",
    "        verbose=Verbose,\n",
    "    )\n",
    "\n",
    "#     cb = TQDMNotebookCallback()\n",
    "#     setattr(cb,'on_train_batch_begin',lambda x,y:None)\n",
    "#     setattr(cb,'on_train_batch_end',lambda x,y:None)\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_model_input, utils.to_categorical(y_train),\n",
    "        validation_data=(val_model_input, utils.to_categorical(y_val)),\n",
    "        batch_size=1024, \n",
    "        epochs=Epochs, \n",
    "        verbose=Verbose,\n",
    "        callbacks=[reduce_lr, es],\n",
    "        class_weight={0: 1, 1: 5.3417},\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    val_pred = model.predict(val_model_input, batch_size=512)[:, 1]\n",
    "    print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "    oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "    y_pred_deepfm += model.predict(test_model_input, batch_size=512)[:, 1].ravel() / (N_Splits)\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF AUC : 0.78632\n"
     ]
    }
   ],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values[:600000], oof_pred_deepfm[:600000]), 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved!\n"
     ]
    }
   ],
   "source": [
    "test_idx = test.id.values\n",
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test_idx,\n",
    "    'target': y_pred_deepfm\n",
    "})\n",
    "submission.to_csv(\"submission_kerasemb_weighted_cv_50.csv\", index=False)\n",
    "print(\"Submission file saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('oof_pred_kerasemb_weighted_cv_50.npy',oof_pred_deepfm)\n",
    "np.save('y_pred_kerasemb_weighted_cv_50.npy',    y_pred_deepfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['target']\n",
    "N_Splits = 5\n",
    "Verbose = 1\n",
    "Epochs = 50\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "DROPOUT = 0.2\n",
    "NNLAYERS = (256, 256, 256)\n",
    "PATIENCE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "oof_pred_deepfm = np.zeros((len(train), ))\n",
    "y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "\n",
    "for fold, (tr_ind, val_ind) in enumerate(skf.split(train, train[target])):\n",
    "    \n",
    "    # Split\n",
    "    X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "    y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "    train_model_input = {name:X_train[name] for name in feature_names}\n",
    "    val_model_input = {name:X_val[name] for name in feature_names}\n",
    "    test_model_input = {name:test[name] for name in feature_names}\n",
    "    \n",
    "    # Define model\n",
    "    model = DeepFM(sparse_feature_columns, sparse_feature_columns + dense_feature_columns,\n",
    "                   dnn_hidden_units=NNLAYERS, dnn_dropout=DROPOUT, dnn_use_bn=False, task='binary')\n",
    "    opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "    \n",
    "    # Define callbacks\n",
    "    es = callbacks.EarlyStopping(\n",
    "        monitor='val_auc', \n",
    "        min_delta=0.0, \n",
    "        patience=PATIENCE, \n",
    "        verbose=Verbose, \n",
    "        mode='max', \n",
    "        baseline=None, \n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    #sb = callbacks.ModelCheckpoint(\n",
    "     #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "    #)\n",
    "#     clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "#                        step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "#                        gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_auc', \n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=3, \n",
    "        min_lr=1e-7,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_model_input, y_train,\n",
    "        validation_data=(val_model_input, y_val),\n",
    "        batch_size=BATCH_SIZE, \n",
    "        epochs=Epochs, \n",
    "        verbose=Verbose,\n",
    "        callbacks=[reduce_lr, es]\n",
    "    )\n",
    "    \n",
    "    # Predict\n",
    "    val_pred = model.predict(val_model_input, batch_size=512)\n",
    "    print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "    oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "    y_pred_deepfm += model.predict(test_model_input, batch_size=512).ravel() / (N_Splits)\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Deeper\n",
    "* decrease LR\n",
    "* back to cyclic lr\n",
    "* decrease patience reduceLRplateau\n",
    "* add reg for DNN\n",
    "* dropout levels\n",
    "* treat emb dim\n",
    "* nunique + 1 ?\n",
    "* pseudo label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "grid_params = {\n",
    "    'dnn_dropout': [0.0, 0.1, 0.2, 0.3, 0.5],\n",
    "    'dnn_hidden_units': [(256,256, 256), (256, 256), (128, 128, 128), (256, 256, 256, 256)],\n",
    "    'emb_dim': [2, 3, 4, 8],\n",
    "    'lr': [1e-3, 1e-4],\n",
    "    'cyclic': [True, False],\n",
    "    'bn': [True, False],\n",
    "    'l2_reg_linear': [1e-05, 1e-04],\n",
    "    'l2_reg_embedding': [1e-05, 1e-04], \n",
    "    'l2_reg_dnn': [0.0, 1e-05, 1e-04]    \n",
    "}\n",
    "list_params = list(ParameterSampler(grid_params,\n",
    "                                    n_iter=50,\n",
    "                                    random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "\n",
    "model = DeepFM(sparse_feature_columns, sparse_feature_columns + dense_feature_columns,\n",
    "               dnn_hidden_units=param['dnn_hidden_units'], dnn_dropout=param['dnn_dropout'], \n",
    "               dnn_use_bn=param['bn'], task='binary',\n",
    "               l2_reg_linear=param['l2_reg_linear'], l2_reg_embedding=param['l2_reg_embedding'], \n",
    "               l2_reg_dnn=param['l2_reg_dnn'], init_std=0.0001,\n",
    "               seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [{'lr': 0.0001,\n",
    "  'l2_reg_linear': 0.0001,\n",
    "  'l2_reg_embedding': 1e-05,\n",
    "  'l2_reg_dnn': 0.0001,\n",
    "  'emb_dim': 4,\n",
    "  'dnn_hidden_units': (256, 256, 256),\n",
    "  'dnn_dropout': 0.0,\n",
    "  'cyclic': False,\n",
    "  'bn': False}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_perf = []\n",
    "\n",
    "for param in tqdm(params):\n",
    "    \n",
    "    oof_pred_deepfm = np.zeros((len(train), ))\n",
    "    y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    sparse_feature_columns = [SparseFeat(feat, train[feat].nunique() + 1, embedding_dim=param['emb_dim']) \n",
    "                          for feat in features]\n",
    "    dense_feature_columns = [DenseFeat(feat, 1) for feat in features_enc + indicator_cols]\n",
    "\n",
    "    feature_names = get_feature_names(sparse_feature_columns + dense_feature_columns)\n",
    "\n",
    "    all_features = features + features_enc + indicator_cols\n",
    "\n",
    "    for fold, (tr_ind, val_ind) in enumerate(skf.split(train, train[target])):\n",
    "\n",
    "        # Split\n",
    "        X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "        y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "        train_model_input = {name:X_train[name] for name in feature_names}\n",
    "        val_model_input = {name:X_val[name] for name in feature_names}\n",
    "        test_model_input = {name:test[name] for name in feature_names}\n",
    "\n",
    "        # Define model\n",
    "        model = DeepFM(sparse_feature_columns, sparse_feature_columns + dense_feature_columns,\n",
    "                       dnn_hidden_units=param['dnn_hidden_units'], dnn_dropout=param['dnn_dropout'], \n",
    "                       dnn_use_bn=param['bn'], task='binary',\n",
    "                       l2_reg_linear=param['l2_reg_linear'], l2_reg_embedding=param['l2_reg_embedding'], \n",
    "                       l2_reg_dnn=param['l2_reg_dnn'], init_std=0.0001,\n",
    "                       seed=SEED)\n",
    "        opt = keras.optimizers.Adam(learning_rate=param['lr'])\n",
    "        model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "\n",
    "        # Define callbacks\n",
    "        es = callbacks.EarlyStopping(\n",
    "            monitor='val_auc', \n",
    "            min_delta=0.0, \n",
    "            patience=PATIENCE, \n",
    "            verbose=Verbose, \n",
    "            mode='max', \n",
    "            baseline=None, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        #sb = callbacks.ModelCheckpoint(\n",
    "         #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "        #)\n",
    "        if param['cyclic']:\n",
    "            reduce_lr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "                           step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "                           gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "        else:\n",
    "            reduce_lr = ReduceLROnPlateau(\n",
    "                monitor='val_auc', \n",
    "                mode='max',\n",
    "                factor=0.5,\n",
    "                patience=3, \n",
    "                min_lr=1e-7,\n",
    "                verbose=True,\n",
    "            )\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train_model_input, y_train,\n",
    "            validation_data=(val_model_input, y_val),\n",
    "            batch_size=BATCH_SIZE, \n",
    "            epochs=Epochs, \n",
    "            verbose=Verbose,\n",
    "            callbacks=[reduce_lr, es]\n",
    "        )\n",
    "\n",
    "        # Predict\n",
    "        val_pred = model.predict(val_model_input, batch_size=512)\n",
    "        print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "        oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "        y_pred_deepfm += model.predict(test_model_input, batch_size=512).ravel() / (N_Splits)\n",
    "        K.clear_session()\n",
    "    cv_perf.append(round(roc_auc_score(train.target.values, oof_pred_deepfm), 5))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_ord = [feat for feat in features if 'ord' in feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_feature_columns = [SparseFeat(feat, train[feat].nunique() + 1, embedding_dim=4) \n",
    "                          for feat in features if 'ord' not in feat]\n",
    "dense_feature_columns = [DenseFeat(feat, 1) for feat in features_enc + indicator_cols + features_ord]\n",
    "\n",
    "feature_names = get_feature_names(sparse_feature_columns + dense_feature_columns)\n",
    "\n",
    "all_features = features + features_enc + indicator_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_perf = []\n",
    "\n",
    "for param in tqdm(list_params[:10]):\n",
    "    \n",
    "    oof_pred_deepfm = np.zeros((len(train), ))\n",
    "    y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    sparse_feature_columns = [SparseFeat(feat, train[feat].nunique() + 1, embedding_dim=param['emb_dim']) \n",
    "                          for feat in features]\n",
    "    dense_feature_columns = [DenseFeat(feat, 1) for feat in features_enc + indicator_cols]\n",
    "\n",
    "    feature_names = get_feature_names(sparse_feature_columns + dense_feature_columns)\n",
    "\n",
    "    all_features = features + features_enc + indicator_cols\n",
    "\n",
    "    for fold, (tr_ind, val_ind) in enumerate(skf.split(train, train[target])):\n",
    "\n",
    "        # Split\n",
    "        X_train, X_val = train[all_features].iloc[tr_ind], train[all_features].iloc[val_ind]\n",
    "        y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "        train_model_input = {name:X_train[name] for name in feature_names}\n",
    "        val_model_input = {name:X_val[name] for name in feature_names}\n",
    "        test_model_input = {name:test[name] for name in feature_names}\n",
    "\n",
    "        # Define model\n",
    "        model = DeepFM(sparse_feature_columns, sparse_feature_columns + dense_feature_columns,\n",
    "                       dnn_hidden_units=param['dnn_hidden_units'], dnn_dropout=param['dnn_dropout'], \n",
    "                       dnn_use_bn=param['bn'], task='binary',\n",
    "                       l2_reg_linear=param['l2_reg_linear'], l2_reg_embedding=param['l2_reg_embedding'], \n",
    "                       l2_reg_dnn=param['l2_reg_dnn'], init_std=0.0001,\n",
    "                       seed=SEED)\n",
    "        opt = keras.optimizers.Adam(learning_rate=param['lr'])\n",
    "        model.compile(opt, \"binary_crossentropy\", metrics=[auc])\n",
    "\n",
    "        # Define callbacks\n",
    "        es = callbacks.EarlyStopping(\n",
    "            monitor='val_auc', \n",
    "            min_delta=0.0, \n",
    "            patience=PATIENCE, \n",
    "            verbose=Verbose, \n",
    "            mode='max', \n",
    "            baseline=None, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        #sb = callbacks.ModelCheckpoint(\n",
    "         #   './nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose\n",
    "        #)\n",
    "        if param['cyclic']:\n",
    "            reduce_lr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "                           step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "                           gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "        else:\n",
    "            reduce_lr = ReduceLROnPlateau(\n",
    "                monitor='val_auc', \n",
    "                mode='max',\n",
    "                factor=0.5,\n",
    "                patience=3, \n",
    "                min_lr=1e-7,\n",
    "                verbose=True,\n",
    "            )\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train_model_input, y_train,\n",
    "            validation_data=(val_model_input, y_val),\n",
    "            batch_size=BATCH_SIZE, \n",
    "            epochs=Epochs, \n",
    "            verbose=Verbose,\n",
    "            callbacks=[reduce_lr, es]\n",
    "        )\n",
    "\n",
    "        # Predict\n",
    "        val_pred = model.predict(val_model_input, batch_size=512)\n",
    "        print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "        oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "        y_pred_deepfm += model.predict(test_model_input, batch_size=512).ravel() / (N_Splits)\n",
    "        K.clear_session()\n",
    "    cv_perf.append(round(roc_auc_score(train.target.values, oof_pred_deepfm), 5))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['target']\n",
    "N_Splits = 10\n",
    "Verbose = 1\n",
    "Epochs = 50\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "grid_params = {\n",
    "    'dnn_dropout': [0.0, 0.1, 0.2],\n",
    "    'dnn_hidden_units': [(256,), (512,), (256, 256), (512, 512)],\n",
    "    'linear': [linear_feature_columns, linear_feature_columns + dnn_feature_columns],\n",
    "    'sparse': [dnn_feature_columns, linear_feature_columns + dnn_feature_columns],\n",
    "    'batch_size': [128, 256, 512],\n",
    "}\n",
    "list_params = list(ParameterSampler(grid_params,\n",
    "                                    n_iter=20,\n",
    "                                    random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_perf = []\n",
    "\n",
    "for param in list_params:\n",
    "\n",
    "    oof_pred_deepfm = np.zeros((len(train), ))\n",
    "    y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "    for fold, (tr_ind, val_ind) in enumerate(skf.split(train, train[target])):\n",
    "        X_train, X_val = train[features+features_enc+indicator_cols].iloc[tr_ind], train[features+features_enc+indicator_cols].iloc[val_ind]\n",
    "        y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "        train_model_input = {name:X_train[name] for name in feature_names}\n",
    "        val_model_input = {name:X_val[name] for name in feature_names}\n",
    "        test_model_input = {name:test[name] for name in feature_names}\n",
    "        model = DeepFM(param['linear'], param['sparse'],\n",
    "                       dnn_hidden_units=param['dnn_hidden_units'], dnn_dropout=param['dnn_dropout'], dnn_use_bn=False, task='binary')\n",
    "        model.compile(\"adam\", \"binary_crossentropy\", metrics=[auc], )\n",
    "        es = callbacks.EarlyStopping(monitor='val_auc', min_delta=0.001, patience=4, verbose=Verbose, mode='max', baseline=None, restore_best_weights=True)\n",
    "        sb = callbacks.ModelCheckpoint('./nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose)\n",
    "        clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "                           step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "                           gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "        history = model.fit(train_model_input, y_train,\n",
    "                            validation_data=(val_model_input, y_val),\n",
    "                            batch_size=param['batch_size'], epochs=Epochs, verbose=Verbose,\n",
    "                            callbacks=[es, sb, clr],)\n",
    "        model.load_weights('./nn_model.w8')\n",
    "        val_pred = model.predict(val_model_input, batch_size=param['batch_size'])\n",
    "        print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "        oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "        y_pred_deepfm += model.predict(test_model_input, batch_size=param['batch_size']).ravel() / (N_Splits)\n",
    "        K.clear_session()\n",
    "    cv_perf.append(round(roc_auc_score(train.target.values, oof_pred_deepfm), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_params[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV 50 best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['target']\n",
    "N_Splits = 50\n",
    "Verbose = 1\n",
    "Epochs = 50\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_pred_deepfm = np.zeros((len(train), ))\n",
    "y_pred_deepfm = np.zeros((len(test),))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_Splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "\n",
    "for fold, (tr_ind, val_ind) in enumerate(skf.split(train, train[target])):\n",
    "    \n",
    "    # Split\n",
    "    X_train, X_val = train[features+features_enc+indicator_cols].iloc[tr_ind], train[features+features_enc+indicator_cols].iloc[val_ind]\n",
    "    y_train, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n",
    "    train_model_input = {name:X_train[name] for name in feature_names}\n",
    "    val_model_input = {name:X_val[name] for name in feature_names}\n",
    "    test_model_input = {name:test[name] for name in feature_names}\n",
    "    \n",
    "    # Define model\n",
    "    model = DeepFM(linear_feature_columns, linear_feature_columns + dnn_feature_columns,\n",
    "                   dnn_hidden_units=(512, 512), dnn_dropout=0.0, dnn_use_bn=False, task='binary')\n",
    "    model.compile(\"adam\", \"binary_crossentropy\", metrics=[auc])\n",
    "    \n",
    "    # Define callbacks\n",
    "    es = callbacks.EarlyStopping(monitor='val_auc', min_delta=0.001, patience=4, verbose=Verbose, mode='max', baseline=None, restore_best_weights=True)\n",
    "    sb = callbacks.ModelCheckpoint('./nn_model.w8', save_weights_only=True, save_best_only=True, verbose=Verbose)\n",
    "#     clr = CyclicLR(base_lr=0.00001 / 100, max_lr = 0.0001, \n",
    "#                        step_size= int(1.0*(test.shape[0])/1024) , mode='exp_range',\n",
    "#                        gamma=1., scale_fn=None, scale_mode='cycle')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_auc', factor=0.2,\n",
    "                                  patience=5, min_lr=0.00001)\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(train_model_input, y_train,\n",
    "                        validation_data=(val_model_input, y_val),\n",
    "                        batch_size=BATCH_SIZE, epochs=Epochs, verbose=Verbose,\n",
    "                        callbacks=[es, sb, reduce_lr],)\n",
    "    model.load_weights('./nn_model.w8')\n",
    "    \n",
    "    # Predict\n",
    "    val_pred = model.predict(val_model_input, batch_size=512)\n",
    "    print(f\"validation AUC fold {fold+1} : {round(roc_auc_score(y_val, val_pred), 5)}\")\n",
    "    oof_pred_deepfm[val_ind] = val_pred.ravel()\n",
    "    y_pred_deepfm += model.predict(test_model_input, batch_size=512).ravel() / (N_Splits)\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"OOF AUC : {round(roc_auc_score(train.target.values, oof_pred_deepfm), 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = test.id.values\n",
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test_idx,\n",
    "    'target': y_pred_deepfm\n",
    "})\n",
    "submission.to_csv(\"submission_deepfm_cv_50.csv\", index=False)\n",
    "print(\"Submission file saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('oof_pred_deepfm_cv_50.npy',oof_pred_deepfm)\n",
    "np.save('y_pred_deepfm_cv_50.npy',    y_pred_deepfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cat_in_dat",
   "language": "python",
   "name": "cat_in_dat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
